<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Advancing Perspectivist Ground Truthing with Social Science – Thesis</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-6bd9cfa162949bde0a231f530c97869d.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">Thesis</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link active" href="./index.html" aria-current="page"> 
<span class="menu-text">Introduction</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">1</span> Introduction</a>
  <ul class="collapse">
  <li><a href="#present-work" id="toc-present-work" class="nav-link" data-scroll-target="#present-work"><span class="header-section-number">1.1</span> Present Work</a></li>
  </ul></li>
  <li><a href="#background" id="toc-background" class="nav-link" data-scroll-target="#background"><span class="header-section-number">2</span> Background</a>
  <ul class="collapse">
  <li><a href="#reference-data-comes-from-humans" id="toc-reference-data-comes-from-humans" class="nav-link" data-scroll-target="#reference-data-comes-from-humans"><span class="header-section-number">2.1</span> Reference data comes from humans</a></li>
  <li><a href="#common-shortcomings-of-reference-data-design" id="toc-common-shortcomings-of-reference-data-design" class="nav-link" data-scroll-target="#common-shortcomings-of-reference-data-design"><span class="header-section-number">2.2</span> Common shortcomings of reference data design</a>
  <ul class="collapse">
  <li><a href="#representational-bias" id="toc-representational-bias" class="nav-link" data-scroll-target="#representational-bias"><span class="header-section-number">2.2.1</span> Representational bias</a></li>
  <li><a href="#the-perspectives-of-annotators" id="toc-the-perspectives-of-annotators" class="nav-link" data-scroll-target="#the-perspectives-of-annotators"><span class="header-section-number">2.2.2</span> The perspectives of annotators</a></li>
  <li><a href="#measurement-bias" id="toc-measurement-bias" class="nav-link" data-scroll-target="#measurement-bias"><span class="header-section-number">2.2.3</span> Measurement bias</a></li>
  <li><a href="#inadequate-reporting" id="toc-inadequate-reporting" class="nav-link" data-scroll-target="#inadequate-reporting"><span class="header-section-number">2.2.4</span> Inadequate reporting</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#present-work-1" id="toc-present-work-1" class="nav-link" data-scroll-target="#present-work-1"><span class="header-section-number">3</span> Present Work</a>
  <ul class="collapse">
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references"><span class="header-section-number">3.1</span> References</a></li>
  </ul></li>
  </ul>
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="index.docx"><i class="bi bi-file-word"></i>MS Word</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Advancing Perspectivist Ground Truthing with Social Science</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="introduction" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Introduction</h1>
<p>AI systems run on data. Data are used to ‘train’ <em>models</em> - imperfect, simplified mathematical or computational representations of a phenomenon or process in the real world <a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>. Where an AI system is a complete application with integrated components e.g.&nbsp;an interface, programmatic logic, and one or more models aimed at performing tasks typically requiring human intelligence<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>, the models themselves are embedded components that take inputs (e.g.&nbsp;media like text, images, audio, or data) and produce outputs (e.g.&nbsp;classifications, predictions or some form of decision). Model <em>behavior</em> - the model’s output or response to a given input - is determined by their <em>parameters</em> - internal settings or values<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>. <em>Algorithms</em> - step by step instructions, executed in order<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> - are used to estimate model parameters from the ‘training’ data. As models used in AI systems are designed to perform tasks, their performance is often evaluated empirically by comparing their outputs to a reference. This reference is often a second form of data, referred to as the ‘ground truth’ or ‘gold standard’, which represents the ideal expected output of the system.</p>
<p>Training data may be tabular data, but is often a form of media - text, audio, images, or video, whereas reference data often contains aggregated input from humans <span class="citation" data-cites="geiger2021garbage geiger2020garbage sav2023annotation muller2021designing">(<a href="#ref-geiger2020garbage" role="doc-biblioref">Geiger et al., 2020</a>, <a href="#ref-geiger2021garbage" role="doc-biblioref">2021</a>; <a href="#ref-muller2021designing" role="doc-biblioref">Muller et al., 2021</a>; <a href="#ref-sav2023annotation" role="doc-biblioref">Sav et al., 2023</a>)</span>. Data sets designed for training, reference or both, are often re-used, likely due to the ease of access - often a mere download - compared to the effort and cost required to design, collect, and evaluate such data sets. Human input may be collected explicitly - where a phenomenon of interest is set as a target, and humans are given a task whereby their responses produce data relevant to the target. Common scenarios include human judges annotating, labeling, or rating a) individual pieces of content of the same form as the training data, or b) generated system outputs for the presence/absence, or degree of the phenomenon of interest<span class="citation" data-cites="muller2021designing">(<a href="#ref-muller2021designing" role="doc-biblioref">Muller et al., 2021</a>)</span>. Data may also be collected implicitly - where digital traces of human behavior, e.g.&nbsp;media consumption, form the target <span class="citation" data-cites="sav2023annotation">(<a href="#ref-sav2023annotation" role="doc-biblioref">Sav et al., 2023</a>)</span>.</p>
<p>Qualities of both forms of data (training and evaluation) determine the quality of the system, whether used to train the system, to evaluate it, or both. Parameters are estimated from the training data, with the aim that observable patterns will be recognized in the data, and affect the behavior of the model used by the system. Thus, if the model accurately reflects the data, parameters will accurately reflect imperfections, inaccuracies, biases etc. in the data as well. Also, as models are evaluated by comparing their outputs to reference data, such that preferred models are those whose outputs most closely resemble the reference data <span class="citation" data-cites="birhane2022values">(<a href="#ref-birhane2022values" role="doc-biblioref">Birhane et al., 2022</a>)</span>, imperfections in the reference are reflected in the models preferred. Thus, the best possible performance in the real world directly corresponds to the degree to which training and reference data represent the phenomenon of interest, in the environment to which it is to be deployed. Thus, evaluating a model in an AI system involves two measurement problems: 1) measuring the phenomenon of interest in the media selected for inclusion in the reference and/or training data, and 2) measuring the similarity of the output of a model to the reference <span class="citation" data-cites="welty2019metrology">(<a href="#ref-welty2019metrology" role="doc-biblioref">Welty et al., 2019</a>)</span>. While the second has received extensive attention, the first has not.</p>
<p>Human input is never identical even when submitted carefully - in other words, people do not perfectly agree <span class="citation" data-cites="cabitza2023toward">(<a href="#ref-cabitza2023toward" role="doc-biblioref">Cabitza et al., 2023</a>)</span>. As most models are only compatible with a single reference point per piece of content, multiple inputs are often collected from different people with reference to items of content, and subsequently aggregated. This ignores the process that produced the reference data by assuming a singular ‘ground truth’ per piece of content and treating all disagreement as ‘noise’ rather than ‘signal’, leading to biased or inaccurate reference data <span class="citation" data-cites="aroyo2015truth cabitza2023toward">(<a href="#ref-aroyo2015truth" role="doc-biblioref">Aroyo &amp; Welty, 2015</a>; <a href="#ref-cabitza2023toward" role="doc-biblioref">Cabitza et al., 2023</a>)</span>. Thus, relevant distributions in the data used as the reference and/or training must resemble those in the environment to which the system will be deployed <span class="citation" data-cites="hullman2022worst">(<a href="#ref-hullman2022worst" role="doc-biblioref">Hullman et al., 2022</a>)</span>, but also account for the fact that human input is never identical <span class="citation" data-cites="aroyo2015truth cabitza2023toward">(<a href="#ref-aroyo2015truth" role="doc-biblioref">Aroyo &amp; Welty, 2015</a>; <a href="#ref-cabitza2023toward" role="doc-biblioref">Cabitza et al., 2023</a>)</span>.</p>
<p>Although the true values of the relevant distributions are unknowable, cues to whether human input resembles useful measurements can be calculated <span class="citation" data-cites="welty2019metrology jacobs2021measurement">(<a href="#ref-jacobs2021measurement" role="doc-biblioref">Jacobs &amp; Wallach, 2021</a>; <a href="#ref-welty2019metrology" role="doc-biblioref">Welty et al., 2019</a>)</span>, and can account for a range of reasonable interpretations in the reference <span class="citation" data-cites="aroyo2015truth cabitza2023toward">(<a href="#ref-aroyo2015truth" role="doc-biblioref">Aroyo &amp; Welty, 2015</a>; <a href="#ref-cabitza2023toward" role="doc-biblioref">Cabitza et al., 2023</a>)</span>. The social sciences have developed sampling methods to represent distributions in populations which can be adapted to the collection of content <span class="citation" data-cites="groves2009survey">(<a href="#ref-groves2009survey" role="doc-biblioref">Groves et al., 2009</a>)</span>, and syntheses show how to leverage variance in human input using knowledge from survey science <span class="citation" data-cites="beck2022improving">(<a href="#ref-beck2022improving" role="doc-biblioref">Beck et al., 2022</a>)</span>, metrology <span class="citation" data-cites="welty2019metrology">(<a href="#ref-welty2019metrology" role="doc-biblioref">Welty et al., 2019</a>)</span>, psychometrics <span class="citation" data-cites="jacobs2021measurement">(<a href="#ref-jacobs2021measurement" role="doc-biblioref">Jacobs &amp; Wallach, 2021</a>)</span>, and the perspective approach to ground-truthing <span class="citation" data-cites="cabitza2023toward">(<a href="#ref-cabitza2023toward" role="doc-biblioref">Cabitza et al., 2023</a>)</span>. Yet, knowledge from these fields has broadly not been applied in the field of Machine Learning, leading to issues of representation and measurement <span class="citation" data-cites="hullman2022worst">(<a href="#ref-hullman2022worst" role="doc-biblioref">Hullman et al., 2022</a>)</span>.</p>
<section id="present-work" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="present-work"><span class="header-section-number">1.1</span> Present Work</h2>
<p>Present work firstly attempts to synthesize input from various fields on how to better gather reference data to be used the ‘ground truth’ for the models that underlie AI systems. Present work then attempts to demonstrate the potential of synthesizing knowledge from the social sciences (psychometrics, survey science) related to sampling and measurement, with extant work on more useful content collection for use in machine learning tasks (metrology, perspectivist ground-truthing). Building on prior work reviewed herein, its main contribution is a synthesized framework that can be used to ground challenging phenomena in various media, following principles from prior work. A secondary contribution is a case study spanning several manuscripts, of a complex evaluation data set creation project. A third contribution, is knowledge directly applicable to the grounding of personal values in text, such as our annotation procedure, analysis of reference data, and statistics of interest for planning and estimating the costs associated. A final contribution is immediately applicable results that work towards estimating personal values in song lyrics using language models.</p>
<p>Included in this thesis are two manuscripts that further motivate the case study: 1) the first reviews strengths and weaknesses of datasets used in the field of Recommender Systems, 2) the second reviews how poor data practices in the field of Signal Processing related to datasets whose interconnections were poorly reported, misleading results. The case study demonstrates the use of principles from the Social Sciences to solve problems of representation and measurement across 4 manuscripts: personal values in song lyrics. In a 5th manuscript, the same principles are applied to a second form of text, political speeches, expected to vary in terms of use of ambiguous language. Despite the moderate success in automatically estimating values in lyrics, this work demonstrates a failure with speeches. It includes recommendations for analyses to observe the potential for success or failure, and to estimate cost via less expensive pilot studies. This thesis follows the case study with work that 6) highlights the potential for shortcomings in the interpretation of AI system evaluations should a more epistemologically sophisticated framework for evaluation not be adopted, and 7) highlights an important component of scientific infrastructure needed for rigorous work on data sets for Machine Learning: the treatment of scientific work as open-source artifacts.</p>
</section>
</section>
<section id="background" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Background</h1>
<section id="reference-data-comes-from-humans" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="reference-data-comes-from-humans"><span class="header-section-number">2.1</span> Reference data comes from humans</h2>
<p>AI runs on data generated by humans. Reference data often uses responses from humans in the form of labels or annotations of content. “[D]ata annotation is the practice of labeling a set of digital representations of objects” <span class="citation" data-cites="cabitza2023toward">(<a href="#ref-cabitza2023toward" role="doc-biblioref">Cabitza et al., 2023</a>)</span>. Although few studies have systematically examined the frequency of use of data from humans, it has been shown that reference data includes human input either explicitly or implicitly very often. <span class="citation" data-cites="geiger2021garbage">Geiger et al. (<a href="#ref-geiger2021garbage" role="doc-biblioref">2021</a>)</span> systematically review 200 randomly sampled papers from 3 broad domains, Social Sciences &amp; Humanities, Life &amp; Biomedical Sciences and Physical &amp; Environmental Sciences. Out of the 140 studies that were classification tasks, 73.05% (or 103 papers) used labels derived from human responses as the reference. <span class="citation" data-cites="geiger2020garbage">Geiger et al. (<a href="#ref-geiger2020garbage" role="doc-biblioref">2020</a>)</span> reviewed 164 papers whose classifiers were trained on Twitter data and observed that 65% of the works reviewed used human annotations for the purposes of training. They further note that this quantity did not include human annotations used for validation, or other meta-data e.g.&nbsp;hashtags contributed by humans. In some domains the contribution of humans is in the form of digital traces, as in the domain of Recommender Systems where it was observed that, out of the most highly cited papers between 2018 and 2022, 86% of the datasets used were transaction data released by vendors such as Amazon or Yelp <span class="citation" data-cites="sav2023annotation">(<a href="#ref-sav2023annotation" role="doc-biblioref">Sav et al., 2023</a>)</span>. Whether human input is explicit or implicit, it is present in almost all reference data.</p>
<p>Furthermore, training/reference data sets are often re-used, in some cases treated as <em>benchmarks</em> - measurement instruments used to produce comparable quantitative assessments of models <span class="citation" data-cites="welty2019metrology">(<a href="#ref-welty2019metrology" role="doc-biblioref">Welty et al., 2019</a>)</span> - magnifying their impact. <span class="citation" data-cites="geiger2021garbage">Geiger et al. (<a href="#ref-geiger2021garbage" role="doc-biblioref">2021</a>)</span> observed that 56.31% of the classification tasks that were reviewed (or 58 papers) used only ‘external’ human labels, i.e.&nbsp;labels that were not collected specifically for the work in the paper, <span class="citation" data-cites="geiger2020garbage">Geiger et al. (<a href="#ref-geiger2020garbage" role="doc-biblioref">2020</a>)</span> observed that 33.3% of the papers used external annotations, and <span class="citation" data-cites="sav2023annotation">Sav et al. (<a href="#ref-sav2023annotation" role="doc-biblioref">2023</a>)</span> observed that just 4 data sets appeared in at least 10% of works reviewed, with the most commonly used data set appearing in 33% of the works reviewed. Examining the most highly cited papers in IEEE CVPR from 2020-2022, the initial papers announcing the benchmark, along with the training and reference data received citation counts in the tens of thousands: Imagenet <span class="citation" data-cites="deng2009imagenet">(<a href="#ref-deng2009imagenet" role="doc-biblioref">Deng et al., 2009</a>)</span> shows over 52k citations, COCO <span class="citation" data-cites="lin2014microsoft">(<a href="#ref-lin2014microsoft" role="doc-biblioref">Lin et al., 2014</a>)</span> shows over 29k, Pascal VOC <span class="citation" data-cites="everingham2010pascal">(<a href="#ref-everingham2010pascal" role="doc-biblioref">Everingham et al., 2010</a>)</span> shows over 15k, according to SCOPUS as of April 2025. Thus, these human input data sets have the potential for long lasting effects on work that follows.</p>
<p>Although far more emphasis is placed on whether models achieve state of the art ‘performance’ or efficiency <span class="citation" data-cites="birhane2022values">(<a href="#ref-birhane2022values" role="doc-biblioref">Birhane et al., 2022</a>)</span>, scholars over the past decade have attempted to draw attention to a lack of sophistication in how training and reference data are selected and evaluated <span class="citation" data-cites="aroyo2015truth">(<a href="#ref-aroyo2015truth" role="doc-biblioref">Aroyo &amp; Welty, 2015</a>)</span>. It has been argued that a focus on improving the data for a given task, will result in bigger gains than a focus on improving model<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>. Importantly, <span class="citation" data-cites="hullman2022worst">Hullman et al. (<a href="#ref-hullman2022worst" role="doc-biblioref">2022</a>)</span> show that optimizing for predictive accuracy does not absolve researchers from shortcomings in reference/training data, a situation exacerbated by the often re-use of data sets. A solution rather entails acknowledging that, whether deliberate or not, informed or not, organized or improvised, data are generated by process that may or may not be deliberately designed <span class="citation" data-cites="muller2021designing">(<a href="#ref-muller2021designing" role="doc-biblioref">Muller et al., 2021</a>)</span>, yet would greatly benefit from design. Building on this, <span class="citation" data-cites="welty2019metrology">Welty et al. (<a href="#ref-welty2019metrology" role="doc-biblioref">2019</a>)</span> argue that datasets used to evaluate AI systems should be treated as measurement instruments in their own right. Drawing on the science of metrology, they propose that benchmark datasets ought to be evaluated using criteria analogous to those used for physical measurement tools.</p>
</section>
<section id="common-shortcomings-of-reference-data-design" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="common-shortcomings-of-reference-data-design"><span class="header-section-number">2.2</span> Common shortcomings of reference data design</h2>
<p>Recent trends in Machine Learning (ML)— especially in deep learning — prioritize empirical performance over theoretical assumptions about the data generating process. A systematic analysis of highly cited ML works shows what is valued most: Performance, Generalization, Quantitative evidence, Efficiency, Building on past work, and Novelty <span class="citation" data-cites="birhane2022values">Birhane et al. (<a href="#ref-birhane2022values" role="doc-biblioref">2022</a>)</span>. Unlike the social sciences (e.g.&nbsp;psychology), ML work ignores attempts to model the process that gives rise to the data, and aims instead at predictive models whose outputs fall within some accepted estimated error bounds, resulting in poor or even biased reference data design <span class="citation" data-cites="hullman2022worst">(<a href="#ref-hullman2022worst" role="doc-biblioref">Hullman et al., 2022</a>)</span>.</p>
<p>Considering the ever-presence of human influence on the reference data, best practices, considerations, and frameworks from the social sciences could inform designs, but have yet to be broadly applied in the computational sciences <span class="citation" data-cites="beck2022improving jacobs2021measurement">(<a href="#ref-beck2022improving" role="doc-biblioref">Beck et al., 2022</a>; <a href="#ref-jacobs2021measurement" role="doc-biblioref">Jacobs &amp; Wallach, 2021</a>)</span>. One reason for this gap may be that ML researchers prefer to work on building systems and evaluating their performance rather than researching, designing and executing ground-truthing projects <span class="citation" data-cites="sambasivan2021everyone muller2021designing">(<a href="#ref-muller2021designing" role="doc-biblioref">Muller et al., 2021</a>; <a href="#ref-sambasivan2021everyone" role="doc-biblioref">Sambasivan et al., 2021</a>)</span>. Another may be a lack of focus on these topics in textbooks, and thus in education more broadly <span class="citation" data-cites="geiger2020garbage">(<a href="#ref-geiger2020garbage" role="doc-biblioref">Geiger et al., 2020</a>)</span>. A third may be that the social and computational sciences have conceptually different focci: the computational sciences focus on the statistical model the system with substantially less emphasis on the content, whereas the social sciences treat the statistical model as a means to better understanding the relationships in the content <span class="citation" data-cites="liem2018psychology">(<a href="#ref-liem2018psychology" role="doc-biblioref">Liem et al., 2018</a>)</span>. Psychology research thus contains many more research projects in which datasets are collected using responses from people, whereas datasets tend to be re-used extensively in machine learning work <span class="citation" data-cites="geiger2021garbage">(<a href="#ref-geiger2021garbage" role="doc-biblioref">Geiger et al., 2021</a>)</span>. A further more practical complication is that work on these topics lacks the acknowledgement that ground-truthing is indeed a measurement problem, and lacks a central academic ‘home’: where psychology and economics have psychometrics and econometrics respectively, fields dedicated to studying field-specific measurement practices, the study of ground-truthing lacks a central banner under which academic work can accumulate and disseminate.</p>
<p>Decisions such as the selection of items for training data <span class="citation" data-cites="hullman2022worst">(<a href="#ref-hullman2022worst" role="doc-biblioref">Hullman et al., 2022</a>)</span>, and the collection of human responses for reference data <span class="citation" data-cites="beck2022improving">(<a href="#ref-beck2022improving" role="doc-biblioref">Beck et al., 2022</a>)</span>, are part of a design of a process that results in data <span class="citation" data-cites="muller2021designing">(<a href="#ref-muller2021designing" role="doc-biblioref">Muller et al., 2021</a>)</span>. Further, an investigation of 15 data science workers, <span class="citation" data-cites="muller2021designing">Muller et al. (<a href="#ref-muller2021designing" role="doc-biblioref">2021</a>)</span> observed common phases, which include determining the annotation scheme - a) all possible labels that can be attributed to digital representations of objects along with any relevant guidelines, b) the actual process of collecting labels, and c) the process by which the annotations are then aggregated into a single label. They note the difficulty of this work: issues in the annotation schemes are often discovered as annotation projects progress, requiring varying degrees of improvised adjustment. For any of these components, decisions are made that impact the resulting reference data, whether or not they are being made by design.</p>
<p>Commonly observed shortcomings of refernce data include: 1) representational biases in the content sampled for inclusion in training/evaluation datasets <span class="citation" data-cites="hullman2022worst">(<a href="#ref-hullman2022worst" role="doc-biblioref">Hullman et al., 2022</a>)</span>, 2) a fallacious assumption of a single canonical ‘ground-truth’ when there are a range of reasonable interpretations <span class="citation" data-cites="aroyo2015truth cabitza2023toward">(<a href="#ref-aroyo2015truth" role="doc-biblioref">Aroyo &amp; Welty, 2015</a>; <a href="#ref-cabitza2023toward" role="doc-biblioref">Cabitza et al., 2023</a>)</span>, 3) measurement biases in the annotations collected <span class="citation" data-cites="jacobs2021measurement hullman2022worst beck2022improving">(<a href="#ref-beck2022improving" role="doc-biblioref">Beck et al., 2022</a>; <a href="#ref-hullman2022worst" role="doc-biblioref">Hullman et al., 2022</a>; <a href="#ref-jacobs2021measurement" role="doc-biblioref">Jacobs &amp; Wallach, 2021</a>)</span>, and 4) poor reporting of necessary information regarding the annotation-collection process <span class="citation" data-cites="hullman2022worst geiger2021garbage">(<a href="#ref-geiger2021garbage" role="doc-biblioref">Geiger et al., 2021</a>; <a href="#ref-hullman2022worst" role="doc-biblioref">Hullman et al., 2022</a>)</span>.</p>
<p>An additional consideration that receives little attention is 5) the estimation of the number of annotations to gather, where fields that focus on gathering data from humans typically also have a strong emphasis on <em>a-priori</em> decisions, such as the pre-registration of calculated of target sample sizes estimated via statistical power analysis <span class="citation" data-cites="cohen1992statistical">(<a href="#ref-cohen1992statistical" role="doc-biblioref">Cohen, 1992</a>)</span>, to mitigate sources of bias that come from the researcher. These considerations are absent in computational fields which appear to favor differing rules of thumb: e.g.&nbsp;in a well-cited textbook, <span class="citation" data-cites="PustejovskyStubbsNLannotation">Pustejovsky &amp; Stubbs (<a href="#ref-PustejovskyStubbsNLannotation" role="doc-biblioref">2013</a>)</span> suggest to “have your corpus annotated by at least two people (more is preferable, but not always practical)”, whereas <span class="citation" data-cites="artstein2008inter">Artstein &amp; Poesio (<a href="#ref-artstein2008inter" role="doc-biblioref">2008</a>)</span> suggest that “measuring reliability with only two coders is seldom enough, except for small-scale studies”.On the one hand, corpora tend to be very large, and resources are finite, making cost a primary factor in design decisions. On the other hand, rules of thumb lack clear substantiation in light of the both 1) the phenomenon being grounded and 2) the ambiguity of the media in which it is grounded. In other words, more variance is expected in annotation targets to the degree they are subjective or based on opinion <span class="citation" data-cites="beck2022improving">(<a href="#ref-beck2022improving" role="doc-biblioref">Beck et al., 2022</a>)</span>, and more variance is expected in content to the degree to which it is ambiguous - i.e.&nbsp;can be interpreted in multiple ways - such as figurative language <span class="citation" data-cites="sandri2023don">(<a href="#ref-sandri2023don" role="doc-biblioref">Sandri et al., 2023</a>)</span>. Further, some degree of variance will always be present when there are multiple annotations or ratings for a given piece of media independent of the target <span class="citation" data-cites="cabitza2023toward">(<a href="#ref-cabitza2023toward" role="doc-biblioref">Cabitza et al., 2023</a>)</span>, based on the range of reasonable interpretations of that target in that media <span class="citation" data-cites="aroyo2015truth">(<a href="#ref-aroyo2015truth" role="doc-biblioref">Aroyo &amp; Welty, 2015</a>)</span>.</p>
<section id="representational-bias" class="level3" data-number="2.2.1">
<h3 data-number="2.2.1" class="anchored" data-anchor-id="representational-bias"><span class="header-section-number">2.2.1</span> Representational bias</h3>
<p>When sampling content to include in training/test datasets, samples for the training/test sets will ideally be drawn from the same distribution as the content in which they will eventually be deployed. <em>Representation bias</em> in content selected for training and/or evaluation datasets refers to the degree to which relevant distributions in data used as reference and/or training data resemble distributions in the environment to which a system is deployed <span class="citation" data-cites="hullman2022worst">(<a href="#ref-hullman2022worst" role="doc-biblioref">Hullman et al., 2022</a>)</span>. If data used for training under-represents parts of the input space of an algorithm that then estimates parameters from that input space, the model resulting will have higher error rates for those under-represented parts of the input space when deployed. If content is selected without appropriate design aimed at representing the population from which samples are drawn, the overall distribution will not represent the population it was drawn from. Thus, optimizing for predictive accuracy using very large datasets does not ‘absolve’ researchers from having to consider the data generating process, and this includes sampling pieces of content to be annotated.</p>
<p>Approaches to representation problems can come from Sampling Theory, which frames the problem as one of selecting elements of a population, from which a sample must be drawn, and where the aim is that measurements of interest in the sample resemble measurements of interest in the population <span class="citation" data-cites="groves2009survey">(<a href="#ref-groves2009survey" role="doc-biblioref">Groves et al., 2009</a>)</span>. This framework is typically applied to selecting people for inclusion in survey studies, whereby their responses to questions lend themselves to inference about a target population. Although there is no ‘one-size-fits-all’ solution to sampling, this thesis makes use of <em>stratified random sampling</em> as a general strategy: namely, the identification of groups of elements within a population that may affect the measurements in question, and the random sampling of elements within the groups, with approximately equal observations. In principle, this allows for the representation of the groups in population, on the measurement of interest, with some margin of error <span class="citation" data-cites="groves2009survey">(<a href="#ref-groves2009survey" role="doc-biblioref">Groves et al., 2009</a>)</span>.</p>
</section>
<section id="the-perspectives-of-annotators" class="level3" data-number="2.2.2">
<h3 data-number="2.2.2" class="anchored" data-anchor-id="the-perspectives-of-annotators"><span class="header-section-number">2.2.2</span> The perspectives of annotators</h3>
<p>The field of machine learning tends to treat all annotation variance as noise rather than signal. Often multiple ratings per piece of content are collected, aggregated, and only then shared, forming a singular ‘ground truth’ for the aspect of the content being labelled or rated. The quality of annotations is typically assessed using inter-annotator agreement, where more agreement is typically thought to indicate higher quality data <span class="citation" data-cites="aroyo2015truth">Aroyo &amp; Welty (<a href="#ref-aroyo2015truth" role="doc-biblioref">2015</a>)</span>. Thus, it is assumed that there is a singular canonical truth for each aspect / content pair, comprised of aggregated human responses, visible as the general agreement of human response, and which forms a target to which we align our automated systems. To illustrate more accurate representation of human responses, however, <span class="citation" data-cites="aroyo2015truth">Aroyo &amp; Welty (<a href="#ref-aroyo2015truth" role="doc-biblioref">2015</a>)</span> operationalize their term ‘crowd truth’ as the ‘gold standard’ being the probability that a sentence contains an element, based on the probability that an annotator annotated that sentence with that element i.e.&nbsp;the label isn’t represented as ‘present’ or ‘not present’, but as a probability that an annotator labelled it as such. The probability that they may label it as such may in part explained by certain characteristics of theirs, such as their backgrounds, personal experiences etc. <span class="citation" data-cites="beck2022improving">(<a href="#ref-beck2022improving" role="doc-biblioref">Beck et al., 2022</a>)</span>.</p>
<p>Disagreement is common and never fully reducible <span class="citation" data-cites="cabitza2019elephant">(<a href="#ref-cabitza2019elephant" role="doc-biblioref">Cabitza et al., 2019</a>)</span>. <span class="citation" data-cites="cabitza2023toward">Cabitza et al. (<a href="#ref-cabitza2023toward" role="doc-biblioref">2023</a>)</span> show that this is the case whether the task is typically thought of as subjective, e.g.&nbsp;NLP tasks <span class="citation" data-cites="aroyo2015truth">(<a href="#ref-aroyo2015truth" role="doc-biblioref">Aroyo &amp; Welty, 2015</a>)</span>, but also in tasks thought to be far less so, e.g.&nbsp;medical cases <span class="citation" data-cites="cabitza2019elephant">(<a href="#ref-cabitza2019elephant" role="doc-biblioref">Cabitza et al., 2019</a>)</span>. Disagreement, observable as variance in the human input, is often removed via 1) adjusting annotator training and instruction so as to reduce variance in the human inputs at the time of collection, 2) adjusting annotations via discussion post-collection, thus allowing annotators to establish conventions, discuss views, and re-think their responses, or 3) completely post-hoc at the time of modeling, via methods like majority voting, without input from the annotators. Each method of reducing variance - e.g.&nbsp;thorough training for crowd-sourced workers, regular annotator meetings to resolve disagreements, or taking a mean of ratings or majority vote - may result in different data independent of content, or the phenomenon of interest being annotated in the content.</p>
<p>Further, variance in observed disagreement can be signal rather than noise. This signal may help to better understand the content being annotated: annotations may vary based on the ambiguity of the stimuli themselves, both in terms of the mode (audio vs.&nbsp;image vs.&nbsp;video vs.&nbsp;text), specific medium (Tweet vs.&nbsp;podcast transcript), or even the specific piece of content being annotated <span class="citation" data-cites="aroyo2015truth">(<a href="#ref-aroyo2015truth" role="doc-biblioref">Aroyo &amp; Welty, 2015</a>)</span>. Thus, not all pieces of content are equally unambiguous, and more ambiguous content is likely to result in greater variances in human input. This signal may help better understand the phenomenon of interest being annotated in the content: for at least some phenomena, the assumption that there is a single ground-truth to approximate with annotations doesn’t hold <span class="citation" data-cites="aroyo2015truth beck2022improving cabitza2023toward">(<a href="#ref-aroyo2015truth" role="doc-biblioref">Aroyo &amp; Welty, 2015</a>; <a href="#ref-beck2022improving" role="doc-biblioref">Beck et al., 2022</a>; <a href="#ref-cabitza2023toward" role="doc-biblioref">Cabitza et al., 2023</a>)</span>. More broadly, variances in the ratings may inform a finite “range of reasonable interpretations” of the phenomenon of interest being annotated in each piece of content, rather than a singular point. This signal may also help to better understand the background of annotators: people’s ethnic and/or cultural backgrounds may determine how they interpret content, and thus characteristics of the annotators may explain variance in the annotations. For example, although we expect hate speech exists, people’s perceptions of what constitutes hate speech may vary <span class="citation" data-cites="beck2022improving">Beck et al. (<a href="#ref-beck2022improving" role="doc-biblioref">2022</a>)</span>. Showing that perceptions vary by identifiable characteristics, e.g.&nbsp;gender identity, ethnicity etc. may help unearth biases, whereby a single group perspective appears ‘objective’ <span class="citation" data-cites="cabitza2023toward">(<a href="#ref-cabitza2023toward" role="doc-biblioref">Cabitza et al., 2023</a>)</span>.</p>
<p>Taking an approach to gathering reference data that attempts to account for the perspectives of the annotators is referred to as the <em>perspectivist</em><a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> approach. It can apply to both the data annotation but also the modelling phase of ML projects, where benefits to ML models has been shown in a number of contexts <span class="citation" data-cites="cabitza2023toward">(<a href="#ref-cabitza2023toward" role="doc-biblioref">Cabitza et al., 2023</a>)</span>. Although typically focused on the annotation of language data, perspectivist approaches can be broadly applied to annotations in reference data: <em>weak</em> perspectivist approaches involve taking perspectives into account while designing and collecting annotations e.g.&nbsp;by ensuring heterogenous raters and gathering enough ratings, as well as sharing and reporting the disaggregated data, but ultimately reducing annotations to a single label or rating for modeling. <em>Strong</em> perspectivist approaches involve taking perspectives into account for ground truthing and modelling phases.</p>
<p>Taking the perspectivist approach has a number of clear benefits, but also costs. It involves substantially more effort required to design the process that will result in annotations, higher costs in terms of the number of annotations and annotators needed in order to examine sources of variance, and challenges validating the data. In addition there are thus far few perspectivist modelling approaches that make full use of the variance in inputs <span class="citation" data-cites="cabitza2023toward">(<a href="#ref-cabitza2023toward" role="doc-biblioref">Cabitza et al., 2023</a>)</span>. However, the perspectivist approach better reflects the reality that collecting annotations is a process that generates data with a number of relevant components <span class="citation" data-cites="hullman2022worst jacobs2021measurement">(<a href="#ref-hullman2022worst" role="doc-biblioref">Hullman et al., 2022</a>; <a href="#ref-jacobs2021measurement" role="doc-biblioref">Jacobs &amp; Wallach, 2021</a>)</span>. Further, it is a more complete report of the data resulting from the annotation process: the inclusion of the varying inputs in turn allows for better understanding of the content being annotated, the annotators annotating it, and the phenomenon of interest being annotated, which in turn allows for the development of models that make use of this information <span class="citation" data-cites="cabitza2023toward">(<a href="#ref-cabitza2023toward" role="doc-biblioref">Cabitza et al., 2023</a>)</span>. This thesis makes use of stratified sampling among annotators in order to account for multiple perspectives.</p>
</section>
<section id="measurement-bias" class="level3" data-number="2.2.3">
<h3 data-number="2.2.3" class="anchored" data-anchor-id="measurement-bias"><span class="header-section-number">2.2.3</span> Measurement bias</h3>
<p>The social sciences treat data from people as imperfect observations of a latent variable called a <em>construct</em> - like the effectiveness of a teacher, or recidivism i.e.&nbsp;the risk that someone will repeat a crime, or personality from the field of Psychology. Social and computational sciences traditionally have different focci: where the social sciences emphasize an interpretable meaning of <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, where <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> are not always directly observable, the computational sciences instead focus on the statistical procedure that correlates <span class="math inline">\(x\)</span> in terms of <span class="math inline">\(y\)</span> (see Fig 1.). Applying the latent variable approach to the gathering of annotations, <span class="citation" data-cites="jacobs2021measurement">Jacobs &amp; Wallach (<a href="#ref-jacobs2021measurement" role="doc-biblioref">2021</a>)</span> suggest there is a ‘measurement error model’ (a term they borrow from the field of Economics), that links the unobservable latent variable, and properties that we can observe - in our case, the data produced when people label or annotate. Thus, the annotations we observe can not be the ‘ground truth’ as such a thing is unknowable. Rather, each annotation is an imperfect indication that can be used to estimate the ground truth.</p>
<p><img src="images/Liem_2018_figure_1.png" class="img-fluid"> Fig. 1: <span class="citation" data-cites="liem2018psychology">Liem et al. (<a href="#ref-liem2018psychology" role="doc-biblioref">2018</a>)</span></p>
<p>To measure constructs the social sciences like Psychology, Survey Science and Cognitive Science research and develop <em>instruments</em>: standardized, systematic procedures designed to compare individuals <span class="citation" data-cites="cronbach1960essentials">(<a href="#ref-cronbach1960essentials" role="doc-biblioref">Cronbach, 1960</a>)</span>. These are often in the form of surveys or standardized tasks designed to measure variables defined as latent, or indirectly observable. As one may measure one’s height with a ruler, one may acknowledge that no measurement is perfect, but estimate one’s latent ‘height’ via multiple measurements <span class="citation" data-cites="jacobs2021measurement">Jacobs &amp; Wallach (<a href="#ref-jacobs2021measurement" role="doc-biblioref">2021</a>)</span>. Similar to the ruler being an instrument to measure height, survey science treats the survey - the standardized process of collecting data from human input - like a measurement instrument. It thus seeks to minimize the influence of sources of ‘noise’: <em>Task Structure</em> involves refining specific wording and response options, including deciding on the inclusion of “I Don’t Know” or otherwise neutral response options, <em>Order Effects</em> involves strategies to randomly present content, as judgements of a specific piece of content are affected by perceptions of immediately previous pieces of content, and <em>Annotator Effects</em> which involves strategies to appropriately account for differences in perception based on the backgrounds, experiences and opinions of the annotators <span class="citation" data-cites="beck2022improving">(<a href="#ref-beck2022improving" role="doc-biblioref">Beck et al., 2022</a>)</span>.</p>
<p>A number of fields provide frameworks for assessing the quality of a measurement instrument, including psychometrics <span class="citation" data-cites="jacobs2021measurement">(<a href="#ref-jacobs2021measurement" role="doc-biblioref">Jacobs &amp; Wallach, 2021</a>)</span>, survey science <span class="citation" data-cites="beck2022improving">(<a href="#ref-beck2022improving" role="doc-biblioref">Beck et al., 2022</a>)</span>, and metrology - the science of measurement - <span class="citation" data-cites="welty2019metrology">(<a href="#ref-welty2019metrology" role="doc-biblioref">Welty et al., 2019</a>)</span>. For instance, the concept of <em>reliability</em> asks whether similar inputs consistently produce similar outputs, either across annotators (inter-rater reliability) or over time (test-retest reliability). The related concept of <em>precision</em> in metrology, separates the similarity of measurements from an instrument into <em>repeatability</em>, the similarity of measurements given that the operator, equipment, calibration, environment, and time between measurements are held constant, and <em>reproducibility</em>, the similarity of measurements given that the aforementioned are not held constant <span class="citation" data-cites="welty2019metrology">(<a href="#ref-welty2019metrology" role="doc-biblioref">Welty et al., 2019</a>)</span>.</p>
<p>Beyond consistency, <em>validity</em> addresses whether the instrument is actually measuring what it claims to measure <span class="citation" data-cites="jacobs2021measurement">(<a href="#ref-jacobs2021measurement" role="doc-biblioref">Jacobs &amp; Wallach, 2021</a>)</span>. This includes checks for face validity (does it seem plausible?), content validity (does it cover the full scope of the concept?), and structural or substantive validity (do the internal patterns make sense given extant theory?). Other forms such as convergent and discriminant validity test whether the measure behaves as expected relative to related or unrelated constructs, while predictive, hypothesis, and consequential validity consider what the measurement enables: does it support useful predictions, align with theoretical expectations, or have appropriate consequences in applied contexts? Although there is no one-size-fits-all solution to estimating the quality of an instrument, these various tools provide insights into whether the measurements appear to have qualities fitting of good measurements.</p>
</section>
<section id="inadequate-reporting" class="level3" data-number="2.2.4">
<h3 data-number="2.2.4" class="anchored" data-anchor-id="inadequate-reporting"><span class="header-section-number">2.2.4</span> Inadequate reporting</h3>
<p><span class="citation" data-cites="geiger2021garbage">Geiger et al. (<a href="#ref-geiger2021garbage" role="doc-biblioref">2021</a>)</span> ML science studies inadequately report ‘ground truth’ <span class="citation" data-cites="hullman2022worst">Hullman et al. (<a href="#ref-hullman2022worst" role="doc-biblioref">2022</a>)</span> thus we cannot know what data generating process the resulting model represents</p>
<p><span class="citation" data-cites="cabitza2023toward">Cabitza et al. (<a href="#ref-cabitza2023toward" role="doc-biblioref">2023</a>)</span> - adequate reporting: - number of raters, - rater expertise - incentive - instructions - length of time for labelling - inter rater agreement - aggregation method - confidence</p>
</section>
</section>
</section>
<section id="present-work-1" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Present Work</h1>
<p>This thesis attempts to synthesize recommendations on how to better collect data to be used as reference and possibly training. It accounts for the aforementioned shortcomings in the design of study, using recommendations from the social sciences and syntheses of material from survey science, metrology.</p>
<p>This thesis attempts to further the field in the following ways:</p>
<ul>
<li>we attempt representative sampling of both media and respondents</li>
<li>we aim to estimate 10-dimensional psychological construct</li>
<li>we select media that is ambiguous (i.e.&nbsp;that will result in subjectivity in the ratings) as well as media that we expect not to be ambiguous for comparison</li>
<li>we estimate a-priori the number of ratings necessary rather than assuming</li>
<li>we take into account perspectives</li>
</ul>
<p>The primary case study of this thesis works towards path (b) in <span class="citation" data-cites="liem2018psychology">Liem et al. (<a href="#ref-liem2018psychology" role="doc-biblioref">2018</a>)</span>, shown in: <img src="images/Liem_2018_figure_2.png" class="img-fluid">, and showcases a design for a challenging ground-truthing project, in terms of the complexity of the phenomenon of interest, ambiguity in the media that selected and annotated. It incorporates design choices to address the aforementioned shortcomings into a singular framework, guided by best practices in the social sciences, which it then extends.</p>
<p>Specifically:</p>
<ul>
<li>We attempt to mitigate representation biases in the content we select for annotation by using a stratified sampling strategy.</li>
<li>We attempt to mitigate measurement biases by treating the target measurement as a latent variable, and the survey we used to gather annotations as an instrument. We build on work that validated a questionnaire for measuring constructs, and estimating its reliability and structural validity when used for annotations.</li>
<li>We account for the potential of multiple perspectives in our dataset by recruiting participants from relevant subgroups in a single target population.</li>
<li>We report the details of the annotation collection process, and share the disaggregated dataset of the annotations</li>
<li>We further show how to estimate the number of annotators<br>
</li>
</ul>
<p>We demonstrate the potential of this framework by grounding a complex phenomenon (a 10-dimensional construct, Personal Values) in ambiguous text (song lyrics). We further show an</p>
<p>When sampling content to include in training/test datasets, samples for the training/test sets will ideally be drawn from the same distribution as the content in which they will eventually be deployed. <em>Representation bias</em> in content selected for training and/or evaluation datasets refers to the degree to which relevant distributions in data used to train and/or evaluate systems resembles the distribution in the environment to which it will be deployed <span class="citation" data-cites="hullman2022worst">(<a href="#ref-hullman2022worst" role="doc-biblioref">Hullman et al., 2022</a>)</span>.</p>
<p>Measurement bias in the annotations collected from humans may also bias</p>
<p>Perspectivism <span class="citation" data-cites="cabitza2023toward">Cabitza et al. (<a href="#ref-cabitza2023toward" role="doc-biblioref">2023</a>)</span> recommendations: - complete labeling schemes, including ‘i don’t know’, ‘none of these’ etc. categories, and the ability to express issues with label set</p>
<p>Reporting</p>
<p>We add: a priori rating number estimation</p>
<p>And although imperfect as leaderboard scores can be gamed, and do not perfectly represent the deployment environment, the typical leaderboard approach has shown evidence that progress can be made towards a target. This thesis thus represents an attempt to define the target better.</p>
<section id="references" class="level2" data-number="3.1">




</section>
</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">3.1 References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2" role="list">
<div id="ref-aroyo2015truth" class="csl-entry" role="listitem">
Aroyo, L., &amp; Welty, C. (2015). Truth is a lie: Crowd truth and the seven myths of human annotation. <em>AI Magazine</em>, <em>36</em>(1), 15–24.
</div>
<div id="ref-artstein2008inter" class="csl-entry" role="listitem">
Artstein, R., &amp; Poesio, M. (2008). Inter-coder agreement for computational linguistics. <em>Computational Linguistics</em>, <em>34</em>(4), 555–596.
</div>
<div id="ref-beck2022improving" class="csl-entry" role="listitem">
Beck, J., Eckman, S., Chew, R., &amp; Kreuter, F. (2022). Improving labeling through social science insights: Results and research agenda. <em>International Conference on Human-Computer Interaction</em>, 245–261.
</div>
<div id="ref-birhane2022values" class="csl-entry" role="listitem">
Birhane, A., Kalluri, P., Card, D., Agnew, W., Dotan, R., &amp; Bao, M. (2022). The values encoded in machine learning research. <em>Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency</em>, 173–184.
</div>
<div id="ref-cabitza2023toward" class="csl-entry" role="listitem">
Cabitza, F., Campagner, A., &amp; Basile, V. (2023). Toward a perspectivist turn in ground truthing for predictive computing. <em>Proceedings of the AAAI Conference on Artificial Intelligence</em>, <em>37</em>, 6860–6868.
</div>
<div id="ref-cabitza2019elephant" class="csl-entry" role="listitem">
Cabitza, F., Locoro, A., Alderighi, C., Rasoini, R., Compagnone, D., &amp; Berjano, P. (2019). The elephant in the record: On the multiplicity of data recording work. <em>Health Informatics Journal</em>, <em>25</em>(3), 475–490.
</div>
<div id="ref-cohen1992statistical" class="csl-entry" role="listitem">
Cohen, J. (1992). Statistical power analysis. <em>Current Directions in Psychological Science</em>, <em>1</em>(3), 98–101.
</div>
<div id="ref-cronbach1960essentials" class="csl-entry" role="listitem">
Cronbach, L. J. (1960). <em>Essentials of psychological testing, 2nd edition</em>. Harper.
</div>
<div id="ref-deng2009imagenet" class="csl-entry" role="listitem">
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., &amp; Fei-Fei, L. (2009). Imagenet: A large-scale hierarchical image database. <em>2009 IEEE Conference on Computer Vision and Pattern Recognition</em>, 248–255.
</div>
<div id="ref-everingham2010pascal" class="csl-entry" role="listitem">
Everingham, M., Van Gool, L., Williams, C. K., Winn, J., &amp; Zisserman, A. (2010). The pascal visual object classes (voc) challenge. <em>International Journal of Computer Vision</em>, <em>88</em>, 303–338.
</div>
<div id="ref-geiger2021garbage" class="csl-entry" role="listitem">
Geiger, R. S., Cope, D., Ip, J., Lotosh, M., Shah, A., Weng, J., &amp; Tang, R. (2021). " garbage in, garbage out" revisited: What do machine learning application papers report about human-labeled training data? <em>arXiv Preprint arXiv:2107.02278</em>.
</div>
<div id="ref-geiger2020garbage" class="csl-entry" role="listitem">
Geiger, R. S., Yu, K., Yang, Y., Dai, M., Qiu, J., Tang, R., &amp; Huang, J. (2020). Garbage in, garbage out? Do machine learning application papers in social computing report where human-labeled training data comes from? <em>Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency</em>, 325–336.
</div>
<div id="ref-groves2009survey" class="csl-entry" role="listitem">
Groves, R. M., Fowler Jr, F. J., Couper, M. P., Lepkowski, J. M., Singer, E., &amp; Tourangeau, R. (2009). <em>Survey methodology</em> (Vol. 561). John Wiley &amp; Sons.
</div>
<div id="ref-hullman2022worst" class="csl-entry" role="listitem">
Hullman, J., Kapoor, S., Nanayakkara, P., Gelman, A., &amp; Narayanan, A. (2022). The worst of both worlds: A comparative analysis of errors in learning from data in psychology and machine learning. <em>Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society</em>, 335–348.
</div>
<div id="ref-jacobs2021measurement" class="csl-entry" role="listitem">
Jacobs, A. Z., &amp; Wallach, H. (2021). Measurement and fairness. <em>Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency</em>, 375–385.
</div>
<div id="ref-liem2018psychology" class="csl-entry" role="listitem">
Liem, C. C., Langer, M., Demetriou, A., Hiemstra, A. M., Sukma Wicaksana, A., Born, M. P., &amp; König, C. J. (2018). Psychology meets machine learning: Interdisciplinary perspectives on algorithmic job candidate screening. <em>Explainable and Interpretable Models in Computer Vision and Machine Learning</em>, 197–253.
</div>
<div id="ref-lin2014microsoft" class="csl-entry" role="listitem">
Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P., &amp; Zitnick, C. L. (2014). Microsoft coco: Common objects in context. <em>Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part v 13</em>, 740–755.
</div>
<div id="ref-muller2021designing" class="csl-entry" role="listitem">
Muller, M., Wolf, C. T., Andres, J., Desmond, M., Joshi, N. N., Ashktorab, Z., Sharma, A., Brimijoin, K., Pan, Q., Duesterwald, E., et al. (2021). Designing ground truth and the social life of labels. <em>Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems</em>, 1–16.
</div>
<div id="ref-PustejovskyStubbsNLannotation" class="csl-entry" role="listitem">
Pustejovsky, J., &amp; Stubbs, A. (2013). <em><span class="nocase">Natural language annotation for machine learning </span></em> (First Edition). O’Reilly Media.
</div>
<div id="ref-sambasivan2021everyone" class="csl-entry" role="listitem">
Sambasivan, N., Kapania, S., Highfill, H., Akrong, D., Paritosh, P., &amp; Aroyo, L. M. (2021). <span>“Everyone wants to do the model work, not the data work”</span>: Data cascades in high-stakes AI. <em>Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems</em>, 1–15.
</div>
<div id="ref-sandri2023don" class="csl-entry" role="listitem">
Sandri, M., Leonardelli, E., Tonelli, S., &amp; Ježek, E. (2023). Why don’t you do it right? Analysing annotators’ disagreement in subjective tasks. <em>Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics</em>, 2428–2441.
</div>
<div id="ref-sav2023annotation" class="csl-entry" role="listitem">
Sav, A.-G., Demetriou, A. M., &amp; Liem, C. C. (2023). Annotation practices in societally impactful machine learning applications: What are popular recommender systems models actually trained on? <em>Perspectives@ RecSys</em>.
</div>
<div id="ref-welty2019metrology" class="csl-entry" role="listitem">
Welty, C., Paritosh, P., &amp; Aroyo, L. (2019). Metrology for AI: From benchmarks to instruments. <em>arXiv Preprint arXiv:1911.01875</em>.
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>https://en.wikipedia.org/wiki/Scientific_modelling<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>https://en.wikipedia.org/wiki/Artificial_intelligence<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>https://en.wikipedia.org/wiki/Statistical_parameter<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>https://en.wikipedia.org/wiki/Algorithm<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>https://www.youtube.com/watch?v=06-AZXmwHjo<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>https://pdai.info/<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>