[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Prospective Ground-Truthing:",
    "section": "",
    "text": "In a 2006 marketing conference, Clive Humby stated “Data is the new oil”. He was positing that, like crude oil, data must be refined in order to gain value, in the form of analysis (Palmer, 2006). The suggestion then was that, by analyzing data, one gains insights which have value. Rephrased, the suggestion is that, data can be made useful, of processed correctly once it’s been collected:\nAI systems take this value one step further, automating tasks - pieces of work typically carried out by people - and are increasingly applied broadly in high-stakes environments. In a poll of IT professionals across countries and sectors, 42% globally, with 18% in the government sector and 25% in the healthcare industry responding that their organization had already deployed AI, and a further 40% globally, with 49% in the government and 47% in the healthcare industry responding that their organization was exploring AI use 1.\nAI systems show issues with fairness - “absence of any prejudice or favoritism toward an individual or group based on their inherent or acquired characteristics” (Mehrabi et al., 2021). One group of contributors are related to bias in the data used to train or evaluate AI systems (Mavrogiorgos et al., 2024; Mehrabi et al., 2021). Solutions thus far often focus on fixing bias once it’s collected (Mavrogiorgos et al., 2024) - essentially treating data like crude oil, with a focus on refinement.\nThis thesis argues that data used for AI systems requires more than refinement, but rather design prior to collection. Essentially, current demands for data require not only refinement, but research and design prior to collection to determine how best to gather data for the given usecase - coined Prospective Ground-Truthing.\nThough works report on design principles drawn from established practices from relevant fields, these have yet to be broadly adopted. The thesis synthesizes design principles from the social sciences with principles from the computational sciences, for the purposes of collecting data to be used in AI systems. It reports on a case-study aimed at applying these principles. Following the case study, this thesis notes the additional effort and cost in approaching data collection this way, and presents a suggestion for a change in infrastructure to support more modern approaches to data collection for AI systems."
  },
  {
    "objectID": "index.html#issues-using-human-annotations-in-ml",
    "href": "index.html#issues-using-human-annotations-in-ml",
    "title": "Advancing Perspectivist Ground Truthing with Social Science",
    "section": "2.3 Issues using human annotations in ML",
    "text": "2.3 Issues using human annotations in ML\n\n2.3.1 Human annotations aren’t always accurate\nGriffin & Brenner (2004) review errors and biases in human judgements7\n\nover/under prediction: confidence score is higher/lower than accuracy\nover/under extremity: confidence is more extreme at ends\n\nalso reviews possible reasons:\n\noptimistic overconfidence\nconfirmation bias\ncase-based judgment\necological probability\nerror model (psychometric model)\n\n\n\n2.3.2 ML treats all annotation variance as noise rather than signal\ndisagreement is common\nreviewed in Cabitza et al. (2023): -social media content: Chandrasekharan 2017 -medical cases: Cabitza 2019 -various NLP tasks Aroyo & Welty (2015)\ndisagreement is often removed\n\nadjusting annotator training and instruction\nadjusting annotations via discussion post-collection\nmajority voting, post-hoc without annotators\n\nBeyond errors in judgment are questions about the target for the annotations. For at least some phenomena, the assumption that there is a single ground-truth to approximate with annotations doesn’t hold.\nAroyo & Welty (2015)\n\nIn order to evaluate ML / AI systems, we compare the output of these systems to reference data.\nOne method for creating reference data is the collection of human annotations.\nThis method typically assumes that, for every piece of content being annotated, there is a single canonical truth\nquality of annotations is assessed using inter-annotator agreement, where more agreement = better annotations\n\nAroyo & Welty (2015)\n7 ‘myths’ of human annotation:\n\nthere is one truth\ndisagreement is bad\ndetailed guidelines help\nexperts are better\none annotator is enough\nall items are created equal\nonce done, forever valid\n\nFor myths 1 and 2:\n\nlist examples from NLP where the disagreement from annotators is sensible\nthey argue that the assumptions of a single ground truth, and that disagreement is indicative of poor annotations are both false.\n\nfor myth 6: disagreement indicates that the media being rated is ambiguous.\n\n2.3.2.1 annotations aim to measure a latent variable\nJacobs & Wallach (2021) there is a ‘measurement error model’ (taken from econ) that links the unobservable latent variable, and observable properties. in annotations this is via individual observations\nalthough paper focuses on attempts at measuring constructs (risk of recidivism, teacher effectiveness, patient benefit) they also show that even ‘representational measurements’ like height, are essentially a latent variable\nAroyo & Welty (2015) operationalize ‘crowd truth’ with an illustration where the ‘gold standard’ is the probability that a sentence contains an element, based on the probability that an annotator annotated that sentence with that element.\n\ni.e. the label isn’t represented as ‘present’ or ‘not present’, but as a probablility\nthus the ‘crowd truth’ attempts to capture the ‘range of reasonable interpretations’\n\nBeck et al. (2022): we should expect more variance to the degree that tasks measure opinion show work on an intuitively perspective-based use-case: hate speech\n\n\n\n2.3.3 Inadequate reporting\nGeiger et al. (2021) ML science studies inadequately report ‘ground truth’\nHullman et al. (2022) thus we cannot know what data generating process the resulting model represents\n[perhaps cat image parable here?]\n\n\n2.3.4 sampling and measurement biases\nHullman et al. (2022)\nWith regards to reference data:\n\nrepresentation bias / non-representative samples\nmeasurement bias / unvalidated measurement instruments\nunderspecification of portions of input space in training data\ntransformation of data to optimize for ‘accuracy’\nlack of or poor dataset documentation\n\nIn other words, optimizing for predictive accuracy using very large datasets does not ‘absolve’ researchers from having to consider the data generating process. They note benefits that both machine learning and psychology could gain by borrowing methods from each other, but note the danger if these are misused. For the benefit of machine learning, there are lessons to be learned from social science, and the replication crisis. Among them are 1) collecting samples whose test/evaluation set distributions are drawn from the same deployment distribution, and 2) using valid measurement instruments.\n\n\n2.3.5 ML doesn’t treat annotation generating process as an instrument\nBeck et al. (2022)\n\nannotation collection requires design thinking\n\nTask Structure: specific wording and response options, including debates over the inclusion of “I don’t Know” option\nOrder Effects: specific judgements are affected by previous perceptions\nAnnotator Effects: backgrounds, opinions, experiences of respondents affect responses\n\n\nJacobs & Wallach (2021)\n\nreliability: do similar inputs to a measurement model present similar outputs?\n\ntest-retest: are measurements of an unobservable latent construct taken at different times via a measurement model similar, assuming the construct hasn’t changed?\n\nvalidity: is it ‘right’?\n\nno single test for validity on purpose, because it requires thinking. do our measurements:\n\nface validity: look plausible/ sensible?\ncontent validity: capture the construct?\n\nstructural validity: show the inter-correlations we expect?\nsubstantive validity: capture only observable properties thought to be related to the construct?\n\nconvergent validity: show correlations with other validated methods?\ndiscriminant validity: show correlations with other construct/properties thought not to be related to the construct?\npredictive validity: show correlations with constructs/properties thought to be related, but not in the operationalization?\nhypothesis validity: shed light on relevant hypotheses about the construct being measured?\nconsequential validity: allow for the consequences obtained from the measurement model to be assessed?\n\n\n\n2.3.6 ML ignores perspectives of annotators\nCabitza et al. (2023): whether the target of the annotation is a subjective phenomenon or not, disagreement is always irreducible. Yet ML typically assumes there is a single ‘ground truth’, and its best indicator is inter-annotator agreement. But taking the perspectives of the annotators into account, both in the data annotation but also the modelling phase of ML projects has recently been shown to benefit ML modelling in a number of contexts.\nweak perspectivist approach: taking perspectives into account while designing and collecting annotations, but ultimately reducing annotations to a single label or rating.\nstrong perspectivist approach: taking perspectives into account for ground truthing and modelling phases.\nbenefits of this approach:\n\nis congruent with the reality of collecting annotations\nincludes the signal in the variance of labels or ratings\navoids majority group perspective appearing to be ‘objective’\nallows for the modelling of human errors and variances\nallows for uncertain, fuzzy, or soft model development\nmore complete report of the data generating process, as it also reports uncertainty\n\ndownsides:\n\nmultiple raters, and therefore costs/time/rater availability are issues\nneed for perspectivist ML approaches\nvalidation becomes more challenging\n\nrecommendations:\n\ncomplete labeling schemes, including ‘i don’t know’, ‘none of these’ etc. categories, and the ability to express issues with label set\nenough raters\nheterogenous raters\nadequate reporting:\n\nnumber of raters,\nrater expertise\nincentive\ninstructions\nlength of time for labelling\ninter rater agreement\naggregation method\nconfidence"
  },
  {
    "objectID": "index.html#present-work-1",
    "href": "index.html#present-work-1",
    "title": "Advancing Perspectivist Ground Truthing with Social Science",
    "section": "5.1 Present Work",
    "text": "5.1 Present Work\nThis thesis incorporates techniques and considerations from the social sciences to address the aforementioned shortcomings.\nshowcases a design for a challenging ground-truthing project, in terms of the complexity of the phenomenon of interest, ambiguity in the media that selected and annotated. It incorporates design choices to address the aforementioned shortcomings into a singular framework, guided by best practices in the social sciences, which it then extends. It focuses unambiguously on the aspect most relevant to the\nSpecifically:\n\nWe attempt to mitigate representation biases in the content we select for annotation by using a stratified sampling strategy.\nWe attempt to mitigate measurement biases by treating the target measurement as a latent variable, and the survey we used to gather annotations as an instrument. We build on work that validated a questionnaire for measuring constructs, and estimating its reliability and structural validity when used for annotations.\nWe account for the potential of multiple perspectives in our dataset by recruiting participants from relevant subgroups in a single target population.\nWe report the details of the annotation collection process, and share the disaggregated dataset of the annotations\nWe further show how to estimate the number of annotators\n\n\nWe demonstrate the potential of this framework by grounding a complex phenomenon (a 10-dimensional construct, Personal Values) in ambiguous text (song lyrics). We further show an\nWhen sampling content to include in training/test datasets, samples for the training/test sets will ideally be drawn from the same distribution as the content in which they will eventually be deployed. Representation bias in content selected for training and/or evaluation datasets refers to the degree to which relevant distributions in data used to train and/or evaluate systems resembles the distribution in the environment to which it will be deployed (Hullman et al., 2022).\nMeasurement bias in the annotations collected from humans may also bias\nPerspectivism\nReporting\nWe add: a priori rating number estimation\nAnd although imperfect as leaderboard scores can be gamed, and do not perfectly represent the deployment environment, the typical leaderboard approach has shown evidence that progress can be made towards a target. This thesis thus represents an attempt to define the target better."
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Prospective Ground-Truthing:",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://pdai.info/↩︎\nhttps://en.wikipedia.org/wiki/Scientific_modelling↩︎\nhttps://en.wikipedia.org/wiki/Artificial_intelligence↩︎\nhttps://en.wikipedia.org/wiki/Statistical_parameter↩︎\nhttps://en.wikipedia.org/wiki/Algorithm↩︎\nhttps://www.youtube.com/watch?v=06-AZXmwHjo↩︎\nhttps://pdai.info/↩︎"
  },
  {
    "objectID": "index.html#measurement",
    "href": "index.html#measurement",
    "title": "Advancing Perspectivist Ground Truthing with Social Science",
    "section": "",
    "text": "An implication of the relationship between dataset quality and the resulting system, is that the evaluation of an AI system is a measurement problem, as is the measurement of the phenomenon of interest in the media via the human input being collected (Welty et al., 2019). To appropriately evaluate a system designed to do a task, both the pieces of media and the distribution of the phenomenon of interest in the data it is evaluated on, and possibly also trained on, must resemble those of the environment in which it is deployed(Hullman et al., 2022), even though the true values of these is unknowable (Welty et al., 2019). Crucially and often ignored in Machine Learning, the resulting inputs from humans must also show qualities fitting of good measurements (Jacobs & Wallach, 2021; Welty et al., 2019), and account for the possibility of a range of reasonable interpretations (Aroyo & Welty, 2015; Cabitza et al., 2023).\nThe social sciences have developed sampling methods to represent populations (Groves et al., 2009), and syntheses that introduce knowledge from survey science (Beck et al., 2022), metrology(Welty et al., 2019), psychometrics(Jacobs & Wallach, 2021), and the perspective approach to ground-truthing(Cabitza et al., 2023) have been published. Yet, knowledge from these fields has broadly not been applied in the field of Machine Learning, leading to issues of representation and measurement(Hullman et al., 2022).\nPresent work attempts to demonstrate the potential of synthesizing knowledge from the social sciences related to sampling and measurement with extant work on data sets for use in machine learning. Building on prior work reviewed herein, its main contribution is a case study investigation, spanning several manuscripts, of a complex evaluation data set creation problem. Included in this thesis are two manuscripts that further motivate the case study: the first reviews strengths and weaknesses of datasets used in the field of Recommender Systems, the second reviews poor practices in Signal Processing related to datasets whose interconnections were poorly reported, misleading results. The case study demonstrates the use of principles from the Social Sciences to solve problems of representation and measurement: personal values in text. It spans two forms of text, expected to vary in terms of use of ambiguous language: song lyrics and political speeches, and demonstrates a moderate success in automatically estimating values in lyrics and a failure in speeches. It includes recommendations for analyses to observe the potential for success or failure, and to estimate cost via less expensive pilot studies. This thesis follows the case study highlighting the potential for shortcomings in the interpretation of AI system evaluations should a more sophisticated framework for evaluation not be adopted, and highlighting"
  },
  {
    "objectID": "index.html#measurement-in-evaluation",
    "href": "index.html#measurement-in-evaluation",
    "title": "Advancing Perspectivist Ground Truthing with Social Science",
    "section": "",
    "text": "An implication of the relationship between dataset quality and the resulting system, is that the evaluation of an AI system is a measurement problem, as is the measurement of the phenomenon of interest in the media via the human input being collected (Welty et al., 2019). To appropriately evaluate a system designed to do a task, both the pieces of media and the distribution of the phenomenon of interest in the data it is evaluated on, and possibly also trained on, must resemble those of the environment in which it is deployed(Hullman et al., 2022), even though the true values of these is unknowable (Welty et al., 2019). Crucially and often ignored in Machine Learning, the resulting inputs from humans must also show qualities fitting of good measurements (Jacobs & Wallach, 2021; Welty et al., 2019), and account for the possibility of a range of reasonable interpretations (Aroyo & Welty, 2015; Cabitza et al., 2023).\nThe social sciences have developed sampling methods to represent populations (Groves et al., 2009), and syntheses that introduce knowledge from survey science (Beck et al., 2022), metrology(Welty et al., 2019), psychometrics(Jacobs & Wallach, 2021), and the perspective approach to ground-truthing(Cabitza et al., 2023) have been published. Yet, knowledge from these fields has broadly not been applied in the field of Machine Learning, leading to issues of representation and measurement(Hullman et al., 2022).\nPresent work attempts to demonstrate the potential of synthesizing knowledge from the social sciences related to sampling and measurement with extant work on data sets for use in machine learning. Building on prior work reviewed herein, its main contribution is a case study investigation, spanning several manuscripts, of a complex evaluation data set creation problem. Included in this thesis are two manuscripts that further motivate the case study: the first reviews strengths and weaknesses of datasets used in the field of Recommender Systems, the second reviews poor practices in Signal Processing related to datasets whose interconnections were poorly reported, misleading results. The case study demonstrates the use of principles from the Social Sciences to solve problems of representation and measurement: personal values in text. It spans two forms of text, expected to vary in terms of use of ambiguous language: song lyrics and political speeches, and demonstrates a moderate success in automatically estimating values in lyrics and a failure in speeches. It includes recommendations for analyses to observe the potential for success or failure, and to estimate cost via less expensive pilot studies. This thesis follows the case study highlighting the potential for shortcomings in the interpretation of AI system evaluations should a more sophisticated framework for evaluation not be adopted, and highlighting"
  },
  {
    "objectID": "index.html#present-work",
    "href": "index.html#present-work",
    "title": "Prospective Ground-Truthing:",
    "section": "1.1 Present Work",
    "text": "1.1 Present Work\nPresent work firstly attempts to synthesize input from various fields on how to better gather reference data to be used the ‘ground truth’ for the models that underlie AI systems. Present work then attempts to demonstrate the potential of synthesizing knowledge from the social sciences (psychometrics, survey science) related to sampling and measurement, with extant work on more useful content collection for use in machine learning tasks (metrology, perspectivist ground-truthing). Building on prior work reviewed herein, its main contribution is a synthesized framework that can be used to ground challenging phenomena in various media, following principles from prior work. A secondary contribution is a case study spanning several manuscripts, of a complex evaluation data set creation project. A third contribution, is knowledge directly applicable to the grounding of personal values in text, such as our annotation procedure, analysis of reference data, and statistics of interest for planning and estimating the costs associated. A final contribution is immediately applicable results that work towards estimating personal values in song lyrics using language models.\nIncluded in this thesis are two manuscripts that further motivate the case study: 1) the first reviews strengths and weaknesses of datasets used in the field of Recommender Systems, 2) the second reviews how poor data practices in the field of Signal Processing related to datasets whose interconnections were poorly reported, misleading results. The case study demonstrates the use of principles from the Social Sciences to solve problems of representation and measurement across 4 manuscripts: personal values in song lyrics. In a 5th manuscript, the same principles are applied to a second form of text, political speeches, expected to vary in terms of use of ambiguous language. Despite the moderate success in automatically estimating values in lyrics, this work demonstrates a failure with speeches. It includes recommendations for analyses to observe the potential for success or failure, and to estimate cost via less expensive pilot studies. This thesis follows the case study with work that 6) highlights the potential for shortcomings in the interpretation of AI system evaluations should a more epistemologically sophisticated framework for evaluation not be adopted, and 7) highlights an important component of scientific infrastructure needed for rigorous work on data sets for Machine Learning: the treatment of scientific work as open-source artifacts."
  },
  {
    "objectID": "index.html#present-work-2",
    "href": "index.html#present-work-2",
    "title": "Advancing Perspectivist Ground Truthing with Social Science",
    "section": "4.1 Present Work",
    "text": "4.1 Present Work\nThis thesis incorporates techniques and considerations from the social sciences to address the aforementioned shortcomings.\nshowcases a design for a challenging ground-truthing project, in terms of the complexity of the phenomenon of interest, ambiguity in the media that selected and annotated. It incorporates design choices to address the aforementioned shortcomings into a singular framework, guided by best practices in the social sciences, which it then extends. It focuses unambiguously on the aspect most relevant to the\nSpecifically:\n\nWe attempt to mitigate representation biases in the content we select for annotation by using a stratified sampling strategy.\nWe attempt to mitigate measurement biases by treating the target measurement as a latent variable, and the survey we used to gather annotations as an instrument. We build on work that validated a questionnaire for measuring constructs, and estimating its reliability and structural validity when used for annotations.\nWe account for the potential of multiple perspectives in our dataset by recruiting participants from relevant subgroups in a single target population.\nWe report the details of the annotation collection process, and share the disaggregated dataset of the annotations\nWe further show how to estimate the number of annotators\n\n\nWe demonstrate the potential of this framework by grounding a complex phenomenon (a 10-dimensional construct, Personal Values) in ambiguous text (song lyrics). We further show an\nWhen sampling content to include in training/test datasets, samples for the training/test sets will ideally be drawn from the same distribution as the content in which they will eventually be deployed. Representation bias in content selected for training and/or evaluation datasets refers to the degree to which relevant distributions in data used to train and/or evaluate systems resembles the distribution in the environment to which it will be deployed (Hullman et al., 2022).\nMeasurement bias in the annotations collected from humans may also bias\nPerspectivism\nReporting\nWe add: a priori rating number estimation\nAnd although imperfect as leaderboard scores can be gamed, and do not perfectly represent the deployment environment, the typical leaderboard approach has shown evidence that progress can be made towards a target. This thesis thus represents an attempt to define the target better."
  },
  {
    "objectID": "index.html#ai-relies-on-reference-data-from-human-annotations",
    "href": "index.html#ai-relies-on-reference-data-from-human-annotations",
    "title": "Advancing Perspectivist Ground Truthing with Social Science",
    "section": "2.1 AI relies on reference data from human annotations",
    "text": "2.1 AI relies on reference data from human annotations\nReference data uses responses from humans in the form of annotations very often (Geiger et al., 2021): “data annotation is the practice of labeling a set of digital representations of objects.” (Cabitza et al., 2023). Geiger et al. (2021) systematically review 200 randomly sampled papers from 3 broad domains, Social Sciences & Humanities, Life & Biomedical Sciences and Physical & Environmental Sciences, using a content-analysis approach. 70% (or 140 papers) were classification tasks, of which 73.05% (or 103 papers) used labels derived from human responses as the reference.\nOften multiple ratings per piece of content are collected and aggregated, forming a singular ‘ground truth’ for the aspect of the content being labelled or rated. Thus, it is assumed that there is a singular canonical truth for each aspect / content pair, comprised of aggregated human responses, which forms a target to which we align our automated systems.\nAn investigation of 15 data science workers, Muller et al. (2021) observed common phases, which include determining the annotation scheme - all possible labels that can be attributed to digital representations of objects along with any relevant guidelines, the actual process of collecting labels, and the process by which the annotations are then aggregated into a single label. They note the difficulty of this work: issues in the annotation schemes are often discovered as annotation projects progress, requiring varying degrees of improvised adjustment.\nFurthermore, data sets are often re-used: 56.31% (or 58 papers) used only ‘external’ human labels, i.e. labels that were not collected specifically for the work in the paper (Geiger et al., 2021). Hullman et al. (2022) compare claims that ML is facing a reproducibility crisis to the crisis in psychology. Among the issues they note relate to benchmark datasets, which researchers often re-use as they publish on standardized benchmarks, and because they are cost prohibitive to collect.\nGriffin & Brenner (2004) review errors and biases in human judgements5"
  },
  {
    "objectID": "index.html#reference-data-comes-from-humans",
    "href": "index.html#reference-data-comes-from-humans",
    "title": "Prospective Ground-Truthing:",
    "section": "2.1 Reference data comes from humans",
    "text": "2.1 Reference data comes from humans\nAI runs on data generated by humans. Reference data often uses responses from humans in the form of labels or annotations of content. “[D]ata annotation is the practice of labeling a set of digital representations of objects” (Cabitza et al., 2023). Although few studies have systematically examined the frequency of use of data from humans, it has been shown that reference data includes human input either explicitly or implicitly very often. Geiger et al. (2021) systematically review 200 randomly sampled papers from 3 broad domains, Social Sciences & Humanities, Life & Biomedical Sciences and Physical & Environmental Sciences. Out of the 140 studies that were classification tasks, 73.05% (or 103 papers) used labels derived from human responses as the reference. Geiger et al. (2020) reviewed 164 papers whose classifiers were trained on Twitter data and observed that 65% of the works reviewed used human annotations for the purposes of training. They further note that this quantity did not include human annotations used for validation, or other meta-data e.g. hashtags contributed by humans. In some domains the contribution of humans is in the form of digital traces, as in the domain of Recommender Systems where it was observed that, out of the most highly cited papers between 2018 and 2022, 86% of the datasets used were transaction data released by vendors such as Amazon or Yelp (Sav et al., 2023). Whether human input is explicit or implicit, it is present in almost all reference data.\nFurthermore, training/reference data sets are often re-used, in some cases treated as benchmarks - measurement instruments used to produce comparable quantitative assessments of models (Welty et al., 2019) - magnifying their impact. Geiger et al. (2021) observed that 56.31% of the classification tasks that were reviewed (or 58 papers) used only ‘external’ human labels, i.e. labels that were not collected specifically for the work in the paper, Geiger et al. (2020) observed that 33.3% of the papers used external annotations, and Sav et al. (2023) observed that just 4 data sets appeared in at least 10% of works reviewed, with the most commonly used data set appearing in 33% of the works reviewed. Examining the most highly cited papers in IEEE CVPR from 2020-2022, the initial papers announcing the benchmark, along with the training and reference data received citation counts in the tens of thousands: Imagenet (Deng et al., 2009) shows over 52k citations, COCO (Lin et al., 2014) shows over 29k, Pascal VOC (Everingham et al., 2010) shows over 15k, according to SCOPUS as of April 2025. Thus, these human input data sets have the potential for long lasting effects on work that follows.\nAlthough far more emphasis is placed on whether models achieve state of the art ‘performance’ or efficiency (Birhane et al., 2022), scholars over the past decade have attempted to draw attention to a lack of sophistication in how training and reference data are selected and evaluated (Aroyo & Welty, 2015). It has been argued that a focus on improving the data for a given task, will result in bigger gains than a focus on improving model6. Importantly, Hullman et al. (2022) show that optimizing for predictive accuracy does not absolve researchers from shortcomings in reference/training data, a situation exacerbated by the often re-use of data sets. A solution rather entails acknowledging that, whether deliberate or not, informed or not, organized or improvised, data are generated by process that may or may not be deliberately designed (Muller et al., 2021), yet would greatly benefit from design. Building on this, Welty et al. (2019) argue that datasets used to evaluate AI systems should be treated as measurement instruments in their own right. Drawing on the science of metrology, they propose that benchmark datasets ought to be evaluated using criteria analogous to those used for physical measurement tools."
  },
  {
    "objectID": "index.html#the-data-work-is-design-work",
    "href": "index.html#the-data-work-is-design-work",
    "title": "Advancing Perspectivist Ground Truthing with Social Science",
    "section": "2.2 The Data Work is Design Work",
    "text": "2.2 The Data Work is Design Work\nConsidering the ever-presence of human influence on the reference data, best practices, considerations, and frameworks from the social sciences could inform designs, but have yet to be broadly applied in the computational sciences (Beck et al., 2022; Jacobs & Wallach, 2021). One reason for this gap may be ML researchers prefer to work on building systems and evaluating their performance rather than researching, designing and executing ground-truthing projects (Muller et al., 2021; Sambasivan et al., 2021). Another may be a lack of focus on these topics in textbooks, and thus in education more broadly (Geiger et al., 2020). A third may be that the social and computational sciences have conceptually different focci: the computational sciences focus on the statistical model the system with substantially less emphasis on the content, whereas the social sciences treat the statistical model as a means to better understanding the relationships in the content (Liem et al., 2018). Psychology research thus contains many more research projects in which datasets are collected using responses from people, whereas datasets tend to be re-used extensively in machine learning work (Geiger et al., 2021). A further more practical complication is that work on these topics and potential solutions lacks a central academic ‘home’: where psychology and economics have psychometrics and econometrics respectively, and where software engineering has software testing, the study of ground-truthing lacks a central banner under which academic work can accumulate and disseminate.\nCommonly observed shortcomings include 1) representational biases in the content sampled for inclusion in training/evaluation datasets (Hullman et al., 2022), 2) measurement biases in the annotations collected (Beck et al., 2022; Hullman et al., 2022; Jacobs & Wallach, 2021), 3) a fallacious assumption of a single canonical ‘ground-truth’ (Aroyo & Welty, 2015; Cabitza et al., 2023), and 4) poor reporting of necessary information regarding the annotation-collection process (Geiger et al., 2021; Hullman et al., 2022). An additional consideration that receives little attention is the number of annotations to gather, where fields that focus on gathering data from humans also have a strong emphasis on a-priori decisions to mitigate bias, such as the pre-registration of calculated of target sample sizes estimated via statistical power analysis (Cohen, 1992). These considerations are absent in computational fields which appear to favor differing rules of thumb: e.g. in a well-cited textbook, Pustejovsky & Stubbs (2013) suggest to “have your corpus annotated by at least two people (more is preferable, but not always practical)”, whereas Artstein & Poesio (2008) suggest that “measuring reliability with only two coders is seldom enough, except for small-scale studies”.\nDecisions such as the selection of items for training data (Hullman et al., 2022), and the collection of human responses for reference data (Beck et al., 2022), are part of a design of a process that results in data. On the one hand, corpora tend to be very large, and resources are finite making cost a primary factor in design decisions (Muller et al., 2021). On the other hand, rules of thumb lack clear substantiation in light of the both 1) the phenomenon being grounded and 2) the ambiguity of the media in which it is grounded. In other words, more variance is expected in annotation targets to the degree they are subjective or based on opinion (Beck et al., 2022), and more variance is expected in content to the degree to which it is ambiguous - i.e. can be interpreted in multiple ways - such as figurative language (Sandri et al., 2023). Further, some degree of variance will always be present when there are multiple annotations or ratings for a given piece of media independent of the target (cabitza?), based on the range of reasonable interpretations of that target in that media (arroyo?)."
  },
  {
    "objectID": "index.html#human-annotations-arent-always-accurate",
    "href": "index.html#human-annotations-arent-always-accurate",
    "title": "Advancing Perspectivist Ground Truthing with Social Science",
    "section": "3.1 Human annotations aren’t always accurate",
    "text": "3.1 Human annotations aren’t always accurate\nGriffin & Brenner (2004) review errors and biases in human judgements6\nGriffin & Brenner (2004) review errors and biases in human judgements7\n\nover/under prediction: confidence score is higher/lower than accuracy\nover/under extremity: confidence is more extreme at ends\n\nalso reviews possible reasons:\n\noptimistic overconfidence\nconfirmation bias\ncase-based judgment\necological probability\nerror model (psychometric model)"
  },
  {
    "objectID": "index.html#ml-treats-all-annotation-variance-as-noise-rather-than-signal",
    "href": "index.html#ml-treats-all-annotation-variance-as-noise-rather-than-signal",
    "title": "Advancing Perspectivist Ground Truthing with Social Science",
    "section": "2.3 ML treats all annotation variance as noise rather than signal",
    "text": "2.3 ML treats all annotation variance as noise rather than signal\nOften multiple ratings per piece of content are collected and aggregated, forming a singular ‘ground truth’ for the aspect of the content being labelled or rated. Thus, it is assumed that there is a singular canonical truth for each aspect / content pair, comprised of aggregated human responses, which forms a target to which we align our automated systems.\ndisagreement is common\nreviewed in Cabitza et al. (2023): -social media content: Chandrasekharan 2017 -medical cases: Cabitza 2019 -various NLP tasks Aroyo & Welty (2015)\ndisagreement is often removed\n\nadjusting annotator training and instruction\nadjusting annotations via discussion post-collection\nmajority voting, post-hoc without annotators\n\nBeyond errors in judgment are questions about the target for the annotations. For at least some phenomena, the assumption that there is a single ground-truth to approximate with annotations doesn’t hold.\nAroyo & Welty (2015)\n\nIn order to evaluate ML / AI systems, we compare the output of these systems to reference data.\nOne method for creating reference data is the collection of human annotations.\nThis method typically assumes that, for every piece of content being annotated, there is a single canonical truth\nquality of annotations is assessed using inter-annotator agreement, where more agreement = better annotations\n\nAroyo & Welty (2015)\n7 ‘myths’ of human annotation:\n\nthere is one truth\ndisagreement is bad\ndetailed guidelines help\nexperts are better\none annotator is enough\nall items are created equal\nonce done, forever valid\n\nFor myths 1 and 2:\n\nlist examples from NLP where the disagreement from annotators is sensible\nthey argue that the assumptions of a single ground truth, and that disagreement is indicative of poor annotations are both false.\n\nfor myth 6: disagreement indicates that the media being rated is ambiguous."
  },
  {
    "objectID": "index.html#annotations-aim-to-measure-a-latent-variable",
    "href": "index.html#annotations-aim-to-measure-a-latent-variable",
    "title": "Advancing Perspectivist Ground Truthing with Social Science",
    "section": "2.4 annotations aim to measure a latent variable",
    "text": "2.4 annotations aim to measure a latent variable\nJacobs & Wallach (2021) there is a ‘measurement error model’ (taken from econ) that links the unobservable latent variable, and observable properties. in annotations this is via individual observations\nalthough paper focuses on attempts at measuring constructs (risk of recidivism, teacher effectiveness, patient benefit) they also show that even ‘representational measurements’ like height, are essentially a latent variable\nAroyo & Welty (2015) operationalize ‘crowd truth’ with an illustration where the ‘gold standard’ is the probability that a sentence contains an element, based on the probability that an annotator annotated that sentence with that element.\n\ni.e. the label isn’t represented as ‘present’ or ‘not present’, but as a probablility\nthus the ‘crowd truth’ attempts to capture the ‘range of reasonable interpretations’\n\nBeck et al. (2022): we should expect more variance to the degree that tasks measure opinion show work on an intuitively perspective-based use-case: hate speech"
  },
  {
    "objectID": "index.html#inadequate-reporting",
    "href": "index.html#inadequate-reporting",
    "title": "Advancing Perspectivist Ground Truthing with Social Science",
    "section": "2.5 Inadequate reporting",
    "text": "2.5 Inadequate reporting\nGeiger et al. (2021) ML science studies inadequately report ‘ground truth’\nHullman et al. (2022) thus we cannot know what data generating process the resulting model represents\n[perhaps cat image parable here?]\nAn investigation of 15 data science workers, Muller et al. (2021) observed common phases, which include determining the annotation scheme - all possible labels that can be attributed to digital representations of objects along with any relevant guidelines, the actual process of collecting labels, and the process by which the annotations are then aggregated into a single label. They note the difficulty of this work: issues in the annotation schemes are often discovered as annotation projects progress, requiring varying degrees of improvised adjustment."
  },
  {
    "objectID": "index.html#sampling-and-measurement-biases",
    "href": "index.html#sampling-and-measurement-biases",
    "title": "Advancing Perspectivist Ground Truthing with Social Science",
    "section": "2.6 sampling and measurement biases",
    "text": "2.6 sampling and measurement biases\nHullman et al. (2022)\nWith regards to reference data:\n\nrepresentation bias / non-representative samples\nmeasurement bias / unvalidated measurement instruments\nunderspecification of portions of input space in training data\ntransformation of data to optimize for ‘accuracy’\nlack of or poor dataset documentation\n\nIn other words, optimizing for predictive accuracy using very large datasets does not ‘absolve’ researchers from having to consider the data generating process. They note benefits that both machine learning and psychology could gain by borrowing methods from each other, but note the danger if these are misused. For the benefit of machine learning, there are lessons to be learned from social science, and the replication crisis. Among them are 1) collecting samples whose test/evaluation set distributions are drawn from the same deployment distribution, and 2) using valid measurement instruments."
  },
  {
    "objectID": "index.html#ml-doesnt-treat-annotation-generating-process-as-an-instrument",
    "href": "index.html#ml-doesnt-treat-annotation-generating-process-as-an-instrument",
    "title": "Advancing Perspectivist Ground Truthing with Social Science",
    "section": "2.7 ML doesn’t treat annotation generating process as an instrument",
    "text": "2.7 ML doesn’t treat annotation generating process as an instrument\nBeck et al. (2022)\n\nannotation collection requires design thinking\n\nTask Structure: specific wording and response options, including debates over the inclusion of “I don’t Know” option\nOrder Effects: specific judgements are affected by previous perceptions\nAnnotator Effects: backgrounds, opinions, experiences of respondents affect responses\n\n\nJacobs & Wallach (2021)\n\nreliability: do similar inputs to a measurement model present similar outputs?\n\ntest-retest: are measurements of an unobservable latent construct taken at different times via a measurement model similar, assuming the construct hasn’t changed?\n\nvalidity: is it ‘right’?\n\nno single test for validity on purpose, because it requires thinking. do our measurements:\n\nface validity: look plausible/ sensible?\ncontent validity: capture the construct?\n\nstructural validity: show the inter-correlations we expect?\nsubstantive validity: capture only observable properties thought to be related to the construct?\n\nconvergent validity: show correlations with other validated methods?\ndiscriminant validity: show correlations with other construct/properties thought not to be related to the construct?\npredictive validity: show correlations with constructs/properties thought to be related, but not in the operationalization?\nhypothesis validity: shed light on relevant hypotheses about the construct being measured?\nconsequential validity: allow for the consequences obtained from the measurement model to be assessed?"
  },
  {
    "objectID": "index.html#ml-ignores-perspectives-of-annotators",
    "href": "index.html#ml-ignores-perspectives-of-annotators",
    "title": "Advancing Perspectivist Ground Truthing with Social Science",
    "section": "2.8 ML ignores perspectives of annotators",
    "text": "2.8 ML ignores perspectives of annotators\nCabitza et al. (2023): whether the target of the annotation is a subjective phenomenon or not, disagreement is always irreducible. Yet ML typically assumes there is a single ‘ground truth’, and its best indicator is inter-annotator agreement. But taking the perspectives of the annotators into account, both in the data annotation but also the modelling phase of ML projects has recently been shown to benefit ML modelling in a number of contexts.\nweak perspectivist approach: taking perspectives into account while designing and collecting annotations, but ultimately reducing annotations to a single label or rating.\nstrong perspectivist approach: taking perspectives into account for ground truthing and modelling phases.\nbenefits of this approach:\n\nis congruent with the reality of collecting annotations\nincludes the signal in the variance of labels or ratings\navoids majority group perspective appearing to be ‘objective’\nallows for the modelling of human errors and variances\nallows for uncertain, fuzzy, or soft model development\nmore complete report of the data generating process, as it also reports uncertainty\n\ndownsides:\n\nmultiple raters, and therefore costs/time/rater availability are issues\nneed for perspectivist ML approaches\nvalidation becomes more challenging\n\nrecommendations:\n\ncomplete labeling schemes, including ‘i don’t know’, ‘none of these’ etc. categories, and the ability to express issues with label set\nenough raters\nheterogenous raters\nadequate reporting:\n\nnumber of raters,\nrater expertise\nincentive\ninstructions\nlength of time for labelling\ninter rater agreement\naggregation method\nconfidence"
  },
  {
    "objectID": "index.html#shortcomings-and-improvements-to-reference-data-design",
    "href": "index.html#shortcomings-and-improvements-to-reference-data-design",
    "title": "Advancing Perspectivist Ground Truthing with Social Science",
    "section": "2.2 Shortcomings and improvements to reference data design",
    "text": "2.2 Shortcomings and improvements to reference data design\nConsidering the ever-presence of human influence on the reference data, best practices, considerations, and frameworks from the social sciences could inform designs, but have yet to be broadly applied in the computational sciences (Beck et al., 2022; Jacobs & Wallach, 2021). One reason for this gap may be ML researchers prefer to work on building systems and evaluating their performance rather than researching, designing and executing ground-truthing projects (Muller et al., 2021; Sambasivan et al., 2021). Another may be a lack of focus on these topics in textbooks, and thus in education more broadly (Geiger et al., 2020). A third may be that the social and computational sciences have conceptually different focci: the computational sciences focus on the statistical model the system with substantially less emphasis on the content, whereas the social sciences treat the statistical model as a means to better understanding the relationships in the content (Liem et al., 2018). Psychology research thus contains many more research projects in which datasets are collected using responses from people, whereas datasets tend to be re-used extensively in machine learning work (Geiger et al., 2021). A further more practical complication is that work on these topics and potential solutions lacks a central academic ‘home’: where psychology and economics have psychometrics and econometrics respectively, and where software engineering has software testing, the study of ground-truthing lacks a central banner under which academic work can accumulate and disseminate.\nCommonly observed shortcomings include 1) representational biases in the content sampled for inclusion in training/evaluation datasets (Hullman et al., 2022), 2) measurement biases in the annotations collected (Beck et al., 2022; Hullman et al., 2022; Jacobs & Wallach, 2021), 3) a fallacious assumption of a single canonical ‘ground-truth’ (Aroyo & Welty, 2015; Cabitza et al., 2023), and 4) poor reporting of necessary information regarding the annotation-collection process (Geiger et al., 2021; Hullman et al., 2022). An additional consideration that receives little attention is the number of annotations to gather, where fields that focus on gathering data from humans also have a strong emphasis on a-priori decisions to mitigate bias, such as the pre-registration of calculated of target sample sizes estimated via statistical power analysis (Cohen, 1992). These considerations are absent in computational fields which appear to favor differing rules of thumb: e.g. in a well-cited textbook, Pustejovsky & Stubbs (2013) suggest to “have your corpus annotated by at least two people (more is preferable, but not always practical)”, whereas Artstein & Poesio (2008) suggest that “measuring reliability with only two coders is seldom enough, except for small-scale studies”.\nDecisions such as the selection of items for training data (Hullman et al., 2022), and the collection of human responses for reference data (Beck et al., 2022), are part of a design of a process that results in data. On the one hand, corpora tend to be very large, and resources are finite making cost a primary factor in design decisions (Muller et al., 2021). On the other hand, rules of thumb lack clear substantiation in light of the both 1) the phenomenon being grounded and 2) the ambiguity of the media in which it is grounded. In other words, more variance is expected in annotation targets to the degree they are subjective or based on opinion (Beck et al., 2022), and more variance is expected in content to the degree to which it is ambiguous - i.e. can be interpreted in multiple ways - such as figurative language (Sandri et al., 2023). Further, some degree of variance will always be present when there are multiple annotations or ratings for a given piece of media independent of the target (cabitza?), based on the range of reasonable interpretations of that target in that media (arroyo?)."
  },
  {
    "objectID": "index.html#a-tale-of-two-studies-the-case-of-personnel-selection",
    "href": "index.html#a-tale-of-two-studies-the-case-of-personnel-selection",
    "title": "Advancing Perspectivist Ground Truthing with Social Science",
    "section": "2.3 A tale of two studies: the case of personnel selection",
    "text": "2.3 A tale of two studies: the case of personnel selection\nPonce-López et al. (2016)\n\nefficient way to gather media and annotation data\nBUT no validation of instrument, or ecologically valid media data\ndistribution of training /eval data don’t come from the target distribution\n\n Figure 2: Ponce-López et al. (2016)\nCompared to Koutsoumpis et al. (2024):\n\necological validity: media data was mock asynchronous video interviews\necological validity: interview questions designed to activate personality facets\npersonality instruments: validated HEXACO scale\nperspectives: self & observer ratings\n\n\n2.3.1 1: representation biases\n\n\n2.3.2 2: measurement biases\n\n\n2.3.3 3. lack of perspectives\n\n\n2.3.4 4. poor reporting\n\n\n2.3.5 Human judgments are imperfect\nGriffin & Brenner (2004) review errors and biases in human judgements6\nGriffin & Brenner (2004) review errors and biases in human judgements7\n\nover/under prediction: confidence score is higher/lower than accuracy\nover/under extremity: confidence is more extreme at ends\n\nalso reviews possible reasons:\n\noptimistic overconfidence\nconfirmation bias\ncase-based judgment\necological probability\nerror model (psychometric model)\n\n\n\n2.3.6 ML treats all annotation variance as noise rather than signal\nOften multiple ratings per piece of content are collected and aggregated, forming a singular ‘ground truth’ for the aspect of the content being labelled or rated. Thus, it is assumed that there is a singular canonical truth for each aspect / content pair, comprised of aggregated human responses, which forms a target to which we align our automated systems.\ndisagreement is common\nreviewed in Cabitza et al. (2023): -social media content: Chandrasekharan 2017 -medical cases: Cabitza 2019 -various NLP tasks Aroyo & Welty (2015)\ndisagreement is often removed\n\nadjusting annotator training and instruction\nadjusting annotations via discussion post-collection\nmajority voting, post-hoc without annotators\n\nBeyond errors in judgment are questions about the target for the annotations. For at least some phenomena, the assumption that there is a single ground-truth to approximate with annotations doesn’t hold.\nAroyo & Welty (2015)\n\nIn order to evaluate ML / AI systems, we compare the output of these systems to reference data.\nOne method for creating reference data is the collection of human annotations.\nThis method typically assumes that, for every piece of content being annotated, there is a single canonical truth\nquality of annotations is assessed using inter-annotator agreement, where more agreement = better annotations\n\nAroyo & Welty (2015)\n7 ‘myths’ of human annotation:\n\nthere is one truth\ndisagreement is bad\ndetailed guidelines help\nexperts are better\none annotator is enough\nall items are created equal\nonce done, forever valid\n\nFor myths 1 and 2:\n\nlist examples from NLP where the disagreement from annotators is sensible\nthey argue that the assumptions of a single ground truth, and that disagreement is indicative of poor annotations are both false.\n\nfor myth 6: disagreement indicates that the media being rated is ambiguous.\n\n\n2.3.7 Inadequate reporting\nGeiger et al. (2021) ML science studies inadequately report ‘ground truth’\nHullman et al. (2022) thus we cannot know what data generating process the resulting model represents\n[perhaps cat image parable here?]\nAn investigation of 15 data science workers, Muller et al. (2021) observed common phases, which include determining the annotation scheme - all possible labels that can be attributed to digital representations of objects along with any relevant guidelines, the actual process of collecting labels, and the process by which the annotations are then aggregated into a single label. They note the difficulty of this work: issues in the annotation schemes are often discovered as annotation projects progress, requiring varying degrees of improvised adjustment.\n\n\n2.3.8 ML doesn’t treat annotation generating process as an instrument\nBeck et al. (2022)\n\nannotation collection requires design thinking\n\nTask Structure: specific wording and response options, including debates over the inclusion of “I don’t Know” option\nOrder Effects: specific judgements are affected by previous perceptions\nAnnotator Effects: backgrounds, opinions, experiences of respondents affect responses\n\n\nJacobs & Wallach (2021)\n\nreliability: do similar inputs to a measurement model present similar outputs?\n\ntest-retest: are measurements of an unobservable latent construct taken at different times via a measurement model similar, assuming the construct hasn’t changed?\n\nvalidity: is it ‘right’?\n\nno single test for validity on purpose, because it requires thinking. do our measurements:\n\nface validity: look plausible/ sensible?\ncontent validity: capture the construct?\n\nstructural validity: show the inter-correlations we expect?\nsubstantive validity: capture only observable properties thought to be related to the construct?\n\nconvergent validity: show correlations with other validated methods?\ndiscriminant validity: show correlations with other construct/properties thought not to be related to the construct?\npredictive validity: show correlations with constructs/properties thought to be related, but not in the operationalization?\nhypothesis validity: shed light on relevant hypotheses about the construct being measured?\nconsequential validity: allow for the consequences obtained from the measurement model to be assessed?\n\n\n\n2.3.9 ML ignores perspectives of annotators\nCabitza et al. (2023): whether the target of the annotation is a subjective phenomenon or not, disagreement is always irreducible. Yet ML typically assumes there is a single ‘ground truth’, and its best indicator is inter-annotator agreement. But taking the perspectives of the annotators into account, both in the data annotation but also the modelling phase of ML projects has recently been shown to benefit ML modelling in a number of contexts.\nweak perspectivist approach: taking perspectives into account while designing and collecting annotations, but ultimately reducing annotations to a single label or rating.\nstrong perspectivist approach: taking perspectives into account for ground truthing and modelling phases.\nbenefits of this approach:\n\nis congruent with the reality of collecting annotations\nincludes the signal in the variance of labels or ratings\navoids majority group perspective appearing to be ‘objective’\nallows for the modelling of human errors and variances\nallows for uncertain, fuzzy, or soft model development\nmore complete report of the data generating process, as it also reports uncertainty\n\ndownsides:\n\nmultiple raters, and therefore costs/time/rater availability are issues\nneed for perspectivist ML approaches\nvalidation becomes more challenging\n\nrecommendations:\n\ncomplete labeling schemes, including ‘i don’t know’, ‘none of these’ etc. categories, and the ability to express issues with label set\nenough raters\nheterogenous raters\nadequate reporting:\n\nnumber of raters,\nrater expertise\nincentive\ninstructions\nlength of time for labelling\ninter rater agreement\naggregation method\nconfidence"
  },
  {
    "objectID": "index.html#shortcomings-of-reference-data-design",
    "href": "index.html#shortcomings-of-reference-data-design",
    "title": "Advancing Perspectivist Ground Truthing with Social Science",
    "section": "2.2 Shortcomings of reference data design",
    "text": "2.2 Shortcomings of reference data design\nConsidering the ever-presence of human influence on the reference data, best practices, considerations, and frameworks from the social sciences could inform designs, but have yet to be broadly applied in the computational sciences (Beck et al., 2022; Jacobs & Wallach, 2021). One reason for this gap may be that ML researchers prefer to work on building systems and evaluating their performance rather than researching, designing and executing ground-truthing projects (Muller et al., 2021; Sambasivan et al., 2021). Another may be a lack of focus on these topics in textbooks, and thus in education more broadly (Geiger et al., 2020). A third may be that the social and computational sciences have conceptually different focci: the computational sciences focus on the statistical model the system with substantially less emphasis on the content, whereas the social sciences treat the statistical model as a means to better understanding the relationships in the content (Liem et al., 2018). Psychology research thus contains many more research projects in which datasets are collected using responses from people, whereas datasets tend to be re-used extensively in machine learning work (Geiger et al., 2021). A further more practical complication is that work on these topics lacks the acknowledgement that ground-truthing is indeed a measurement problem, and lacks a central academic ‘home’: where psychology and economics have psychometrics and econometrics respectively, fields dedicated to studying field-specific measurement practices, the study of ground-truthing lacks a central banner under which academic work can accumulate and disseminate.\nCommonly observed shortcomings include 1) representational biases in the content sampled for inclusion in training/evaluation datasets (Hullman et al., 2022), 2) measurement biases in the annotations collected (Beck et al., 2022; Hullman et al., 2022; Jacobs & Wallach, 2021), 3) a fallacious assumption of a single canonical ‘ground-truth’ (Aroyo & Welty, 2015; Cabitza et al., 2023), and 4) poor reporting of necessary information regarding the annotation-collection process (Geiger et al., 2021; Hullman et al., 2022). An additional consideration that receives little attention is the number of annotations to gather, where fields that focus on gathering data from humans also have a strong emphasis on a-priori decisions to mitigate bias, such as the pre-registration of calculated of target sample sizes estimated via statistical power analysis (Cohen, 1992). These considerations are absent in computational fields which appear to favor differing rules of thumb: e.g. in a well-cited textbook, Pustejovsky & Stubbs (2013) suggest to “have your corpus annotated by at least two people (more is preferable, but not always practical)”, whereas Artstein & Poesio (2008) suggest that “measuring reliability with only two coders is seldom enough, except for small-scale studies”.\nDecisions such as the selection of items for training data (Hullman et al., 2022), and the collection of human responses for reference data (Beck et al., 2022), are part of a design of a process that results in data (Muller et al., 2021). On the one hand, corpora tend to be very large, and resources are finite, making cost a primary factor in design decisions. On the other hand, rules of thumb lack clear substantiation in light of the both 1) the phenomenon being grounded and 2) the ambiguity of the media in which it is grounded. In other words, more variance is expected in annotation targets to the degree they are subjective or based on opinion (Beck et al., 2022), and more variance is expected in content to the degree to which it is ambiguous - i.e. can be interpreted in multiple ways - such as figurative language (Sandri et al., 2023). Further, some degree of variance will always be present when there are multiple annotations or ratings for a given piece of media independent of the target (cabitza?), based on the range of reasonable interpretations of that target in that media (arroyo?).\n\n2.2.1 sampling bias\nHullman et al. (2022)\nWith regards to reference data:\n\nrepresentation bias / non-representative samples\nmeasurement bias / unvalidated measurement instruments\nunderspecification of portions of input space in training data\ntransformation of data to optimize for ‘accuracy’\nlack of or poor dataset documentation\n\nIn other words, optimizing for predictive accuracy using very large datasets does not ‘absolve’ researchers from having to consider the data generating process. They note benefits that both machine learning and psychology could gain by borrowing methods from each other, but note the danger if these are misused. For the benefit of machine learning, there are lessons to be learned from social science, and the replication crisis. Among them are 1) collecting samples whose test/evaluation set distributions are drawn from the same deployment distribution, and 2) using valid measurement instruments.\n\n\n2.2.2 measurement biases"
  },
  {
    "objectID": "index.html#common-shortcomings-of-reference-data-design",
    "href": "index.html#common-shortcomings-of-reference-data-design",
    "title": "Prospective Ground-Truthing:",
    "section": "2.2 Common shortcomings of reference data design",
    "text": "2.2 Common shortcomings of reference data design\nRecent trends in Machine Learning (ML) — especially in deep learning — prioritize empirical performance over theoretical assumptions about the data generating process. A systematic analysis of highly cited ML works shows what is valued most: Performance, Generalization, Quantitative evidence, Efficiency, Building on past work, and Novelty Birhane et al. (2022). Unlike the social sciences (e.g. psychology), ML work often ignores attempts to model the process that gives rise to the data, and aims instead at predictive models whose outputs fall within some accepted estimated error bounds, resulting in poor or even biased reference data design (Hullman et al., 2022).\nConsidering the ever-presence of human influence on the reference data, best practices, considerations, and frameworks from the social sciences could inform designs, but have yet to be broadly applied in the computational sciences (Beck et al., 2022; Jacobs & Wallach, 2021). One reason for this gap may be that ML researchers prefer to work on building systems and evaluating their performance rather than researching, designing and executing ground-truthing projects (Muller et al., 2021; Sambasivan et al., 2021). Another may be a lack of focus on these topics in textbooks, and thus in education more broadly (Geiger et al., 2020). A third may be that the social and computational sciences have conceptually different focci: the computational sciences focus on the statistical model the system with substantially less emphasis on the content, whereas the social sciences treat the statistical model as a means to better understanding the relationships in the content (C. C. Liem et al., 2018). Psychology research thus contains many more research projects in which datasets are collected using responses from people, whereas datasets tend to be re-used extensively in machine learning work (Geiger et al., 2021). A further more practical complication is that work on these topics lacks the acknowledgement that ground-truthing is indeed a measurement problem, and lacks a central academic ‘home’: where psychology and economics have psychometrics and econometrics respectively, fields dedicated to studying field-specific measurement practices, the study of ground-truthing lacks a central banner under which academic work can accumulate and disseminate.\nDecisions such as the selection of items for training data (Hullman et al., 2022), and the collection of human responses for reference data (Beck et al., 2022), are part of a design of a process that results in data (Muller et al., 2021). For example, an investigation of 15 data science workers, Muller et al. (2021) observed common phases, which include determining the annotation scheme - a) all possible labels that can be attributed to digital representations of objects along with any relevant guidelines, b) the actual process of collecting labels, and c) the process by which the annotations are then aggregated into a single label. They note the difficulty of this work: issues in the annotation schemes are often discovered as annotation projects progress, requiring varying degrees of improvised adjustment. For any of these components, decisions are made that impact the resulting reference data, whether or not they are being made by design.\nCommonly observed shortcomings of data used in AI systems include: 1) representational biases in the content sampled for inclusion in training/evaluation datasets (Hullman et al., 2022), 2) a fallacious assumption of a single canonical ‘ground-truth’ when there are a range of reasonable interpretations (Aroyo & Welty, 2015; Cabitza et al., 2023), 3) measurement biases in the annotations collected (Beck et al., 2022; Hullman et al., 2022; Jacobs & Wallach, 2021), and 4) poor reporting of necessary information regarding the annotation-collection process (Geiger et al., 2021; Hullman et al., 2022).\nAn additional consideration that receives little attention is 5) the estimation of the number of annotations to gather, where fields that focus on gathering data from humans typically also have a strong emphasis on a-priori decisions, such as the pre-registration of calculated of target sample sizes estimated via statistical power analysis (Cohen, 1992), to mitigate sources of bias that come from the researcher. These considerations are absent in computational fields which appear to favor differing rules of thumb: e.g. in a well-cited textbook, Pustejovsky & Stubbs (2013) suggest to “have your corpus annotated by at least two people (more is preferable, but not always practical)”, whereas Artstein & Poesio (2008) suggest that “measuring reliability with only two coders is seldom enough, except for small-scale studies”.On the one hand, corpora tend to be very large, and resources are finite, making cost a primary factor in design decisions. On the other hand, rules of thumb lack clear substantiation in light of the both 1) the phenomenon being grounded and 2) the ambiguity of the media in which it is grounded. In other words, more variance is expected in annotation targets to the degree they are subjective or based on opinion (Beck et al., 2022), and more variance is expected in content to the degree to which it is ambiguous - i.e. can be interpreted in multiple ways - such as figurative language (Sandri et al., 2023). Further, some degree of variance will always be present when there are multiple annotations or ratings for a given piece of media independent of the target (Cabitza et al., 2023), and based on the range of reasonable interpretations of that target in that media (Aroyo & Welty, 2015).\n\n2.2.1 Representational bias\nWhen sampling content to include in training/test datasets, samples for the training/test sets will ideally be drawn from the same distribution as the content in which they will eventually be deployed. Representation bias in content selected for training and/or evaluation datasets refers to the degree to which relevant distributions in data used as reference and/or training data resemble distributions in the environment to which a system is deployed (Hullman et al., 2022). If data used for training under-represents parts of the input space of an algorithm that then estimates parameters from that input space, the model resulting will have higher error rates for those under-represented parts of the input space when deployed. If content is selected without appropriate design aimed at representing the population from which samples are drawn, the overall distribution will not represent the population it was drawn from. Thus, optimizing for predictive accuracy using very large datasets does not ‘absolve’ researchers from having to consider the data generating process, and this includes sampling pieces of content to be annotated.\nApproaches to representation problems can come from Sampling Theory, which frames the problem as one of selecting elements of a population, from which a sample must be drawn, and where the aim is that measurements of interest in the sample resemble measurements of interest in the population (Groves et al., 2009). This framework is typically applied to selecting people for inclusion in survey studies, whereby their responses to questions lend themselves to inference about a target population. Although there is no ‘one-size-fits-all’ solution to sampling, this thesis makes use of stratified random sampling as a general strategy: namely, the identification of groups of elements within a population that may affect the measurements in question, and the random sampling of elements within the groups, with approximately equal observations. In principle, this allows for the representation of the groups in population, on the measurement of interest, with some margin of error (Groves et al., 2009).\n\n\n2.2.2 The perspectives of annotators\nThe field of machine learning tends to treat all annotation variance as noise rather than signal. Often multiple ratings per piece of content are collected, aggregated, and only then shared, forming a singular ‘ground truth’ for the aspect of the content being labelled or rated. The quality of annotations is typically assessed using inter-annotator agreement, where more agreement is typically thought to indicate higher quality data Aroyo & Welty (2015). Thus, it is assumed that there is a singular canonical truth for each aspect / content pair, comprised of aggregated human responses, visible as the general agreement of human response, and which forms a target to which we align our automated systems. To illustrate more accurate representation of human responses, however, Aroyo & Welty (2015) operationalize their term ‘crowd truth’ as the ‘gold standard’ being the probability that a sentence contains an element, based on the probability that an annotator annotated that sentence with that element i.e. the label isn’t represented as ‘present’ or ‘not present’, but as a probability that an annotator labelled it as such. The probability that they may label it as such may in part explained by certain characteristics of theirs, such as their backgrounds, personal experiences etc. (Beck et al., 2022).\nDisagreement is common and never fully reducible (Cabitza et al., 2019). Cabitza et al. (2023) show that this is the case whether the task is typically thought of as subjective, e.g. NLP tasks (Aroyo & Welty, 2015), but also in tasks thought to be far less so, e.g. medical cases (Cabitza et al., 2019). Disagreement, observable as variance in the human input, is often removed via 1) adjusting annotator training and instruction so as to reduce variance in the human inputs at the time of collection, 2) adjusting annotations via discussion post-collection, thus allowing annotators to establish conventions, discuss views, and re-think their responses, or 3) completely post-hoc at the time of modeling, via methods like majority voting, without input from the annotators. Each method of reducing variance - e.g. thorough training for crowd-sourced workers, regular annotator meetings to resolve disagreements, or taking a mean of ratings or majority vote - may result in different data independent of content, or the phenomenon of interest being annotated in the content.\nFurther, variance in observed disagreement can be signal rather than noise. This signal may help to better understand the content being annotated: annotations may vary based on the ambiguity of the stimuli themselves, both in terms of the mode (audio vs. image vs. video vs. text), specific medium (Tweet vs. podcast transcript), or even the specific piece of content being annotated (Aroyo & Welty, 2015). Thus, not all pieces of content are equally unambiguous, and more ambiguous content is likely to result in greater variances in human input. This signal may help better understand the phenomenon of interest being annotated in the content: for at least some phenomena, the assumption that there is a single ground-truth to approximate with annotations doesn’t hold (Aroyo & Welty, 2015; Beck et al., 2022; Cabitza et al., 2023). More broadly, variances in the ratings may inform a finite “range of reasonable interpretations” of the phenomenon of interest being annotated in each piece of content, rather than a singular point. This signal may also help to better understand the background of annotators: people’s ethnic and/or cultural backgrounds may determine how they interpret content, and thus characteristics of the annotators may explain variance in the annotations. For example, although we expect hate speech exists, people’s perceptions of what constitutes hate speech may vary Beck et al. (2022). Showing that perceptions vary by identifiable characteristics, e.g. gender identity, ethnicity etc. may help unearth biases, whereby a single group perspective appears ‘objective’ (Cabitza et al., 2023).\nTaking an approach to gathering reference data that attempts to account for the perspectives of the annotators is referred to as the perspectivist7 approach. It can apply to both the data annotation but also the modelling phase of ML projects, where benefits to ML models has been shown in a number of contexts (Cabitza et al., 2023). Although typically focused on the annotation of language data, perspectivist approaches can be broadly applied to annotations in reference data: weak perspectivist approaches involve taking perspectives into account while designing and collecting annotations e.g. by ensuring heterogenous raters and gathering enough ratings, as well as sharing and reporting the disaggregated data, but ultimately reducing annotations to a single label or rating for modeling. Strong perspectivist approaches involve taking perspectives into account for ground truthing and modelling phases.\nTaking the perspectivist approach has a number of clear benefits, but also costs. It involves substantially more effort required to design the process that will result in annotations, higher costs in terms of the number of annotations and annotators needed in order to examine sources of variance, and challenges validating the data. In addition there are thus far few perspectivist modelling approaches that make full use of the variance in inputs (Cabitza et al., 2023). However, the perspectivist approach better reflects the reality that collecting annotations is a process that generates data with a number of relevant components (Hullman et al., 2022; Jacobs & Wallach, 2021). Further, it is a more complete report of the data resulting from the annotation process: the inclusion of the varying inputs in turn allows for better understanding of the content being annotated, the annotators annotating it, and the phenomenon of interest being annotated, which in turn allows for the development of models that make use of this information (Cabitza et al., 2023). This thesis accounts for annotator perspectives by collecting data using stratified sampling (Groves et al., 2009) among annotators, using cross-classified multilevel models to assess whether participant characteristics have statistically significant effects on their ratings (Doedens et al., 2022), and reporting disaggregated data (Cabitza et al., 2023).\n\n\n2.2.3 Measurement bias\nThe social sciences treat data from people as imperfect observations of a latent variable called a construct - like the effectiveness of a teacher, or recidivism i.e. the risk that someone will repeat a crime, or personality from the field of Psychology (Cronbach & Meehl, 1955). Social and computational sciences traditionally have different focci: where the social sciences emphasize an interpretable meaning of \\(x\\) and \\(y\\), where \\(x\\) and \\(y\\) are not always directly observable, the computational sciences instead focus on the statistical procedure that correlates \\(x\\) in terms of \\(y\\) (see Fig 1.). Applying the latent variable approach to the gathering of annotations, Jacobs & Wallach (2021) suggest there is a ‘measurement error model’ (a term they borrow from the field of Economics), that links the unobservable latent variable, and properties that we can observe - in our case, the data produced when people label or annotate. Thus, the annotations we observe can not be the ‘ground truth’ as such a thing is unknowable. Rather, each annotation is an imperfect indication that can be used to estimate the ground truth.\n Fig. 1: C. C. Liem et al. (2018)\nAs one may measure one’s height with a ruler, one may acknowledge that no measurement is perfect, but estimate one’s latent ‘height’ via multiple measurements Jacobs & Wallach (2021). Similar to the ruler being an instrument to measure height, the social sciences - e.g. Psychology, Survey Science and Cognitive Science - research and develop instruments: standardized, systematic procedures designed to compare individuals (Cronbach, 1960). These are often surveys or standardized tasks, are designed to measure one or more constructs, and undergo assessment prior to being considered acceptably valid and useful (Cronbach & Meehl, 1955).\nSurvey science focuses specifically on surveys. It seeks to minimize the influence of sources of ‘noise’ via survey design: Task Structure involves refining specific wording and response options, including deciding on the inclusion of “I Don’t Know” or otherwise neutral response options, Order Effects involves strategies to randomly present content, as judgements of a specific piece of content are affected by perceptions of immediately previous pieces of content, and Annotator Effects which involves strategies to appropriately account for differences in perception based on the backgrounds, experiences and opinions of the annotators (Beck et al., 2022).\nA number of other fields provide frameworks for assessing the quality of a measurement instrument, including psychometrics (Jacobs & Wallach, 2021), and metrology - the science of measurement - (Welty et al., 2019). For instance, the concept of reliability asks whether similar inputs consistently produce similar outputs, either across annotators (inter-rater reliability) or over time (test-retest reliability). The related concept of precision in metrology, separates the similarity of measurements from an instrument into repeatability, the similarity of measurements given that the operator, equipment, calibration, environment, and time between measurements are held constant, and reproducibility, the similarity of measurements given that the aforementioned are not held constant (Welty et al., 2019).\nBeyond consistency, validity addresses whether the instrument is actually measuring what it claims to measure (Cronbach & Meehl, 1955; Jacobs & Wallach, 2021). This includes checks for face validity (does it seem plausible?), content validity (does it cover the full scope of the concept?), and structural or substantive validity (do the internal patterns make sense given extant theory?). Other forms such as convergent and discriminant validity test whether the measure behaves as expected relative to related or unrelated constructs, while predictive, hypothesis, and consequential validity consider what the measurement enables: does it support useful predictions, align with theoretical expectations, or have appropriate consequences in applied contexts? The ultimate conclusion is thus an estimate of construct validity: does the instrument measure the construct it intends to? (Cronbach & Meehl, 1955)\nAlthough there is no one-size-fits-all solution to estimating the quality of an instrument, these various tools provide insights into whether the measurements appear to have qualities fitting of good measurements. This case study in this thesis builds on extant work by starting with a validated questionnaire. It then makes use of estimates of inter-rater reliability, precision, structural validity, to assess the quality of measurements.\n\n\n2.2.4 Inadequate reporting\nDespite the central role that human-labeled data play in machine learning, studies often provide insufficient documentation about how these data were created. In their systematic review, Geiger et al. (2021) and Geiger et al. (2020) found that many ML papers fail to adequately describe the processes used to create ‘ground truth’ labels, leaving unclear what exactly is being measured or how. As a result, the reference data often function as opaque black boxes, preventing meaningful scrutiny of what a model has learned. This is particularly concerning as they show that such datasets are reused across multiple studies, amplifying the impact of unreported or poorly understood annotation processes. Building on this, Hullman et al. (2022) argues that when reference data are under-specified, it becomes impossible to determine what data-generating process a trained model actually represents. Without transparency about how labels were created, we cannot evaluate the model’s fitness for deployment, its fairness, or its generalizability.\nGeiger et al. (2021) provide concrete recommendations for improving transparency and accountability when reporting annotation procedures. They argue that researchers should report who the annotators are (including demographics or expertise where relevant), how they were recruited, and what task instructions were given. Additionally, they recommend documenting the labeling environment, such as tools or interfaces used, and whether annotators worked independently or collaboratively. Reporting should also include measures of inter-annotator agreement, details on how disagreement was handled (e.g., aggregation method, arbitration), and any quality control mechanisms applied. These reporting practices not only support reproducibility and critical evaluation but also help surface the social and epistemic assumptions embedded in the annotation process — assumptions which directly shape the model’s understanding of the world. Cabitza et al. (2023) further emphasize that adequate reporting should include details such as the number and expertise of raters, their incentives, instructions provided, time spent per annotation, inter-rater agreement metrics, the method used to aggregate annotations, and any confidence measures. These elements are necessary not only for reproducibility but also for evaluating the quality and appropriateness of the annotation process itself. Without them, assessments of model performance risk being built on shaky foundations, misrepresenting both the model and the phenomenon it is intended to capture."
  }
]