[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Advancing Perspectivist Ground Truthing with Social Science",
    "section": "",
    "text": "Hullman et al. (2022) compare claims that ML is facing a reproducibility crisis to the crisis in psychology. Among the issues they note relate to reference datasets.\nThey note that researchers often re-use datasets as they publish on standardized benchmarks, and because they are cost prohibitive to collect. It may also be the case that ML researchers broadly prefer to work on building and evaluating performance, rather than executing ground-truthing projects Sambasivan et al. (2021).\n\nuse of non-representative samples\nmeasurement bias\nunderspecification of the population captured in data\ntransformation of data to optimize for ‘accuracy’\n\nWith caveats, they note that there are lessons to be learned from social science, and their replication crisis.\n\n\nsocial sciences:\n\ninterpretable meaning of x and y\ndesign is informed by theory\n\ncomputational sciences:\n\nlearning procedure f(x)\n\n Fig. 1: Liem et al. (2018)\nthus, positive yet unrealized interdisciplinary potentia, but also strong negative potential when disciplines ignore fundamental scientific issues in their research pipelines i.e. the “worst of both worlds” Hullman et al. (2022)\n\n\nPonce-López et al. (2016)\n\nefficient way to gather media and annotation data\nBUT no validation of instrument, or ecologically valid media data\n\n Figure 2: Ponce-López et al. (2016)\nCompared to Koutsoumpis et al. (2024):\n\necological validity: media data was mock asynchronous video interviews\necological validity: interview questions designed to activate personality facets\npersonality instruments: validated HEXACO scale\nperspectives: self & observer ratings\n\n\n\n\n\nGeiger et al. (2021)\n\n200 randomly sampled ML papers from 3 domains:\n\nSocial Sciences & Humanities\nLife & Biomedical Sciences\nPhysical & Environmental Sciences\n\nOut of 141 classification tasks, 103 (73.05%) used human labels\nOut of 103 human labels, 58 (56.31%) used only external labels\n\ni.e. ML re-uses external labels, and inadequately reports ‘ground truth’\nwithout details of ground truth, we cannot know what the resulting model represents [perhaps cat image parable here?]\n\n\n\nGriffin & Brenner (2004) review errors and biases in human judgements1\n\nover/under prediction: confidence score is higher/lower than accuracy\nover/under extremity: confidence is more extreme at ends\n\nalso reviews possible reasons:\n\noptimistic overconfidence\nconfirmation bias\ncase-based judgment\necological probability\nerror model (psychometric model)\n\n\n\n\n[crowd truth; perspectivist approach here]\n\n\nSolutions to sampling problems can come from sampling theory: Groves et al. (2009)\nconsiderations:\n\nsampling frame: the elements in from populations that you have access to\nineligible units: elements in the sampling frame that are not your target\nundercoverage: elements from target population that are not in the frame\n\nsolutions:\n\nstratified sampling\n\n\n\n\nBeck et al. (2022)\n\nannotation collection requires design thinking\n\nTask Structure: specific wording and response options, including debates over the inclusion of “I don’t Know” option\nOrder Effects: specific judgements are affected by previous perceptions\nAnnotator Effects: backgrounds, opinions, experiences of respondents affect responses\n\n\n[another ref that describes the target as a latent variable]"
  },
  {
    "objectID": "index.html#poor-reproducibilityreplicability-in-ml-research",
    "href": "index.html#poor-reproducibilityreplicability-in-ml-research",
    "title": "Advancing Perspectivist Ground Truthing with Social Science",
    "section": "",
    "text": "Hullman et al. (2022) compare claims that ML is facing a reproducibility crisis to the crisis in psychology. Among the issues they note relate to reference datasets.\nThey note that researchers often re-use datasets as they publish on standardized benchmarks, and because they are cost prohibitive to collect. It may also be the case that ML researchers broadly prefer to work on building and evaluating performance, rather than executing ground-truthing projects Sambasivan et al. (2021).\n\nuse of non-representative samples\nmeasurement bias\nunderspecification of the population captured in data\ntransformation of data to optimize for ‘accuracy’\n\nWith caveats, they note that there are lessons to be learned from social science, and their replication crisis.\n\n\nsocial sciences:\n\ninterpretable meaning of x and y\ndesign is informed by theory\n\ncomputational sciences:\n\nlearning procedure f(x)\n\n Fig. 1: Liem et al. (2018)\nthus, positive yet unrealized interdisciplinary potentia, but also strong negative potential when disciplines ignore fundamental scientific issues in their research pipelines i.e. the “worst of both worlds” Hullman et al. (2022)\n\n\nPonce-López et al. (2016)\n\nefficient way to gather media and annotation data\nBUT no validation of instrument, or ecologically valid media data\n\n Figure 2: Ponce-López et al. (2016)\nCompared to Koutsoumpis et al. (2024):\n\necological validity: media data was mock asynchronous video interviews\necological validity: interview questions designed to activate personality facets\npersonality instruments: validated HEXACO scale\nperspectives: self & observer ratings\n\n\n\n\n\nGeiger et al. (2021)\n\n200 randomly sampled ML papers from 3 domains:\n\nSocial Sciences & Humanities\nLife & Biomedical Sciences\nPhysical & Environmental Sciences\n\nOut of 141 classification tasks, 103 (73.05%) used human labels\nOut of 103 human labels, 58 (56.31%) used only external labels\n\ni.e. ML re-uses external labels, and inadequately reports ‘ground truth’\nwithout details of ground truth, we cannot know what the resulting model represents [perhaps cat image parable here?]\n\n\n\nGriffin & Brenner (2004) review errors and biases in human judgements1\n\nover/under prediction: confidence score is higher/lower than accuracy\nover/under extremity: confidence is more extreme at ends\n\nalso reviews possible reasons:\n\noptimistic overconfidence\nconfirmation bias\ncase-based judgment\necological probability\nerror model (psychometric model)\n\n\n\n\n[crowd truth; perspectivist approach here]\n\n\nSolutions to sampling problems can come from sampling theory: Groves et al. (2009)\nconsiderations:\n\nsampling frame: the elements in from populations that you have access to\nineligible units: elements in the sampling frame that are not your target\nundercoverage: elements from target population that are not in the frame\n\nsolutions:\n\nstratified sampling\n\n\n\n\nBeck et al. (2022)\n\nannotation collection requires design thinking\n\nTask Structure: specific wording and response options, including debates over the inclusion of “I don’t Know” option\nOrder Effects: specific judgements are affected by previous perceptions\nAnnotator Effects: backgrounds, opinions, experiences of respondents affect responses\n\n\n[another ref that describes the target as a latent variable]"
  },
  {
    "objectID": "index.html#present-work",
    "href": "index.html#present-work",
    "title": "Advancing Perspectivist Ground Truthing with Social Science",
    "section": "Present Work",
    "text": "Present Work\nWe incorporate these considerations in the design of our study, and attempt to further the field in the following ways:\n\nwe attempt representative sampling of both media and respondents\nwe aim to estimate 10-dimensional psychological construct\nwe select media that is ambiguous (i.e. that will result in subjectivity in the ratings) as well as media that we expect not to be ambiguous for comparison\nwe estimate a-priori the number of ratings necessary rather than assuming\nwe take into account perspectives\n\ncase study of this thesis works towards path (b) in Liem et al. (2018) shown in:"
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Advancing Perspectivist Ground Truthing with Social Science",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nGriffin & Brenner (2004) note that much of this work was about people guessing knowledge from an almanac, and then guessing how accurate they were↩︎"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  }
]