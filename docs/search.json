[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Advancing Perspectivist Ground Truthing with Social Science",
    "section": "",
    "text": "Aroyo & Welty (2015)\n\nIn order to evaluate ML / AI systems, we compare the output of these systems to reference data.\nOne method for creating reference data is the collection of human annotations.\nThis method typically assumes that, for every piece of content being annotated, there is a single canonical truth\nquality of annotations is assessed using inter-annotator agreement, where more agreement = better annotations\n\n\n\nGeiger et al. (2021)\n\n200 randomly sampled ML papers from 3 domains:\n\nSocial Sciences & Humanities\nLife & Biomedical Sciences\nPhysical & Environmental Sciences\n\nOut of 141 classification tasks, 103 (73.05%) used human labels\nOut of 103 human labels, 58 (56.31%) used only external labels\n\ni.e. ML re-uses external labels, and inadequately reports ‘ground truth’\nwithout details of ground truth, we cannot know what data generating process the resulting model represents Hullman et al. (2022). [perhaps cat image parable here?]\n\n\n\nGriffin & Brenner (2004) review errors and biases in human judgements1\n\nover/under prediction: confidence score is higher/lower than accuracy\nover/under extremity: confidence is more extreme at ends\n\nalso reviews possible reasons:\n\noptimistic overconfidence\nconfirmation bias\ncase-based judgment\necological probability\nerror model (psychometric model)\n\n\n\n\nHullman et al. (2022) compare claims that ML is facing a reproducibility crisis to the crisis in psychology. Among the issues they note relate to benchmark datasets, which researchers often re-use as they publish on standardized benchmarks, and because they are cost prohibitive to collect. It may also be the case that ML researchers broadly prefer to work on building and evaluating performance, rather than executing ground-truthing projects Sambasivan et al. (2021).\nWith regards to reference data:\n\nrepresentation bias / non-representative samples\nmeasurement bias / unvalidated measurement instruments\nunderspecification of portions of input space in training data\ntransformation of data to optimize for ‘accuracy’\nlack of or poor dataset documentation\n\nIn other words, optimizing for predictive accuracy using very large datasets does not ‘absolve’ researchers from having to consider the data generating process. They note benefits that both machine learning and psychology could gain by borrowing methods from each other, but note the danger if these are misused. For the benefit of machine learning, there are lessons to be learned from social science, and the replication crisis. Among them are 1) collecting samples whose test/evaluation set distributions are drawn from the same deployment distribution, and 2) using valid measurement instruments.\n\n\n\nBeyond errors in judgment are questions about the target for the annotations. For at least some phenomena, the assumption that there is a single ground-truth to approximate with annotations doesn’t hold.\nAroyo & Welty (2015)\n7 ‘myths’ of human annotation:\n\nthere is one truth\ndisagreement is bad\ndetailed guidelines help\nexperts are better\none annotator is enough\nall items are created equal\nonce done, forever valid\n\nFor myths 1 and 2:\n\nlist examples from NLP where the disagreement from annotators is sensible\nthey argue that the assumptions of a single ground truth, and that disagreement is indicative of poor annotations are both false.\n\nWhile it can be indicative of annotation quality, e.g. the annotator is annotating incorrectly, in other cases disagreement indicates that the media being rated is ambiguous.\nBeck et al. (2022): we should expect more variance to the degree that tasks measure opinion\n\nshow work on an intuitively perspective-based use-case: hate speech\n\nAroyo & Welty (2015) operationalize ‘crowd truth’ with an illustration where the ‘gold standard’ is the probability that a sentence contains an element, based on the probability that an annotator annotated that sentence with that element.\n\ni.e. the label isn’t represented as ‘present’ or ‘not present’, but as a probablility\nthus the ‘crowd truth’ attempts to capture the ‘range of reasonable interpretations’"
  },
  {
    "objectID": "index.html#poor-reproducibilityreplicability-in-ml-research",
    "href": "index.html#poor-reproducibilityreplicability-in-ml-research",
    "title": "Advancing Perspectivist Ground Truthing with Social Science",
    "section": "Poor reproducibility/replicability in ML research",
    "text": "Poor reproducibility/replicability in ML research\nHullman et al. (2022) compare claims that ML is facing a reproducibility crisis to the crisis in psychology. Among the issues they note relate to benchmark datasets, which researchers often re-use as they publish on standardized benchmarks, and because they are cost prohibitive to collect. It may also be the case that ML researchers broadly prefer to work on building and evaluating performance, rather than executing ground-truthing projects Sambasivan et al. (2021).\nWith regards to reference data:\n\nrepresentation bias / non-representative samples\nmeasurement bias / unvalidated measurement instruments\nunderspecification of portions of input space in training data\ntransformation of data to optimize for ‘accuracy’\nlack of or poor dataset documentation\n\nIn other words, optimizing for predictive accuracy using very large datasets does not ‘absolve’ researchers from having to consider the data generating process. They note benefits that both machine learning and psychology could gain by borrowing methods from each other, but note the danger if these are misused. For the benefit of machine learning, there are lessons to be learned from social science, and the replication crisis. Among them are 1) collecting samples whose test/evaluation set distributions are drawn from the same deployment distribution, and 2) using valid measurement instruments.\n\nSocial and computational sciences have different focci\nsocial sciences:\n\ninterpretable meaning of x and y\ndesign is informed by theory\n\ncomputational sciences:\n\nlearning procedure f(x)\n\n Fig. 1: Liem et al. (2018)\nRecent trends—especially in deep learning—prioritize empirical performance over theoretical assumptions about the data generating process. A systematic analysis of highly cited ML works shows that Performance, Generalization, Quantitative evidence, Efficiency, Building on past work, and Novelty Birhane et al. (2022). Unlike the social sciences (e.g. psychology), ML work ignores attempts to model the process that gives rise to the data, assuming it cannot be learned, and aims instead at predictors that will within estimable error bounds Hullman et al. (2022).\n\nA tale of two studies: the case of personnel selection\nPonce-López et al. (2016)\n\nefficient way to gather media and annotation data\nBUT no validation of instrument, or ecologically valid media data\ndistribution of training /eval data don’t come from the target distribution\n\n Figure 2: Ponce-López et al. (2016)\nCompared to Koutsoumpis et al. (2024):\n\necological validity: media data was mock asynchronous video interviews\necological validity: interview questions designed to activate personality facets\npersonality instruments: validated HEXACO scale\nperspectives: self & observer ratings\n\n\n\nIssues with sampling\nSolutions to sampling problems can come from sampling theory: Groves et al. (2009)\nconsiderations:\n\nsampling frame: the elements in from populations that you have access to\nineligible units: elements in the sampling frame that are not your target\nundercoverage: elements from target population that are not in the frame\n\nsolutions:\n\nstratified sampling\n\n\n\nIssues with instruments\nBeck et al. (2022)\n\nannotation collection requires design thinking\n\nTask Structure: specific wording and response options, including debates over the inclusion of “I don’t Know” option\nOrder Effects: specific judgements are affected by previous perceptions\nAnnotator Effects: backgrounds, opinions, experiences of respondents affect responses\n\n\n[another ref that describes the target as a latent variable]"
  },
  {
    "objectID": "index.html#present-work",
    "href": "index.html#present-work",
    "title": "Advancing Perspectivist Ground Truthing with Social Science",
    "section": "Present Work",
    "text": "Present Work\nWe incorporate these considerations in the design of our study, and attempt to further the field in the following ways:\n\nwe attempt representative sampling of both media and respondents\nwe aim to estimate 10-dimensional psychological construct\nwe select media that is ambiguous (i.e. that will result in subjectivity in the ratings) as well as media that we expect not to be ambiguous for comparison\nwe estimate a-priori the number of ratings necessary rather than assuming\nwe take into account perspectives\n\ncase study of this thesis works towards path (b) in Liem et al. (2018) shown in:"
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Advancing Perspectivist Ground Truthing with Social Science",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nGriffin & Brenner (2004) note that much of this work was about people guessing knowledge from an almanac, and then guessing how accurate they were↩︎"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html#ml-relies-reference-data",
    "href": "index.html#ml-relies-reference-data",
    "title": "Advancing Perspectivist Ground Truthing with Social Science",
    "section": "",
    "text": "Geiger et al. (2021)\n\n200 randomly sampled ML papers from 3 domains:\n\nSocial Sciences & Humanities\nLife & Biomedical Sciences\nPhysical & Environmental Sciences\n\nOut of 141 classification tasks, 103 (73.05%) used human labels\nOut of 103 human labels, 58 (56.31%) used only external labels\n\ni.e. ML re-uses external labels, and inadequately reports ‘ground truth’\nwithout details of ground truth, we cannot know what data generating process the resulting model represents Hullman et al. (2022). [perhaps cat image parable here?]\n\n\n\nGriffin & Brenner (2004) review errors and biases in human judgements1\n\nover/under prediction: confidence score is higher/lower than accuracy\nover/under extremity: confidence is more extreme at ends\n\nalso reviews possible reasons:\n\noptimistic overconfidence\nconfirmation bias\ncase-based judgment\necological probability\nerror model (psychometric model)\n\n\n\n\n[crowd truth; perspectivist approach here]"
  },
  {
    "objectID": "index.html#mlai-relies-reference-data",
    "href": "index.html#mlai-relies-reference-data",
    "title": "Advancing Perspectivist Ground Truthing with Social Science",
    "section": "",
    "text": "Aroyo & Welty (2015)\n\nIn order to evaluate ML / AI systems, we compare the output of these systems to reference data.\nOne method for creating reference data is the collection of human annotations.\nThis method typically assumes that, for every piece of content being annotated, there is a single canonical truth\nquality of annotations is assessed using inter-annotator agreement, where more agreement = better annotations\n\n\n\nGeiger et al. (2021)\n\n200 randomly sampled ML papers from 3 domains:\n\nSocial Sciences & Humanities\nLife & Biomedical Sciences\nPhysical & Environmental Sciences\n\nOut of 141 classification tasks, 103 (73.05%) used human labels\nOut of 103 human labels, 58 (56.31%) used only external labels\n\ni.e. ML re-uses external labels, and inadequately reports ‘ground truth’\nwithout details of ground truth, we cannot know what data generating process the resulting model represents Hullman et al. (2022). [perhaps cat image parable here?]\n\n\n\nGriffin & Brenner (2004) review errors and biases in human judgements1\n\nover/under prediction: confidence score is higher/lower than accuracy\nover/under extremity: confidence is more extreme at ends\n\nalso reviews possible reasons:\n\noptimistic overconfidence\nconfirmation bias\ncase-based judgment\necological probability\nerror model (psychometric model)\n\n\n\n\n[crowd truth; perspectivist approach here]"
  },
  {
    "objectID": "index.html#mlai-relies-on-reference-data",
    "href": "index.html#mlai-relies-on-reference-data",
    "title": "Advancing Perspectivist Ground Truthing with Social Science",
    "section": "",
    "text": "Aroyo & Welty (2015)\n\nIn order to evaluate ML / AI systems, we compare the output of these systems to reference data.\nOne method for creating reference data is the collection of human annotations.\nThis method typically assumes that, for every piece of content being annotated, there is a single canonical truth\nquality of annotations is assessed using inter-annotator agreement, where more agreement = better annotations\n\n\n\nGeiger et al. (2021)\n\n200 randomly sampled ML papers from 3 domains:\n\nSocial Sciences & Humanities\nLife & Biomedical Sciences\nPhysical & Environmental Sciences\n\nOut of 141 classification tasks, 103 (73.05%) used human labels\nOut of 103 human labels, 58 (56.31%) used only external labels\n\ni.e. ML re-uses external labels, and inadequately reports ‘ground truth’\nwithout details of ground truth, we cannot know what data generating process the resulting model represents Hullman et al. (2022). [perhaps cat image parable here?]\n\n\n\nGriffin & Brenner (2004) review errors and biases in human judgements1\n\nover/under prediction: confidence score is higher/lower than accuracy\nover/under extremity: confidence is more extreme at ends\n\nalso reviews possible reasons:\n\noptimistic overconfidence\nconfirmation bias\ncase-based judgment\necological probability\nerror model (psychometric model)\n\n\n\n\nAroyo & Welty (2015)\n\nlist examples from NLP where the disagreement from annotators is sensible\nthey argue that the assumptions of a single ground truth, and that disagreement is indicative of poor annotations are both false.\n\nWhile it can be indicative of annotation quality, e.g. the annotator is annotating incorrectly, in other cases disagreement indicates that the media being rated is ambiguous.\n\n\n\nHullman et al. (2022) compare claims that ML is facing a reproducibility crisis to the crisis in psychology. Among the issues they note relate to benchmark datasets, which researchers often re-use as they publish on standardized benchmarks, and because they are cost prohibitive to collect. It may also be the case that ML researchers broadly prefer to work on building and evaluating performance, rather than executing ground-truthing projects Sambasivan et al. (2021).\nWith regards to reference data:\n\nrepresentation bias / non-representative samples\nmeasurement bias / unvalidated measurement instruments\nunderspecification of portions of input space in training data\ntransformation of data to optimize for ‘accuracy’\nlack of or poor dataset documentation\n\nIn other words, optimizing for predictive accuracy using very large datasets does not ‘absolve’ researchers from having to consider the data generating process. They note benefits that both machine learning and psychology could gain by borrowing methods from each other, but note the danger if these are misused. For the benefit of machine learning, there are lessons to be learned from social science, and the replication crisis. Among them are 1) collecting samples whose test/evaluation set distributions are drawn from the same deployment distribution, and 2) using valid measurement instruments.\n\n\n\nsocial sciences:\n\ninterpretable meaning of x and y\ndesign is informed by theory\n\ncomputational sciences:\n\nlearning procedure f(x)\n\n Fig. 1: Liem et al. (2018)\nRecent trends—especially in deep learning—prioritize empirical performance over theoretical assumptions about the data generating process. A systematic analysis of highly cited ML works shows that Performance, Generalization, Quantitative evidence, Efficiency, Building on past work, and Novelty Birhane et al. (2022). Unlike the social sciences (e.g. psychology), ML work ignores attempts to model the process that gives rise to the data, assuming it cannot be learned, and aims instead at predictors that will within estimable error bounds Hullman et al. (2022).\n\n\nPonce-López et al. (2016)\n\nefficient way to gather media and annotation data\nBUT no validation of instrument, or ecologically valid media data\ndistribution of training /eval data don’t come from the target distribution\n\n Figure 2: Ponce-López et al. (2016)\nCompared to Koutsoumpis et al. (2024):\n\necological validity: media data was mock asynchronous video interviews\necological validity: interview questions designed to activate personality facets\npersonality instruments: validated HEXACO scale\nperspectives: self & observer ratings\n\n\n\n\nSolutions to sampling problems can come from sampling theory: Groves et al. (2009)\nconsiderations:\n\nsampling frame: the elements in from populations that you have access to\nineligible units: elements in the sampling frame that are not your target\nundercoverage: elements from target population that are not in the frame\n\nsolutions:\n\nstratified sampling\n\n\n\n\nBeck et al. (2022)\n\nannotation collection requires design thinking\n\nTask Structure: specific wording and response options, including debates over the inclusion of “I don’t Know” option\nOrder Effects: specific judgements are affected by previous perceptions\nAnnotator Effects: backgrounds, opinions, experiences of respondents affect responses\n\n\n[another ref that describes the target as a latent variable]"
  },
  {
    "objectID": "index.html#tools-from-social-science-can-help",
    "href": "index.html#tools-from-social-science-can-help",
    "title": "Advancing Perspectivist Ground Truthing with Social Science",
    "section": "",
    "text": "social sciences:\n\ninterpretable meaning of x and y\ndesign is informed by theory\n\ncomputational sciences:\n\nlearning procedure f(x)\n\n Fig. 1: Liem et al. (2018)\nRecent trends—especially in deep learning—prioritize empirical performance over theoretical assumptions about the data generating process. A systematic analysis of highly cited ML works shows that Performance, Generalization, Quantitative evidence, Efficiency, Building on past work, and Novelty Birhane et al. (2022). Unlike the social sciences (e.g. psychology), ML work ignores attempts to model the process that gives rise to the data, assuming it cannot be learned, and aims instead at predictors that will within estimable error bounds Hullman et al. (2022).\n\n\nPonce-López et al. (2016)\n\nefficient way to gather media and annotation data\nBUT no validation of instrument, or ecologically valid media data\ndistribution of training /eval data don’t come from the target distribution\n\n Figure 2: Ponce-López et al. (2016)\nCompared to Koutsoumpis et al. (2024):\n\necological validity: media data was mock asynchronous video interviews\necological validity: interview questions designed to activate personality facets\npersonality instruments: validated HEXACO scale\nperspectives: self & observer ratings\n\n\n\n\nSolutions to sampling problems can come from sampling theory: Groves et al. (2009)\nconsiderations:\n\nsampling frame: the elements in from populations that you have access to\nineligible units: elements in the sampling frame that are not your target\nundercoverage: elements from target population that are not in the frame\n\nsolutions:\n\nstratified sampling\n\n\n\n\nBeck et al. (2022)\n\nannotation collection requires design thinking\n\nTask Structure: specific wording and response options, including debates over the inclusion of “I don’t Know” option\nOrder Effects: specific judgements are affected by previous perceptions\nAnnotator Effects: backgrounds, opinions, experiences of respondents affect responses\n\n\n[another ref that describes the target as a latent variable]"
  },
  {
    "objectID": "index.html#social-and-computational-sciences-have-different-focci",
    "href": "index.html#social-and-computational-sciences-have-different-focci",
    "title": "Advancing Perspectivist Ground Truthing with Social Science",
    "section": "Social and computational sciences have different focci",
    "text": "Social and computational sciences have different focci\nsocial sciences:\n\ninterpretable meaning of x and y\ndesign is informed by theory\n\ncomputational sciences:\n\nlearning procedure f(x)\n\n Fig. 1: Liem et al. (2018)\nRecent trends—especially in deep learning—prioritize empirical performance over theoretical assumptions about the data generating process. A systematic analysis of highly cited ML works shows that Performance, Generalization, Quantitative evidence, Efficiency, Building on past work, and Novelty Birhane et al. (2022). Unlike the social sciences (e.g. psychology), ML work ignores attempts to model the process that gives rise to the data, assuming it cannot be learned, and aims instead at predictors that will within estimable error bounds Hullman et al. (2022).\n\nA tale of two studies: the case of personnel selection\nPonce-López et al. (2016)\n\nefficient way to gather media and annotation data\nBUT no validation of instrument, or ecologically valid media data\ndistribution of training /eval data don’t come from the target distribution\n\n Figure 2: Ponce-López et al. (2016)\nCompared to Koutsoumpis et al. (2024):\n\necological validity: media data was mock asynchronous video interviews\necological validity: interview questions designed to activate personality facets\npersonality instruments: validated HEXACO scale\nperspectives: self & observer ratings\n\n\n\nIssues with sampling\nSolutions to sampling problems can come from sampling theory: Groves et al. (2009)\nconsiderations:\n\nsampling frame: the elements in from populations that you have access to\nineligible units: elements in the sampling frame that are not your target\nundercoverage: elements from target population that are not in the frame\n\nsolutions:\n\nstratified sampling\n\n\n\nIssues with instruments\nBeck et al. (2022)\n\nannotation collection requires design thinking\n\nTask Structure: specific wording and response options, including debates over the inclusion of “I don’t Know” option\nOrder Effects: specific judgements are affected by previous perceptions\nAnnotator Effects: backgrounds, opinions, experiences of respondents affect responses\n\n\n[another ref that describes the target as a latent variable]"
  }
]