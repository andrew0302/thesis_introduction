[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Advancing Perspectivist Ground Truthing with Social Science",
    "section": "",
    "text": "AI systems run on data. Data are used to ‘train’ models - imperfect, simplified mathematical or computational representations of a phenomenon or process in the real world 1. Where an AI system is a complete application with integrated components e.g. an interface, programmatic logic, and one or more models aimed at performing tasks typically requiring human intelligence2, the models themselves are embedded components that take inputs (e.g. media like text, images, audio, or data) and produce outputs (e.g. classifications, predictions or some form of decision). Model behavior - the model’s output or response to a given input - is determined by their parameters - internal settings or values3. Algorithms - step by step instructions, executed in order4 - are used to estimate model parameters from the ‘training’ data. As models used in AI systems are designed to perform tasks, their performance is often evaluated empirically by comparing their outputs to a reference. This reference is often a second form of data, referred to as the ‘ground truth’ or ‘gold standard’, which represents the ideal expected output of the system.\nData sets designed for training, reference or both, are often re-used likely due to the relative ease of access compared to the effort and cost required to design, collect, and evaluate datasets. Training data may be tabular data, but is often a form of media - text, audio, images, or video, whereas reference data often contains aggregated input from humans (Geiger et al., 2020, 2021; Muller et al., 2021; Sav et al., 2023). This input may be collected explicitly - where a phenomenon of interest is set as a target, and human judges are given a task that produces data relevant to the target from their input. Common scenarios include human judges annotating, labeling, or rating a) individual pieces of content of the same form as the training data, or b) generated system outputs for the presence/absence, or degree of the phenomenon of interest. Data may also be collected implicitly - where digital traces of human behavior, e.g. media consumption, form the target (Sav et al., 2023).\nWhether used as part of the development of an AI system, or to evaluate it, the quality of these two forms of data determines the quality of the system. As parameters are estimated from the training data, such that observable patterns recognized in the data affect internal model values or settings, imperfections, inaccuracies, biases etc. in the data are reflected in the parameters of the resulting model. As models are evaluated by comparing their outputs to reference data, such that ‘better’ models are those whose outputs most closely resemble the reference data (Birhane et al., 2022), imperfections in the reference are reflected in the models preferred. Thus, the quality of the data used for training and reference represents the upper boundary of potential performance of AI systems when deployed: the best possible performance directly corresponds to the degree to which training and reference data represent the phenomenon of interest in the environment to which it is deployed.\nAn implication of the relationship between dataset quality and the resulting system, is that the evaluation of an AI system is a measurement problem, as is the measurement of the phenomenon of interest in the media via the human input being collected (Welty et al., 2019). To appropriately evaluate a system designed to do a task, in the data to be used for evaluation and possibly also for training, both the pieces of media and the distribution of the phenomenon of interest must resemble those of the environment in which systems are to be deployed (Hullman et al., 2022). This is a challenge, as the true values of these is unknowable and must be estimated using appropriate measurement methods (Welty et al., 2019). Crucially and often ignored in Machine Learning, the resulting inputs from humans must also show qualities fitting of good measurements (Jacobs & Wallach, 2021; Welty et al., 2019), and account for the possibility of a range of reasonable interpretations (Aroyo & Welty, 2015; Cabitza et al., 2023).\nThe social sciences have developed sampling methods to represent populations (Groves et al., 2009), and syntheses that introduce knowledge from survey science (Beck et al., 2022), metrology(Welty et al., 2019), psychometrics(Jacobs & Wallach, 2021), and the perspective approach to ground-truthing(Cabitza et al., 2023) have been published. Yet, knowledge from these fields has broadly not been applied in the field of Machine Learning, leading to issues of representation and measurement(Hullman et al., 2022).\n\n\nPresent work attempts to demonstrate the potential of synthesizing knowledge from the social sciences related to sampling and measurement with extant work on data sets for use in machine learning. Building on prior work reviewed herein, its main contribution is a case study investigation, spanning several manuscripts, of a complex evaluation data set creation problem. A secondary contribution is a synthesized framework that can be used to ground challenging phenomena in various media, following principles from prior work. A third contribution, is knowledge directly applicable to the grounding of personal values in text, such as our adapted questionnaire, and statistics of interest for planning and estimating the costs associated with such ground-truthing projects. A final contribution is immediately applicable results that work towards estimating personal values in song lyrics using language models.\nIncluded in this thesis are two manuscripts that further motivate the case study: 1) the first reviews strengths and weaknesses of datasets used in the field of Recommender Systems, 2) the second reviews poor practices in Signal Processing related to datasets whose interconnections were poorly reported, misleading results. The case study demonstrates the use of principles from the Social Sciences to solve problems of representation and measurement across 4 manuscripts: personal values in song lyrics. In a 5th manuscript, the same principles are applied to a second form of text, political speeches, expected to vary in terms of use of ambiguous language. Despite the moderate success in automatically estimating values in lyrics, this work demonstrates a failure with speeches. It includes recommendations for analyses to observe the potential for success or failure, and to estimate cost via less expensive pilot studies. This thesis follows the case study with work that 6) highlights the potential for shortcomings in the interpretation of AI system evaluations should a more epistemologically sophisticated framework for evaluation not be adopted, and 7) highlights an important component of scientific infrastructure needed for rigorous work on data sets for Machine Learning: the treatment of scientific work as open-source artifacts."
  },
  {
    "objectID": "index.html#issues-using-human-annotations-in-ml",
    "href": "index.html#issues-using-human-annotations-in-ml",
    "title": "Advancing Perspectivist Ground Truthing with Social Science",
    "section": "2.3 Issues using human annotations in ML",
    "text": "2.3 Issues using human annotations in ML\n\n2.3.1 Human annotations aren’t always accurate\nGriffin & Brenner (2004) review errors and biases in human judgements7\n\nover/under prediction: confidence score is higher/lower than accuracy\nover/under extremity: confidence is more extreme at ends\n\nalso reviews possible reasons:\n\noptimistic overconfidence\nconfirmation bias\ncase-based judgment\necological probability\nerror model (psychometric model)\n\n\n\n2.3.2 ML treats all annotation variance as noise rather than signal\ndisagreement is common\nreviewed in Cabitza et al. (2023): -social media content: Chandrasekharan 2017 -medical cases: Cabitza 2019 -various NLP tasks Aroyo & Welty (2015)\ndisagreement is often removed\n\nadjusting annotator training and instruction\nadjusting annotations via discussion post-collection\nmajority voting, post-hoc without annotators\n\nBeyond errors in judgment are questions about the target for the annotations. For at least some phenomena, the assumption that there is a single ground-truth to approximate with annotations doesn’t hold.\nAroyo & Welty (2015)\n\nIn order to evaluate ML / AI systems, we compare the output of these systems to reference data.\nOne method for creating reference data is the collection of human annotations.\nThis method typically assumes that, for every piece of content being annotated, there is a single canonical truth\nquality of annotations is assessed using inter-annotator agreement, where more agreement = better annotations\n\nAroyo & Welty (2015)\n7 ‘myths’ of human annotation:\n\nthere is one truth\ndisagreement is bad\ndetailed guidelines help\nexperts are better\none annotator is enough\nall items are created equal\nonce done, forever valid\n\nFor myths 1 and 2:\n\nlist examples from NLP where the disagreement from annotators is sensible\nthey argue that the assumptions of a single ground truth, and that disagreement is indicative of poor annotations are both false.\n\nfor myth 6: disagreement indicates that the media being rated is ambiguous.\n\n2.3.2.1 annotations aim to measure a latent variable\nJacobs & Wallach (2021) there is a ‘measurement error model’ (taken from econ) that links the unobservable latent variable, and observable properties. in annotations this is via individual observations\nalthough paper focuses on attempts at measuring constructs (risk of recidivism, teacher effectiveness, patient benefit) they also show that even ‘representational measurements’ like height, are essentially a latent variable\nAroyo & Welty (2015) operationalize ‘crowd truth’ with an illustration where the ‘gold standard’ is the probability that a sentence contains an element, based on the probability that an annotator annotated that sentence with that element.\n\ni.e. the label isn’t represented as ‘present’ or ‘not present’, but as a probablility\nthus the ‘crowd truth’ attempts to capture the ‘range of reasonable interpretations’\n\nBeck et al. (2022): we should expect more variance to the degree that tasks measure opinion show work on an intuitively perspective-based use-case: hate speech\n\n\n\n2.3.3 Inadequate reporting\nGeiger et al. (2021) ML science studies inadequately report ‘ground truth’\nHullman et al. (2022) thus we cannot know what data generating process the resulting model represents\n[perhaps cat image parable here?]\n\n\n2.3.4 sampling and measurement biases\nHullman et al. (2022)\nWith regards to reference data:\n\nrepresentation bias / non-representative samples\nmeasurement bias / unvalidated measurement instruments\nunderspecification of portions of input space in training data\ntransformation of data to optimize for ‘accuracy’\nlack of or poor dataset documentation\n\nIn other words, optimizing for predictive accuracy using very large datasets does not ‘absolve’ researchers from having to consider the data generating process. They note benefits that both machine learning and psychology could gain by borrowing methods from each other, but note the danger if these are misused. For the benefit of machine learning, there are lessons to be learned from social science, and the replication crisis. Among them are 1) collecting samples whose test/evaluation set distributions are drawn from the same deployment distribution, and 2) using valid measurement instruments.\n\n\n2.3.5 ML doesn’t treat annotation generating process as an instrument\nBeck et al. (2022)\n\nannotation collection requires design thinking\n\nTask Structure: specific wording and response options, including debates over the inclusion of “I don’t Know” option\nOrder Effects: specific judgements are affected by previous perceptions\nAnnotator Effects: backgrounds, opinions, experiences of respondents affect responses\n\n\nJacobs & Wallach (2021)\n\nreliability: do similar inputs to a measurement model present similar outputs?\n\ntest-retest: are measurements of an unobservable latent construct taken at different times via a measurement model similar, assuming the construct hasn’t changed?\n\nvalidity: is it ‘right’?\n\nno single test for validity on purpose, because it requires thinking. do our measurements:\n\nface validity: look plausible/ sensible?\ncontent validity: capture the construct?\n\nstructural validity: show the inter-correlations we expect?\nsubstantive validity: capture only observable properties thought to be related to the construct?\n\nconvergent validity: show correlations with other validated methods?\ndiscriminant validity: show correlations with other construct/properties thought not to be related to the construct?\npredictive validity: show correlations with constructs/properties thought to be related, but not in the operationalization?\nhypothesis validity: shed light on relevant hypotheses about the construct being measured?\nconsequential validity: allow for the consequences obtained from the measurement model to be assessed?\n\n\n\n2.3.6 ML ignores perspectives of annotators\nCabitza et al. (2023): whether the target of the annotation is a subjective phenomenon or not, disagreement is always irreducible. Yet ML typically assumes there is a single ‘ground truth’, and its best indicator is inter-annotator agreement. But taking the perspectives of the annotators into account, both in the data annotation but also the modelling phase of ML projects has recently been shown to benefit ML modelling in a number of contexts.\nweak perspectivist approach: taking perspectives into account while designing and collecting annotations, but ultimately reducing annotations to a single label or rating.\nstrong perspectivist approach: taking perspectives into account for ground truthing and modelling phases.\nbenefits of this approach:\n\nis congruent with the reality of collecting annotations\nincludes the signal in the variance of labels or ratings\navoids majority group perspective appearing to be ‘objective’\nallows for the modelling of human errors and variances\nallows for uncertain, fuzzy, or soft model development\nmore complete report of the data generating process, as it also reports uncertainty\n\ndownsides:\n\nmultiple raters, and therefore costs/time/rater availability are issues\nneed for perspectivist ML approaches\nvalidation becomes more challenging\n\nrecommendations:\n\ncomplete labeling schemes, including ‘i don’t know’, ‘none of these’ etc. categories, and the ability to express issues with label set\nenough raters\nheterogenous raters\nadequate reporting:\n\nnumber of raters,\nrater expertise\nincentive\ninstructions\nlength of time for labelling\ninter rater agreement\naggregation method\nconfidence"
  },
  {
    "objectID": "index.html#present-work-1",
    "href": "index.html#present-work-1",
    "title": "Advancing Perspectivist Ground Truthing with Social Science",
    "section": "5.1 Present Work",
    "text": "5.1 Present Work\nThis thesis incorporates techniques and considerations from the social sciences to address the aforementioned shortcomings.\nshowcases a design for a challenging ground-truthing project, in terms of the complexity of the phenomenon of interest, ambiguity in the media that selected and annotated. It incorporates design choices to address the aforementioned shortcomings into a singular framework, guided by best practices in the social sciences, which it then extends. It focuses unambiguously on the aspect most relevant to the\nSpecifically:\n\nWe attempt to mitigate representation biases in the content we select for annotation by using a stratified sampling strategy.\nWe attempt to mitigate measurement biases by treating the target measurement as a latent variable, and the survey we used to gather annotations as an instrument. We build on work that validated a questionnaire for measuring constructs, and estimating its reliability and structural validity when used for annotations.\nWe account for the potential of multiple perspectives in our dataset by recruiting participants from relevant subgroups in a single target population.\nWe report the details of the annotation collection process, and share the disaggregated dataset of the annotations\nWe further show how to estimate the number of annotators\n\n\nWe demonstrate the potential of this framework by grounding a complex phenomenon (a 10-dimensional construct, Personal Values) in ambiguous text (song lyrics). We further show an\nWhen sampling content to include in training/test datasets, samples for the training/test sets will ideally be drawn from the same distribution as the content in which they will eventually be deployed. Representation bias in content selected for training and/or evaluation datasets refers to the degree to which relevant distributions in data used to train and/or evaluate systems resembles the distribution in the environment to which it will be deployed (Hullman et al., 2022).\nMeasurement bias in the annotations collected from humans may also bias\nPerspectivism\nReporting\nWe add: a priori rating number estimation\nAnd although imperfect as leaderboard scores can be gamed, and do not perfectly represent the deployment environment, the typical leaderboard approach has shown evidence that progress can be made towards a target. This thesis thus represents an attempt to define the target better."
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Advancing Perspectivist Ground Truthing with Social Science",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://en.wikipedia.org/wiki/Scientific_modelling↩︎\nhttps://en.wikipedia.org/wiki/Artificial_intelligence↩︎\nhttps://en.wikipedia.org/wiki/Statistical_parameter↩︎\nhttps://en.wikipedia.org/wiki/Algorithm↩︎\nhttps://www.youtube.com/watch?v=06-AZXmwHjo↩︎\nGriffin & Brenner (2004) note that much of this work was about people guessing knowledge from an almanac, and then guessing how accurate they were↩︎\nGriffin & Brenner (2004) note that much of this work was about people guessing knowledge from an almanac, and then guessing how accurate they were↩︎"
  },
  {
    "objectID": "index.html#measurement",
    "href": "index.html#measurement",
    "title": "Advancing Perspectivist Ground Truthing with Social Science",
    "section": "",
    "text": "An implication of the relationship between dataset quality and the resulting system, is that the evaluation of an AI system is a measurement problem, as is the measurement of the phenomenon of interest in the media via the human input being collected (Welty et al., 2019). To appropriately evaluate a system designed to do a task, both the pieces of media and the distribution of the phenomenon of interest in the data it is evaluated on, and possibly also trained on, must resemble those of the environment in which it is deployed(Hullman et al., 2022), even though the true values of these is unknowable (Welty et al., 2019). Crucially and often ignored in Machine Learning, the resulting inputs from humans must also show qualities fitting of good measurements (Jacobs & Wallach, 2021; Welty et al., 2019), and account for the possibility of a range of reasonable interpretations (Aroyo & Welty, 2015; Cabitza et al., 2023).\nThe social sciences have developed sampling methods to represent populations (Groves et al., 2009), and syntheses that introduce knowledge from survey science (Beck et al., 2022), metrology(Welty et al., 2019), psychometrics(Jacobs & Wallach, 2021), and the perspective approach to ground-truthing(Cabitza et al., 2023) have been published. Yet, knowledge from these fields has broadly not been applied in the field of Machine Learning, leading to issues of representation and measurement(Hullman et al., 2022).\nPresent work attempts to demonstrate the potential of synthesizing knowledge from the social sciences related to sampling and measurement with extant work on data sets for use in machine learning. Building on prior work reviewed herein, its main contribution is a case study investigation, spanning several manuscripts, of a complex evaluation data set creation problem. Included in this thesis are two manuscripts that further motivate the case study: the first reviews strengths and weaknesses of datasets used in the field of Recommender Systems, the second reviews poor practices in Signal Processing related to datasets whose interconnections were poorly reported, misleading results. The case study demonstrates the use of principles from the Social Sciences to solve problems of representation and measurement: personal values in text. It spans two forms of text, expected to vary in terms of use of ambiguous language: song lyrics and political speeches, and demonstrates a moderate success in automatically estimating values in lyrics and a failure in speeches. It includes recommendations for analyses to observe the potential for success or failure, and to estimate cost via less expensive pilot studies. This thesis follows the case study highlighting the potential for shortcomings in the interpretation of AI system evaluations should a more sophisticated framework for evaluation not be adopted, and highlighting"
  },
  {
    "objectID": "index.html#measurement-in-evaluation",
    "href": "index.html#measurement-in-evaluation",
    "title": "Advancing Perspectivist Ground Truthing with Social Science",
    "section": "",
    "text": "An implication of the relationship between dataset quality and the resulting system, is that the evaluation of an AI system is a measurement problem, as is the measurement of the phenomenon of interest in the media via the human input being collected (Welty et al., 2019). To appropriately evaluate a system designed to do a task, both the pieces of media and the distribution of the phenomenon of interest in the data it is evaluated on, and possibly also trained on, must resemble those of the environment in which it is deployed(Hullman et al., 2022), even though the true values of these is unknowable (Welty et al., 2019). Crucially and often ignored in Machine Learning, the resulting inputs from humans must also show qualities fitting of good measurements (Jacobs & Wallach, 2021; Welty et al., 2019), and account for the possibility of a range of reasonable interpretations (Aroyo & Welty, 2015; Cabitza et al., 2023).\nThe social sciences have developed sampling methods to represent populations (Groves et al., 2009), and syntheses that introduce knowledge from survey science (Beck et al., 2022), metrology(Welty et al., 2019), psychometrics(Jacobs & Wallach, 2021), and the perspective approach to ground-truthing(Cabitza et al., 2023) have been published. Yet, knowledge from these fields has broadly not been applied in the field of Machine Learning, leading to issues of representation and measurement(Hullman et al., 2022).\nPresent work attempts to demonstrate the potential of synthesizing knowledge from the social sciences related to sampling and measurement with extant work on data sets for use in machine learning. Building on prior work reviewed herein, its main contribution is a case study investigation, spanning several manuscripts, of a complex evaluation data set creation problem. Included in this thesis are two manuscripts that further motivate the case study: the first reviews strengths and weaknesses of datasets used in the field of Recommender Systems, the second reviews poor practices in Signal Processing related to datasets whose interconnections were poorly reported, misleading results. The case study demonstrates the use of principles from the Social Sciences to solve problems of representation and measurement: personal values in text. It spans two forms of text, expected to vary in terms of use of ambiguous language: song lyrics and political speeches, and demonstrates a moderate success in automatically estimating values in lyrics and a failure in speeches. It includes recommendations for analyses to observe the potential for success or failure, and to estimate cost via less expensive pilot studies. This thesis follows the case study highlighting the potential for shortcomings in the interpretation of AI system evaluations should a more sophisticated framework for evaluation not be adopted, and highlighting"
  },
  {
    "objectID": "index.html#present-work",
    "href": "index.html#present-work",
    "title": "Advancing Perspectivist Ground Truthing with Social Science",
    "section": "",
    "text": "Present work attempts to demonstrate the potential of synthesizing knowledge from the social sciences related to sampling and measurement with extant work on data sets for use in machine learning. Building on prior work reviewed herein, its main contribution is a case study investigation, spanning several manuscripts, of a complex evaluation data set creation problem. A secondary contribution is a synthesized framework that can be used to ground challenging phenomena in various media, following principles from prior work. A third contribution, is knowledge directly applicable to the grounding of personal values in text, such as our adapted questionnaire, and statistics of interest for planning and estimating the costs associated with such ground-truthing projects. A final contribution is immediately applicable results that work towards estimating personal values in song lyrics using language models.\nIncluded in this thesis are two manuscripts that further motivate the case study: 1) the first reviews strengths and weaknesses of datasets used in the field of Recommender Systems, 2) the second reviews poor practices in Signal Processing related to datasets whose interconnections were poorly reported, misleading results. The case study demonstrates the use of principles from the Social Sciences to solve problems of representation and measurement across 4 manuscripts: personal values in song lyrics. In a 5th manuscript, the same principles are applied to a second form of text, political speeches, expected to vary in terms of use of ambiguous language. Despite the moderate success in automatically estimating values in lyrics, this work demonstrates a failure with speeches. It includes recommendations for analyses to observe the potential for success or failure, and to estimate cost via less expensive pilot studies. This thesis follows the case study with work that 6) highlights the potential for shortcomings in the interpretation of AI system evaluations should a more epistemologically sophisticated framework for evaluation not be adopted, and 7) highlights an important component of scientific infrastructure needed for rigorous work on data sets for Machine Learning: the treatment of scientific work as open-source artifacts."
  },
  {
    "objectID": "index.html#present-work-2",
    "href": "index.html#present-work-2",
    "title": "Advancing Perspectivist Ground Truthing with Social Science",
    "section": "4.1 Present Work",
    "text": "4.1 Present Work\nThis thesis incorporates techniques and considerations from the social sciences to address the aforementioned shortcomings.\nshowcases a design for a challenging ground-truthing project, in terms of the complexity of the phenomenon of interest, ambiguity in the media that selected and annotated. It incorporates design choices to address the aforementioned shortcomings into a singular framework, guided by best practices in the social sciences, which it then extends. It focuses unambiguously on the aspect most relevant to the\nSpecifically:\n\nWe attempt to mitigate representation biases in the content we select for annotation by using a stratified sampling strategy.\nWe attempt to mitigate measurement biases by treating the target measurement as a latent variable, and the survey we used to gather annotations as an instrument. We build on work that validated a questionnaire for measuring constructs, and estimating its reliability and structural validity when used for annotations.\nWe account for the potential of multiple perspectives in our dataset by recruiting participants from relevant subgroups in a single target population.\nWe report the details of the annotation collection process, and share the disaggregated dataset of the annotations\nWe further show how to estimate the number of annotators\n\n\nWe demonstrate the potential of this framework by grounding a complex phenomenon (a 10-dimensional construct, Personal Values) in ambiguous text (song lyrics). We further show an\nWhen sampling content to include in training/test datasets, samples for the training/test sets will ideally be drawn from the same distribution as the content in which they will eventually be deployed. Representation bias in content selected for training and/or evaluation datasets refers to the degree to which relevant distributions in data used to train and/or evaluate systems resembles the distribution in the environment to which it will be deployed (Hullman et al., 2022).\nMeasurement bias in the annotations collected from humans may also bias\nPerspectivism\nReporting\nWe add: a priori rating number estimation\nAnd although imperfect as leaderboard scores can be gamed, and do not perfectly represent the deployment environment, the typical leaderboard approach has shown evidence that progress can be made towards a target. This thesis thus represents an attempt to define the target better."
  },
  {
    "objectID": "index.html#ai-relies-on-reference-data-from-human-annotations",
    "href": "index.html#ai-relies-on-reference-data-from-human-annotations",
    "title": "Advancing Perspectivist Ground Truthing with Social Science",
    "section": "2.1 AI relies on reference data from human annotations",
    "text": "2.1 AI relies on reference data from human annotations\nReference data uses responses from humans in the form of annotations very often (Geiger et al., 2021): “data annotation is the practice of labeling a set of digital representations of objects.” (Cabitza et al., 2023). Geiger et al. (2021) systematically review 200 randomly sampled papers from 3 broad domains, Social Sciences & Humanities, Life & Biomedical Sciences and Physical & Environmental Sciences, using a content-analysis approach. 70% (or 140 papers) were classification tasks, of which 73.05% (or 103 papers) used labels derived from human responses as the reference.\nOften multiple ratings per piece of content are collected and aggregated, forming a singular ‘ground truth’ for the aspect of the content being labelled or rated. Thus, it is assumed that there is a singular canonical truth for each aspect / content pair, comprised of aggregated human responses, which forms a target to which we align our automated systems.\nAn investigation of 15 data science workers, Muller et al. (2021) observed common phases, which include determining the annotation scheme - all possible labels that can be attributed to digital representations of objects along with any relevant guidelines, the actual process of collecting labels, and the process by which the annotations are then aggregated into a single label. They note the difficulty of this work: issues in the annotation schemes are often discovered as annotation projects progress, requiring varying degrees of improvised adjustment.\nFurthermore, data sets are often re-used: 56.31% (or 58 papers) used only ‘external’ human labels, i.e. labels that were not collected specifically for the work in the paper (Geiger et al., 2021). Hullman et al. (2022) compare claims that ML is facing a reproducibility crisis to the crisis in psychology. Among the issues they note relate to benchmark datasets, which researchers often re-use as they publish on standardized benchmarks, and because they are cost prohibitive to collect.\nGriffin & Brenner (2004) review errors and biases in human judgements5"
  },
  {
    "objectID": "index.html#reference-data-comes-from-humans",
    "href": "index.html#reference-data-comes-from-humans",
    "title": "Advancing Perspectivist Ground Truthing with Social Science",
    "section": "2.1 Reference data comes from humans",
    "text": "2.1 Reference data comes from humans\nAI runs on data generated by humans. Reference data often uses responses from humans in the form of responses called labels or annotations. “[D]ata annotation is the practice of labeling a set of digital representations of objects” (Cabitza et al., 2023). Although Few studies have systematically examined the frequency of use of data from humans, those that have observed that almost all reference data includes human input either explicitly or implicitly. Geiger et al. (2021) systematically review 200 randomly sampled papers from 3 broad domains, Social Sciences & Humanities, Life & Biomedical Sciences and Physical & Environmental Sciences. Out of the 140 studies that were classification tasks, 73.05% (or 103 papers) used labels derived from human responses as the reference. Geiger et al. (2020) reviewed 164 papers whose classifiers were trained on Twitter data and observed that 65% of the works reviewed used human annotations for the purposes of training. They further note that this quantity did not include human annotations used for validation, or other meta-data e.g. hashtags contributed by humans. In some domains the contribution of humans is in the form of digital traces, as in the domain of Recommender Systems where it was observed that, out of the most highly cited papers between 2018 and 2022, 86% of the datasets used were transaction data released by vendors such as Amazon or Yelp (Sav et al., 2023). Whether human input is explicit or implicit, it is present in almost all reference data.\nFurthermore, training/reference data sets are often re-used. Geiger et al. (2021) observed that 56.31% of the classification tasks that were reviewed (or 58 papers) used only ‘external’ human labels, i.e. labels that were not collected specifically for the work in the paper, Geiger et al. (2020) observed that 33.3% of the papers used external annotations, and Sav et al. (2023) observed that just 4 datasets appeared in at least 10% of works reviewed, with the most commonly used dataset appearing in 33% of the works reviewed. Examining the most highly cited papers in IEEE CVPR from 2020-2022, the initial papers announcing the benchmark, along with the training and reference data received citation counts in the tens of thousands: Imagenet (Deng et al., 2009) shows over 52k citations, COCO (Lin et al., 2014) shows over 29k, Pascal VOC (Everingham et al., 2010) shows over 15k, according to SCOPUS as of April 2025. Thus, these human input datasets have the potential for massive impact.\nAlthough far more emphasis is placed on whether models achieve state of the art ‘performance’ or efficiency (Birhane et al., 2022), scholars over the past decade have attempted to draw attention to a lack of sophistication in how training and reference data are selected and evaluated (Aroyo & Welty, 2015). It has been argued that a focus on improving the data for a given task, will result in bigger gains than a focus on improving model5. Importantly, Hullman et al. (2022) show that shortcomings in reference/training data cannot be compensated for by the size of the dataset overall, a situation exacerbated by the often re-use of data sets. A solution rather entails acknowledging that, whether deliberate or not, informed or not, organized or improvised, data are collected by design (Muller et al., 2021)."
  },
  {
    "objectID": "index.html#the-data-work-is-design-work",
    "href": "index.html#the-data-work-is-design-work",
    "title": "Advancing Perspectivist Ground Truthing with Social Science",
    "section": "2.2 The Data Work is Design Work",
    "text": "2.2 The Data Work is Design Work\nConsidering the ever-presence of human influence on the reference data, best practices, considerations, and frameworks from the social sciences could inform designs, but have yet to be broadly applied in the computational sciences (Beck et al., 2022; Jacobs & Wallach, 2021). One reason for this gap may be ML researchers prefer to work on building systems and evaluating their performance rather than researching, designing and executing ground-truthing projects (Muller et al., 2021; Sambasivan et al., 2021). Another may be a lack of focus on these topics in textbooks, and thus in education more broadly (Geiger et al., 2020). A third may be that the social and computational sciences have conceptually different focci: the computational sciences focus on the statistical model the system with substantially less emphasis on the content, whereas the social sciences treat the statistical model as a means to better understanding the relationships in the content (Liem et al., 2018). Psychology research thus contains many more research projects in which datasets are collected using responses from people, whereas datasets tend to be re-used extensively in machine learning work (Geiger et al., 2021). A further more practical complication is that work on these topics and potential solutions lacks a central academic ‘home’: where psychology and economics have psychometrics and econometrics respectively, and where software engineering has software testing, the study of ground-truthing lacks a central banner under which academic work can accumulate and disseminate.\nCommonly observed shortcomings include 1) representational biases in the content sampled for inclusion in training/evaluation datasets (Hullman et al., 2022), 2) measurement biases in the annotations collected (Beck et al., 2022; Hullman et al., 2022; Jacobs & Wallach, 2021), 3) a fallacious assumption of a single canonical ‘ground-truth’ (Aroyo & Welty, 2015; Cabitza et al., 2023), and 4) poor reporting of necessary information regarding the annotation-collection process (Geiger et al., 2021; Hullman et al., 2022). An additional consideration that receives little attention is the number of annotations to gather, where fields that focus on gathering data from humans also have a strong emphasis on a-priori decisions to mitigate bias, such as the pre-registration of calculated of target sample sizes estimated via statistical power analysis (Cohen, 1992). These considerations are absent in computational fields which appear to favor differing rules of thumb: e.g. in a well-cited textbook, Pustejovsky & Stubbs (2013) suggest to “have your corpus annotated by at least two people (more is preferable, but not always practical)”, whereas Artstein & Poesio (2008) suggest that “measuring reliability with only two coders is seldom enough, except for small-scale studies”.\nDecisions such as the selection of items for training data (Hullman et al., 2022), and the collection of human responses for reference data (Beck et al., 2022), are part of a design of a process that results in data. On the one hand, corpora tend to be very large, and resources are finite making cost a primary factor in design decisions (Muller et al., 2021). On the other hand, rules of thumb lack clear substantiation in light of the both 1) the phenomenon being grounded and 2) the ambiguity of the media in which it is grounded. In other words, more variance is expected in annotation targets to the degree they are subjective or based on opinion (Beck et al., 2022), and more variance is expected in content to the degree to which it is ambiguous - i.e. can be interpreted in multiple ways - such as figurative language (Sandri et al., 2023). Further, some degree of variance will always be present when there are multiple annotations or ratings for a given piece of media independent of the target (cabitza?), based on the range of reasonable interpretations of that target in that media (arroyo?)."
  },
  {
    "objectID": "index.html#human-annotations-arent-always-accurate",
    "href": "index.html#human-annotations-arent-always-accurate",
    "title": "Advancing Perspectivist Ground Truthing with Social Science",
    "section": "3.1 Human annotations aren’t always accurate",
    "text": "3.1 Human annotations aren’t always accurate\nGriffin & Brenner (2004) review errors and biases in human judgements6\nGriffin & Brenner (2004) review errors and biases in human judgements7\n\nover/under prediction: confidence score is higher/lower than accuracy\nover/under extremity: confidence is more extreme at ends\n\nalso reviews possible reasons:\n\noptimistic overconfidence\nconfirmation bias\ncase-based judgment\necological probability\nerror model (psychometric model)"
  },
  {
    "objectID": "index.html#ml-treats-all-annotation-variance-as-noise-rather-than-signal",
    "href": "index.html#ml-treats-all-annotation-variance-as-noise-rather-than-signal",
    "title": "Advancing Perspectivist Ground Truthing with Social Science",
    "section": "2.3 ML treats all annotation variance as noise rather than signal",
    "text": "2.3 ML treats all annotation variance as noise rather than signal\nOften multiple ratings per piece of content are collected and aggregated, forming a singular ‘ground truth’ for the aspect of the content being labelled or rated. Thus, it is assumed that there is a singular canonical truth for each aspect / content pair, comprised of aggregated human responses, which forms a target to which we align our automated systems.\ndisagreement is common\nreviewed in Cabitza et al. (2023): -social media content: Chandrasekharan 2017 -medical cases: Cabitza 2019 -various NLP tasks Aroyo & Welty (2015)\ndisagreement is often removed\n\nadjusting annotator training and instruction\nadjusting annotations via discussion post-collection\nmajority voting, post-hoc without annotators\n\nBeyond errors in judgment are questions about the target for the annotations. For at least some phenomena, the assumption that there is a single ground-truth to approximate with annotations doesn’t hold.\nAroyo & Welty (2015)\n\nIn order to evaluate ML / AI systems, we compare the output of these systems to reference data.\nOne method for creating reference data is the collection of human annotations.\nThis method typically assumes that, for every piece of content being annotated, there is a single canonical truth\nquality of annotations is assessed using inter-annotator agreement, where more agreement = better annotations\n\nAroyo & Welty (2015)\n7 ‘myths’ of human annotation:\n\nthere is one truth\ndisagreement is bad\ndetailed guidelines help\nexperts are better\none annotator is enough\nall items are created equal\nonce done, forever valid\n\nFor myths 1 and 2:\n\nlist examples from NLP where the disagreement from annotators is sensible\nthey argue that the assumptions of a single ground truth, and that disagreement is indicative of poor annotations are both false.\n\nfor myth 6: disagreement indicates that the media being rated is ambiguous."
  },
  {
    "objectID": "index.html#annotations-aim-to-measure-a-latent-variable",
    "href": "index.html#annotations-aim-to-measure-a-latent-variable",
    "title": "Advancing Perspectivist Ground Truthing with Social Science",
    "section": "2.4 annotations aim to measure a latent variable",
    "text": "2.4 annotations aim to measure a latent variable\nJacobs & Wallach (2021) there is a ‘measurement error model’ (taken from econ) that links the unobservable latent variable, and observable properties. in annotations this is via individual observations\nalthough paper focuses on attempts at measuring constructs (risk of recidivism, teacher effectiveness, patient benefit) they also show that even ‘representational measurements’ like height, are essentially a latent variable\nAroyo & Welty (2015) operationalize ‘crowd truth’ with an illustration where the ‘gold standard’ is the probability that a sentence contains an element, based on the probability that an annotator annotated that sentence with that element.\n\ni.e. the label isn’t represented as ‘present’ or ‘not present’, but as a probablility\nthus the ‘crowd truth’ attempts to capture the ‘range of reasonable interpretations’\n\nBeck et al. (2022): we should expect more variance to the degree that tasks measure opinion show work on an intuitively perspective-based use-case: hate speech"
  },
  {
    "objectID": "index.html#inadequate-reporting",
    "href": "index.html#inadequate-reporting",
    "title": "Advancing Perspectivist Ground Truthing with Social Science",
    "section": "2.5 Inadequate reporting",
    "text": "2.5 Inadequate reporting\nGeiger et al. (2021) ML science studies inadequately report ‘ground truth’\nHullman et al. (2022) thus we cannot know what data generating process the resulting model represents\n[perhaps cat image parable here?]\nAn investigation of 15 data science workers, Muller et al. (2021) observed common phases, which include determining the annotation scheme - all possible labels that can be attributed to digital representations of objects along with any relevant guidelines, the actual process of collecting labels, and the process by which the annotations are then aggregated into a single label. They note the difficulty of this work: issues in the annotation schemes are often discovered as annotation projects progress, requiring varying degrees of improvised adjustment."
  },
  {
    "objectID": "index.html#sampling-and-measurement-biases",
    "href": "index.html#sampling-and-measurement-biases",
    "title": "Advancing Perspectivist Ground Truthing with Social Science",
    "section": "2.6 sampling and measurement biases",
    "text": "2.6 sampling and measurement biases\nHullman et al. (2022)\nWith regards to reference data:\n\nrepresentation bias / non-representative samples\nmeasurement bias / unvalidated measurement instruments\nunderspecification of portions of input space in training data\ntransformation of data to optimize for ‘accuracy’\nlack of or poor dataset documentation\n\nIn other words, optimizing for predictive accuracy using very large datasets does not ‘absolve’ researchers from having to consider the data generating process. They note benefits that both machine learning and psychology could gain by borrowing methods from each other, but note the danger if these are misused. For the benefit of machine learning, there are lessons to be learned from social science, and the replication crisis. Among them are 1) collecting samples whose test/evaluation set distributions are drawn from the same deployment distribution, and 2) using valid measurement instruments."
  },
  {
    "objectID": "index.html#ml-doesnt-treat-annotation-generating-process-as-an-instrument",
    "href": "index.html#ml-doesnt-treat-annotation-generating-process-as-an-instrument",
    "title": "Advancing Perspectivist Ground Truthing with Social Science",
    "section": "2.7 ML doesn’t treat annotation generating process as an instrument",
    "text": "2.7 ML doesn’t treat annotation generating process as an instrument\nBeck et al. (2022)\n\nannotation collection requires design thinking\n\nTask Structure: specific wording and response options, including debates over the inclusion of “I don’t Know” option\nOrder Effects: specific judgements are affected by previous perceptions\nAnnotator Effects: backgrounds, opinions, experiences of respondents affect responses\n\n\nJacobs & Wallach (2021)\n\nreliability: do similar inputs to a measurement model present similar outputs?\n\ntest-retest: are measurements of an unobservable latent construct taken at different times via a measurement model similar, assuming the construct hasn’t changed?\n\nvalidity: is it ‘right’?\n\nno single test for validity on purpose, because it requires thinking. do our measurements:\n\nface validity: look plausible/ sensible?\ncontent validity: capture the construct?\n\nstructural validity: show the inter-correlations we expect?\nsubstantive validity: capture only observable properties thought to be related to the construct?\n\nconvergent validity: show correlations with other validated methods?\ndiscriminant validity: show correlations with other construct/properties thought not to be related to the construct?\npredictive validity: show correlations with constructs/properties thought to be related, but not in the operationalization?\nhypothesis validity: shed light on relevant hypotheses about the construct being measured?\nconsequential validity: allow for the consequences obtained from the measurement model to be assessed?"
  },
  {
    "objectID": "index.html#ml-ignores-perspectives-of-annotators",
    "href": "index.html#ml-ignores-perspectives-of-annotators",
    "title": "Advancing Perspectivist Ground Truthing with Social Science",
    "section": "2.8 ML ignores perspectives of annotators",
    "text": "2.8 ML ignores perspectives of annotators\nCabitza et al. (2023): whether the target of the annotation is a subjective phenomenon or not, disagreement is always irreducible. Yet ML typically assumes there is a single ‘ground truth’, and its best indicator is inter-annotator agreement. But taking the perspectives of the annotators into account, both in the data annotation but also the modelling phase of ML projects has recently been shown to benefit ML modelling in a number of contexts.\nweak perspectivist approach: taking perspectives into account while designing and collecting annotations, but ultimately reducing annotations to a single label or rating.\nstrong perspectivist approach: taking perspectives into account for ground truthing and modelling phases.\nbenefits of this approach:\n\nis congruent with the reality of collecting annotations\nincludes the signal in the variance of labels or ratings\navoids majority group perspective appearing to be ‘objective’\nallows for the modelling of human errors and variances\nallows for uncertain, fuzzy, or soft model development\nmore complete report of the data generating process, as it also reports uncertainty\n\ndownsides:\n\nmultiple raters, and therefore costs/time/rater availability are issues\nneed for perspectivist ML approaches\nvalidation becomes more challenging\n\nrecommendations:\n\ncomplete labeling schemes, including ‘i don’t know’, ‘none of these’ etc. categories, and the ability to express issues with label set\nenough raters\nheterogenous raters\nadequate reporting:\n\nnumber of raters,\nrater expertise\nincentive\ninstructions\nlength of time for labelling\ninter rater agreement\naggregation method\nconfidence"
  },
  {
    "objectID": "index.html#shortcomings-and-improvements-to-reference-data-design",
    "href": "index.html#shortcomings-and-improvements-to-reference-data-design",
    "title": "Advancing Perspectivist Ground Truthing with Social Science",
    "section": "2.2 Shortcomings and improvements to reference data design",
    "text": "2.2 Shortcomings and improvements to reference data design\nConsidering the ever-presence of human influence on the reference data, best practices, considerations, and frameworks from the social sciences could inform designs, but have yet to be broadly applied in the computational sciences (Beck et al., 2022; Jacobs & Wallach, 2021). One reason for this gap may be ML researchers prefer to work on building systems and evaluating their performance rather than researching, designing and executing ground-truthing projects (Muller et al., 2021; Sambasivan et al., 2021). Another may be a lack of focus on these topics in textbooks, and thus in education more broadly (Geiger et al., 2020). A third may be that the social and computational sciences have conceptually different focci: the computational sciences focus on the statistical model the system with substantially less emphasis on the content, whereas the social sciences treat the statistical model as a means to better understanding the relationships in the content (Liem et al., 2018). Psychology research thus contains many more research projects in which datasets are collected using responses from people, whereas datasets tend to be re-used extensively in machine learning work (Geiger et al., 2021). A further more practical complication is that work on these topics and potential solutions lacks a central academic ‘home’: where psychology and economics have psychometrics and econometrics respectively, and where software engineering has software testing, the study of ground-truthing lacks a central banner under which academic work can accumulate and disseminate.\nCommonly observed shortcomings include 1) representational biases in the content sampled for inclusion in training/evaluation datasets (Hullman et al., 2022), 2) measurement biases in the annotations collected (Beck et al., 2022; Hullman et al., 2022; Jacobs & Wallach, 2021), 3) a fallacious assumption of a single canonical ‘ground-truth’ (Aroyo & Welty, 2015; Cabitza et al., 2023), and 4) poor reporting of necessary information regarding the annotation-collection process (Geiger et al., 2021; Hullman et al., 2022). An additional consideration that receives little attention is the number of annotations to gather, where fields that focus on gathering data from humans also have a strong emphasis on a-priori decisions to mitigate bias, such as the pre-registration of calculated of target sample sizes estimated via statistical power analysis (Cohen, 1992). These considerations are absent in computational fields which appear to favor differing rules of thumb: e.g. in a well-cited textbook, Pustejovsky & Stubbs (2013) suggest to “have your corpus annotated by at least two people (more is preferable, but not always practical)”, whereas Artstein & Poesio (2008) suggest that “measuring reliability with only two coders is seldom enough, except for small-scale studies”.\nDecisions such as the selection of items for training data (Hullman et al., 2022), and the collection of human responses for reference data (Beck et al., 2022), are part of a design of a process that results in data. On the one hand, corpora tend to be very large, and resources are finite making cost a primary factor in design decisions (Muller et al., 2021). On the other hand, rules of thumb lack clear substantiation in light of the both 1) the phenomenon being grounded and 2) the ambiguity of the media in which it is grounded. In other words, more variance is expected in annotation targets to the degree they are subjective or based on opinion (Beck et al., 2022), and more variance is expected in content to the degree to which it is ambiguous - i.e. can be interpreted in multiple ways - such as figurative language (Sandri et al., 2023). Further, some degree of variance will always be present when there are multiple annotations or ratings for a given piece of media independent of the target (cabitza?), based on the range of reasonable interpretations of that target in that media (arroyo?)."
  },
  {
    "objectID": "index.html#a-tale-of-two-studies-the-case-of-personnel-selection",
    "href": "index.html#a-tale-of-two-studies-the-case-of-personnel-selection",
    "title": "Advancing Perspectivist Ground Truthing with Social Science",
    "section": "2.3 A tale of two studies: the case of personnel selection",
    "text": "2.3 A tale of two studies: the case of personnel selection\nPonce-López et al. (2016)\n\nefficient way to gather media and annotation data\nBUT no validation of instrument, or ecologically valid media data\ndistribution of training /eval data don’t come from the target distribution\n\n Figure 2: Ponce-López et al. (2016)\nCompared to Koutsoumpis et al. (2024):\n\necological validity: media data was mock asynchronous video interviews\necological validity: interview questions designed to activate personality facets\npersonality instruments: validated HEXACO scale\nperspectives: self & observer ratings\n\n\n2.3.1 Human judgments are imperfect\nGriffin & Brenner (2004) review errors and biases in human judgements6\nGriffin & Brenner (2004) review errors and biases in human judgements7\n\nover/under prediction: confidence score is higher/lower than accuracy\nover/under extremity: confidence is more extreme at ends\n\nalso reviews possible reasons:\n\noptimistic overconfidence\nconfirmation bias\ncase-based judgment\necological probability\nerror model (psychometric model)\n\n\n\n2.3.2 ML treats all annotation variance as noise rather than signal\nOften multiple ratings per piece of content are collected and aggregated, forming a singular ‘ground truth’ for the aspect of the content being labelled or rated. Thus, it is assumed that there is a singular canonical truth for each aspect / content pair, comprised of aggregated human responses, which forms a target to which we align our automated systems.\ndisagreement is common\nreviewed in Cabitza et al. (2023): -social media content: Chandrasekharan 2017 -medical cases: Cabitza 2019 -various NLP tasks Aroyo & Welty (2015)\ndisagreement is often removed\n\nadjusting annotator training and instruction\nadjusting annotations via discussion post-collection\nmajority voting, post-hoc without annotators\n\nBeyond errors in judgment are questions about the target for the annotations. For at least some phenomena, the assumption that there is a single ground-truth to approximate with annotations doesn’t hold.\nAroyo & Welty (2015)\n\nIn order to evaluate ML / AI systems, we compare the output of these systems to reference data.\nOne method for creating reference data is the collection of human annotations.\nThis method typically assumes that, for every piece of content being annotated, there is a single canonical truth\nquality of annotations is assessed using inter-annotator agreement, where more agreement = better annotations\n\nAroyo & Welty (2015)\n7 ‘myths’ of human annotation:\n\nthere is one truth\ndisagreement is bad\ndetailed guidelines help\nexperts are better\none annotator is enough\nall items are created equal\nonce done, forever valid\n\nFor myths 1 and 2:\n\nlist examples from NLP where the disagreement from annotators is sensible\nthey argue that the assumptions of a single ground truth, and that disagreement is indicative of poor annotations are both false.\n\nfor myth 6: disagreement indicates that the media being rated is ambiguous.\n\n\n2.3.3 Inadequate reporting\nGeiger et al. (2021) ML science studies inadequately report ‘ground truth’\nHullman et al. (2022) thus we cannot know what data generating process the resulting model represents\n[perhaps cat image parable here?]\nAn investigation of 15 data science workers, Muller et al. (2021) observed common phases, which include determining the annotation scheme - all possible labels that can be attributed to digital representations of objects along with any relevant guidelines, the actual process of collecting labels, and the process by which the annotations are then aggregated into a single label. They note the difficulty of this work: issues in the annotation schemes are often discovered as annotation projects progress, requiring varying degrees of improvised adjustment.\n\n\n2.3.4 sampling and measurement biases\nHullman et al. (2022)\nWith regards to reference data:\n\nrepresentation bias / non-representative samples\nmeasurement bias / unvalidated measurement instruments\nunderspecification of portions of input space in training data\ntransformation of data to optimize for ‘accuracy’\nlack of or poor dataset documentation\n\nIn other words, optimizing for predictive accuracy using very large datasets does not ‘absolve’ researchers from having to consider the data generating process. They note benefits that both machine learning and psychology could gain by borrowing methods from each other, but note the danger if these are misused. For the benefit of machine learning, there are lessons to be learned from social science, and the replication crisis. Among them are 1) collecting samples whose test/evaluation set distributions are drawn from the same deployment distribution, and 2) using valid measurement instruments.\n\n\n2.3.5 ML doesn’t treat annotation generating process as an instrument\nBeck et al. (2022)\n\nannotation collection requires design thinking\n\nTask Structure: specific wording and response options, including debates over the inclusion of “I don’t Know” option\nOrder Effects: specific judgements are affected by previous perceptions\nAnnotator Effects: backgrounds, opinions, experiences of respondents affect responses\n\n\nJacobs & Wallach (2021)\n\nreliability: do similar inputs to a measurement model present similar outputs?\n\ntest-retest: are measurements of an unobservable latent construct taken at different times via a measurement model similar, assuming the construct hasn’t changed?\n\nvalidity: is it ‘right’?\n\nno single test for validity on purpose, because it requires thinking. do our measurements:\n\nface validity: look plausible/ sensible?\ncontent validity: capture the construct?\n\nstructural validity: show the inter-correlations we expect?\nsubstantive validity: capture only observable properties thought to be related to the construct?\n\nconvergent validity: show correlations with other validated methods?\ndiscriminant validity: show correlations with other construct/properties thought not to be related to the construct?\npredictive validity: show correlations with constructs/properties thought to be related, but not in the operationalization?\nhypothesis validity: shed light on relevant hypotheses about the construct being measured?\nconsequential validity: allow for the consequences obtained from the measurement model to be assessed?\n\n\n\n2.3.6 ML ignores perspectives of annotators\nCabitza et al. (2023): whether the target of the annotation is a subjective phenomenon or not, disagreement is always irreducible. Yet ML typically assumes there is a single ‘ground truth’, and its best indicator is inter-annotator agreement. But taking the perspectives of the annotators into account, both in the data annotation but also the modelling phase of ML projects has recently been shown to benefit ML modelling in a number of contexts.\nweak perspectivist approach: taking perspectives into account while designing and collecting annotations, but ultimately reducing annotations to a single label or rating.\nstrong perspectivist approach: taking perspectives into account for ground truthing and modelling phases.\nbenefits of this approach:\n\nis congruent with the reality of collecting annotations\nincludes the signal in the variance of labels or ratings\navoids majority group perspective appearing to be ‘objective’\nallows for the modelling of human errors and variances\nallows for uncertain, fuzzy, or soft model development\nmore complete report of the data generating process, as it also reports uncertainty\n\ndownsides:\n\nmultiple raters, and therefore costs/time/rater availability are issues\nneed for perspectivist ML approaches\nvalidation becomes more challenging\n\nrecommendations:\n\ncomplete labeling schemes, including ‘i don’t know’, ‘none of these’ etc. categories, and the ability to express issues with label set\nenough raters\nheterogenous raters\nadequate reporting:\n\nnumber of raters,\nrater expertise\nincentive\ninstructions\nlength of time for labelling\ninter rater agreement\naggregation method\nconfidence"
  },
  {
    "objectID": "index.html#shortcomings-of-reference-data-design",
    "href": "index.html#shortcomings-of-reference-data-design",
    "title": "Advancing Perspectivist Ground Truthing with Social Science",
    "section": "2.2 Shortcomings of reference data design",
    "text": "2.2 Shortcomings of reference data design\nConsidering the ever-presence of human influence on the reference data, best practices, considerations, and frameworks from the social sciences could inform designs, but have yet to be broadly applied in the computational sciences (Beck et al., 2022; Jacobs & Wallach, 2021). One reason for this gap may be ML researchers prefer to work on building systems and evaluating their performance rather than researching, designing and executing ground-truthing projects (Muller et al., 2021; Sambasivan et al., 2021). Another may be a lack of focus on these topics in textbooks, and thus in education more broadly (Geiger et al., 2020). A third may be that the social and computational sciences have conceptually different focci: the computational sciences focus on the statistical model the system with substantially less emphasis on the content, whereas the social sciences treat the statistical model as a means to better understanding the relationships in the content (Liem et al., 2018). Psychology research thus contains many more research projects in which datasets are collected using responses from people, whereas datasets tend to be re-used extensively in machine learning work (Geiger et al., 2021). A further more practical complication is that work on these topics and potential solutions lacks a central academic ‘home’: where psychology and economics have psychometrics and econometrics respectively, and where software engineering has software testing, the study of ground-truthing lacks a central banner under which academic work can accumulate and disseminate.\nCommonly observed shortcomings include 1) representational biases in the content sampled for inclusion in training/evaluation datasets (Hullman et al., 2022), 2) measurement biases in the annotations collected (Beck et al., 2022; Hullman et al., 2022; Jacobs & Wallach, 2021), 3) a fallacious assumption of a single canonical ‘ground-truth’ (Aroyo & Welty, 2015; Cabitza et al., 2023), and 4) poor reporting of necessary information regarding the annotation-collection process (Geiger et al., 2021; Hullman et al., 2022). An additional consideration that receives little attention is the number of annotations to gather, where fields that focus on gathering data from humans also have a strong emphasis on a-priori decisions to mitigate bias, such as the pre-registration of calculated of target sample sizes estimated via statistical power analysis (Cohen, 1992). These considerations are absent in computational fields which appear to favor differing rules of thumb: e.g. in a well-cited textbook, Pustejovsky & Stubbs (2013) suggest to “have your corpus annotated by at least two people (more is preferable, but not always practical)”, whereas Artstein & Poesio (2008) suggest that “measuring reliability with only two coders is seldom enough, except for small-scale studies”.\nDecisions such as the selection of items for training data (Hullman et al., 2022), and the collection of human responses for reference data (Beck et al., 2022), are part of a design of a process that results in data. On the one hand, corpora tend to be very large, and resources are finite making cost a primary factor in design decisions (Muller et al., 2021). On the other hand, rules of thumb lack clear substantiation in light of the both 1) the phenomenon being grounded and 2) the ambiguity of the media in which it is grounded. In other words, more variance is expected in annotation targets to the degree they are subjective or based on opinion (Beck et al., 2022), and more variance is expected in content to the degree to which it is ambiguous - i.e. can be interpreted in multiple ways - such as figurative language (Sandri et al., 2023). Further, some degree of variance will always be present when there are multiple annotations or ratings for a given piece of media independent of the target (cabitza?), based on the range of reasonable interpretations of that target in that media (arroyo?)."
  }
]