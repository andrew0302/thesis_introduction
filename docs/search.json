[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Advancing Perspectivist Ground Truthing with Social Science",
    "section": "",
    "text": "AI systems run on data. Data are used to ‘train’ models - imperfect, simplified mathematical or computational representations of a phenomenon or process in the real world 1. Where an AI system is a complete application with integrated components e.g. an interface, programmatic logic, and one or more models aimed at performing tasks typically requiring human intelligence, the models themselves are embedded components that take inputs (e.g. media like text, images, audio, or data) and produce outputs (e.g. classifications, predictions or some form of decision). The behavior of models, i.e. how they respond to a given input, is determined by their parameters - internal settings or values. Algorithms - step by step instructions, executed in order - are used to estimate model parameters from the ‘training’ data. As models used in AI systems are designed to perform tasks, their performance is evaluated empirically by comparing their outputs to a reference. This reference is often a second form of data, referred to as the ‘ground truth’, ‘gold standard’, or simply ‘annotations’, which represents the ideal output of the system.\nTraining data may be tabular data, but is often a form of media - text, audio, images, or video - whereas reference data very often contains aggregated input from humans. This implies that a phenomenon of interest is set as a target, and human judges are given a task that produces data relevant to the target from their input. Commonscenarios include human judges annotating, labeling, or rating a) individual pieces of content of the same form as the training data, or b) generated system outputs. Multiple ratings per piece of content are collected and usually aggregated, forming a singular ‘ground truth’ for the aspect of the content being labelled or rated. Thus, it is assumed that there is a singular canonical truth for each aspect / content pair, comprised of aggregated human responses, which forms a target to which we align our automated systems.\nWhether used as part of the development of an AI system, or to evaluate it, the quality of these two forms of data determines the quality of the system. As parameters are estimated from the training data, such that observable patterns recognized in the data affect internal model values or settings, imperfections, inaccuracies, biases etc. in the data are reflected in the parameters of the resulting model. As models are evaluated by comparing their outputs to reference data, such that ‘better’ models are those whose outputs most closely resemble the reference data, imperfections in the reference are reflected in the models preferred. Thus, the quality of the data used for training and reference represents the upper boundary of potential performance of AI systems when deployed: the best possible performance directly corresponds to the degree to which training and reference data represent the phenomenon of interest to the system when deployed."
  },
  {
    "objectID": "index.html#issues-using-human-annotations-in-ml",
    "href": "index.html#issues-using-human-annotations-in-ml",
    "title": "Advancing Perspectivist Ground Truthing with Social Science",
    "section": "Issues using human annotations in ML",
    "text": "Issues using human annotations in ML\nA number of works have shown issues with annotations in ML\nHullman et al. (2022) compare claims that ML is facing a reproducibility crisis to the crisis in psychology. Among the issues they note relate to benchmark datasets, which researchers often re-use as they publish on standardized benchmarks, and because they are cost prohibitive to collect.\nAnother may be that the skills are not being taught: [geiger, first paper, show lack of reporting in ML textbooks on ground truthing]\nA number of papers have drawn from the social sciences to synthesize knowledge on how best to gather annotations.\n\nML treats all annotation variance as noise rather than signal\ndisagreement is common\nreviewed in Cabitza et al. (2023): -social media content: Chandrasekharan 2017 -medical cases: Cabitza 2019 -various NLP tasks Aroyo & Welty (2015)\ndisagreement is often removed\n\nadjusting annotator training and instruction\nadjusting annotations via discussion post-collection\nmajority voting, post-hoc without annotators\n\nBeyond errors in judgment are questions about the target for the annotations. For at least some phenomena, the assumption that there is a single ground-truth to approximate with annotations doesn’t hold.\nAroyo & Welty (2015)\n7 ‘myths’ of human annotation:\n\nthere is one truth\ndisagreement is bad\ndetailed guidelines help\nexperts are better\none annotator is enough\nall items are created equal\nonce done, forever valid\n\nFor myths 1 and 2:\n\nlist examples from NLP where the disagreement from annotators is sensible\nthey argue that the assumptions of a single ground truth, and that disagreement is indicative of poor annotations are both false.\n\nfor myth 6: disagreement indicates that the media being rated is ambiguous.\n\nannotations aim to measure a latent variable\nJacobs & Wallach (2021) there is a ‘measurement error model’ (taken from econ) that links the unobservable latent variable, and observable properties. in annotations this is via individual observations\nalthough paper focuses on attempts at measuring constructs (risk of recidivism, teacher effectiveness, patient benefit) they also show that even ‘representational measurements’ like height, are essentially a latent variable\nAroyo & Welty (2015) operationalize ‘crowd truth’ with an illustration where the ‘gold standard’ is the probability that a sentence contains an element, based on the probability that an annotator annotated that sentence with that element.\n\ni.e. the label isn’t represented as ‘present’ or ‘not present’, but as a probablility\nthus the ‘crowd truth’ attempts to capture the ‘range of reasonable interpretations’\n\nBeck et al. (2022): we should expect more variance to the degree that tasks measure opinion show work on an intuitively perspective-based use-case: hate speech\n\n\n\nHuman annotations aren’t always accurate\nGriffin & Brenner (2004) review errors and biases in human judgements3\n\nover/under prediction: confidence score is higher/lower than accuracy\nover/under extremity: confidence is more extreme at ends\n\nalso reviews possible reasons:\n\noptimistic overconfidence\nconfirmation bias\ncase-based judgment\necological probability\nerror model (psychometric model)\n\n\n\nInadequate reporting\nGeiger et al. (2021) ML science studies inadequately report ‘ground truth’\nHullman et al. (2022) thus we cannot know what data generating process the resulting model represents\n[perhaps cat image parable here?]\n\n\nsampling and measurement biases\nHullman et al. (2022)\nWith regards to reference data:\n\nrepresentation bias / non-representative samples\nmeasurement bias / unvalidated measurement instruments\nunderspecification of portions of input space in training data\ntransformation of data to optimize for ‘accuracy’\nlack of or poor dataset documentation\n\nIn other words, optimizing for predictive accuracy using very large datasets does not ‘absolve’ researchers from having to consider the data generating process. They note benefits that both machine learning and psychology could gain by borrowing methods from each other, but note the danger if these are misused. For the benefit of machine learning, there are lessons to be learned from social science, and the replication crisis. Among them are 1) collecting samples whose test/evaluation set distributions are drawn from the same deployment distribution, and 2) using valid measurement instruments.\n\n\nML doesn’t treat annotation generating process as an instrument\nBeck et al. (2022)\n\nannotation collection requires design thinking\n\nTask Structure: specific wording and response options, including debates over the inclusion of “I don’t Know” option\nOrder Effects: specific judgements are affected by previous perceptions\nAnnotator Effects: backgrounds, opinions, experiences of respondents affect responses\n\n\nJacobs & Wallach (2021)\n\nreliability: do similar inputs to a measurement model present similar outputs?\n\ntest-retest: are measurements of an unobservable latent construct taken at different times via a measurement model similar, assuming the construct hasn’t changed?\n\nvalidity: is it ‘right’?\n\nno single test for validity on purpose, because it requires thinking. do our measurements:\n\nface validity: look plausible/ sensible?\ncontent validity: capture the construct?\n\nstructural validity: show the inter-correlations we expect?\nsubstantive validity: capture only observable properties thought to be related to the construct?\n\nconvergent validity: show correlations with other validated methods?\ndiscriminant validity: show correlations with other construct/properties thought not to be related to the construct?\npredictive validity: show correlations with constructs/properties thought to be related, but not in the operationalization?\nhypothesis validity: shed light on relevant hypotheses about the construct being measured?\nconsequential validity: allow for the consequences obtained from the measurement model to be assessed?\n\n\n\nML ignores perspectives of annotators\nCabitza et al. (2023): whether the target of the annotation is a subjective phenomenon or not, disagreement is always irreducible. Yet ML typically assumes there is a single ‘ground truth’, and its best indicator is inter-annotator agreement. But taking the perspectives of the annotators into account, both in the data annotation but also the modelling phase of ML projects has recently been shown to benefit ML modelling in a number of contexts.\nweak perspectivist approach: taking perspectives into account while designing and collecting annotations, but ultimately reducing annotations to a single label or rating.\nstrong perspectivist approach: taking perspectives into account for ground truthing and modelling phases.\nbenefits of this approach:\n\nis congruent with the reality of collecting annotations\nincludes the signal in the variance of labels or ratings\navoids majority group perspective appearing to be ‘objective’\nallows for the modelling of human errors and variances\nallows for uncertain, fuzzy, or soft model development\nmore complete report of the data generating process, as it also reports uncertainty\n\ndownsides:\n\nmultiple raters, and therefore costs/time/rater availability are issues\nneed for perspectivist ML approaches\nvalidation becomes more challenging\n\nrecommendations:\n\ncomplete labeling schemes, including ‘i don’t know’, ‘none of these’ etc. categories, and the ability to express issues with label set\nenough raters\nheterogenous raters\nadequate reporting:\n\nnumber of raters,\nrater expertise\nincentive\ninstructions\nlength of time for labelling\ninter rater agreement\naggregation method\nconfidence"
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Advancing Perspectivist Ground Truthing with Social Science",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://en.wikipedia.org/wiki/Scientific_modelling↩︎\nhttps://www.youtube.com/watch?v=06-AZXmwHjo↩︎\nGriffin & Brenner (2004) note that much of this work was about people guessing knowledge from an almanac, and then guessing how accurate they were↩︎"
  },
  {
    "objectID": "index.html#present-work",
    "href": "index.html#present-work",
    "title": "Advancing Perspectivist Ground Truthing with Social Science",
    "section": "Present Work",
    "text": "Present Work\nAlthough works have been published that take the aforementioned\nThis thesis incorporates techniques and considerations from the social sciences to address the aforementioned shortcomings\nshowcases a design for a challenging ground-truthing project, in terms of the complexity of the phenomenon of interest, ambiguity in the media that selected and annotated. It incorporates design choices to address the aforementioned shortcomings into a singular framework, guided by best practices in the social sciences, which it then extends. It focuses unambiguously on the aspect most relevant to the\nSpecifically:\n\nWe attempt to mitigate representation biases in the content we select for annotation by using a stratified sampling strategy.\nWe attempt to mitigate measurement biases by treating the target measurement as a latent variable, and the survey we used to gather annotations as an instrument. We build on work that validated a questionnaire for measuring constructs, and estimating its reliability and structural validity when used for annotations.\nWe account for the potential of multiple perspectives in our dataset by recruiting participants from relevant subgroups in a single target population.\nWe report the details of the annotation collection process, and share the disaggregated dataset of the annotations\nWe further show how to estimate the number of annotators\n\n\nWe demonstrate the potential of this framework by grounding a complex phenomenon (a 10-dimensional construct, Personal Values) in ambiguous text (song lyrics). We further show an\nWhen sampling content to include in training/test datasets, samples for the training/test sets will ideally be drawn from the same distribution as the content in which they will eventually be deployed. Representation bias in content selected for training and/or evaluation datasets refers to the degree to which relevant distributions in data used to train and/or evaluate systems resembles the distribution in the environment to which it will be deployed (Hullman et al., 2022).\nMeasurement bias in the annotations collected from humans may also bias\nPerspectivism\nReporting\nWe add: a priori rating number estimation\nAnd although imperfect as leaderboard scores can be gamed, and do not perfectly represent the deployment environment, the typical leaderboard approach has shown evidence that progress can be made towards a target. This thesis thus represents an attempt to define the target better."
  }
]