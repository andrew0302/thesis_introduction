[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Advancing Perspectivist Ground Truthing with Social Science",
    "section": "",
    "text": "Aroyo & Welty (2015)\n\nIn order to evaluate ML / AI systems, we compare the output of these systems to reference data.\nOne method for creating reference data is the collection of human annotations.\nThis method typically assumes that, for every piece of content being annotated, there is a single canonical truth\nquality of annotations is assessed using inter-annotator agreement, where more agreement = better annotations\n\nCabitza et al. (2023): “data annotation is the practice of labeling a set of digital representations of objects.”\naccording to Muller et al., 2012 it has three components\n\ndata collection: the labeling scheme\ndata annotation: the actual labeling by experts or crowd workers\ndata aggregation: producing a single or a single set of labels from the multiple labels collected\n\nGeiger et al. (2021): ML uses human annotations very often\n\n200 randomly sampled ML papers from 3 domains:\n\nSocial Sciences & Humanities\nLife & Biomedical Sciences\nPhysical & Environmental Sciences\n\nOut of 141 classification tasks, 103 (73.05%) used human labels\nOut of 103 human labels, 58 (56.31%) used only external labels\n\ni.e. ML re-uses external labels\n\n\nA number of works have shown issues with annotations in ML\nHullman et al. (2022) compare claims that ML is facing a reproducibility crisis to the crisis in psychology. Among the issues they note relate to benchmark datasets, which researchers often re-use as they publish on standardized benchmarks, and because they are cost prohibitive to collect.\nOne reason may be: Sambasivan et al. (2021) ML researchers broadly prefer to work on building and evaluating performance, rather than executing ground-truthing projects.\nAnother may be that the skills are not being taught: [geiger, first paper, show lack of reporting in ML textbooks on ground truthing]\nA number of papers have drawn from the social sciences to synthesize knowledge on how best to gather annotations.\n\n\ndisagreement is common\nreviewed in Cabitza et al. (2023): -social media content: Chandrasekharan 2017 -medical cases: Cabitza 2019 -various NLP tasks Aroyo & Welty (2015)\ndisagreement is often removed\n\nadjusting annotator training and instruction\nadjusting annotations via discussion post-collection\nmajority voting, post-hoc without annotators\n\nBeyond errors in judgment are questions about the target for the annotations. For at least some phenomena, the assumption that there is a single ground-truth to approximate with annotations doesn’t hold.\nAroyo & Welty (2015)\n7 ‘myths’ of human annotation:\n\nthere is one truth\ndisagreement is bad\ndetailed guidelines help\nexperts are better\none annotator is enough\nall items are created equal\nonce done, forever valid\n\nFor myths 1 and 2:\n\nlist examples from NLP where the disagreement from annotators is sensible\nthey argue that the assumptions of a single ground truth, and that disagreement is indicative of poor annotations are both false.\n\nfor myth 6: disagreement indicates that the media being rated is ambiguous.\n\n\nJacobs & Wallach (2021) there is a ‘measurement error model’ (taken from econ) that links the unobservable latent variable, and observable properties. in annotations this is via individual observations\nalthough paper focuses on attempts at measuring constructs (risk of recidivism, teacher effectiveness, patient benefit) they also show that even ‘representational measurements’ like height, are essentially a latent variable\nAroyo & Welty (2015) operationalize ‘crowd truth’ with an illustration where the ‘gold standard’ is the probability that a sentence contains an element, based on the probability that an annotator annotated that sentence with that element.\n\ni.e. the label isn’t represented as ‘present’ or ‘not present’, but as a probablility\nthus the ‘crowd truth’ attempts to capture the ‘range of reasonable interpretations’\n\nBeck et al. (2022): we should expect more variance to the degree that tasks measure opinion show work on an intuitively perspective-based use-case: hate speech\n\n\n\n\nGriffin & Brenner (2004) review errors and biases in human judgements1\n\nover/under prediction: confidence score is higher/lower than accuracy\nover/under extremity: confidence is more extreme at ends\n\nalso reviews possible reasons:\n\noptimistic overconfidence\nconfirmation bias\ncase-based judgment\necological probability\nerror model (psychometric model)\n\n\n\n\nGeiger et al. (2021) ML science studies inadequately report ‘ground truth’\nHullman et al. (2022) thus we cannot know what data generating process the resulting model represents\n[perhaps cat image parable here?]\n\n\n\nHullman et al. (2022)\nWith regards to reference data:\n\nrepresentation bias / non-representative samples\nmeasurement bias / unvalidated measurement instruments\nunderspecification of portions of input space in training data\ntransformation of data to optimize for ‘accuracy’\nlack of or poor dataset documentation\n\nIn other words, optimizing for predictive accuracy using very large datasets does not ‘absolve’ researchers from having to consider the data generating process. They note benefits that both machine learning and psychology could gain by borrowing methods from each other, but note the danger if these are misused. For the benefit of machine learning, there are lessons to be learned from social science, and the replication crisis. Among them are 1) collecting samples whose test/evaluation set distributions are drawn from the same deployment distribution, and 2) using valid measurement instruments.\n\n\n\nBeck et al. (2022)\n\nannotation collection requires design thinking\n\nTask Structure: specific wording and response options, including debates over the inclusion of “I don’t Know” option\nOrder Effects: specific judgements are affected by previous perceptions\nAnnotator Effects: backgrounds, opinions, experiences of respondents affect responses\n\n\nJacobs & Wallach (2021)\n\nreliability: do similar inputs to a measurement model present similar outputs?\n\ntest-retest: are measurements of an unobservable latent construct taken at different times via a measurement model similar, assuming the construct hasn’t changed?\n\nvalidity: is it ‘right’?\n\nno single test for validity on purpose, because it requires thinking. do our measurements:\n\nface validity: look plausible/ sensible?\ncontent validity: capture the construct?\n\nstructural validity: show the inter-correlations we expect?\nsubstantive validity: capture only observable properties thought to be related to the construct?\n\nconvergent validity: show correlations with other validated methods?\ndiscriminant validity: show correlations with other construct/properties thought not to be related to the construct?\npredictive validity: show correlations with constructs/properties thought to be related, but not in the operationalization?\nhypothesis validity: shed light on relevant hypotheses about the construct being measured?\nconsequential validity: allow for the consequences obtained from the measurement model to be assessed?\n\n\n\n\nCabitza et al. (2023): whether the target of the annotation is a subjective phenomenon or not, disagreement is always irreducible. Yet ML typically assumes there is a single ‘ground truth’, and its best indicator is inter-annotator agreement. But taking the perspectives of the annotators into account, both in the data annotation but also the modelling phase of ML projects has recently been shown to benefit ML modelling in a number of contexts.\nweak perspectivist approach: taking perspectives into account while designing and collecting annotations, but ultimately reducing annotations to a single label or rating.\nstrong perspectivist approach: taking perspectives into account for ground truthing and modelling phases.\nbenefits of this approach:\n\nis congruent with the reality of collecting annotations\nincludes the signal in the variance of labels or ratings\navoids majority group perspective appearing to be ‘objective’\nallows for the modelling of human errors and variances\nallows for uncertain, fuzzy, or soft model development\nmore complete report of the data generating process, as it also reports uncertainty\n\ndownsides:\n\nmultiple raters, and therefore costs/time/rater availability are issues\nneed for perspectivist ML approaches\nvalidation becomes more challenging\n\nrecommendations:\n\ncomplete labeling schemes, including ‘i don’t know’, ‘none of these’ etc. categories, and the ability to express issues with label set\nenough raters\nheterogenous raters\nadequate reporting:\n\nnumber of raters,\nrater expertise\nincentive\ninstructions\nlength of time for labelling\ninter rater agreement\naggregation method\nconfidence"
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Advancing Perspectivist Ground Truthing with Social Science",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nGriffin & Brenner (2004) note that much of this work was about people guessing knowledge from an almanac, and then guessing how accurate they were↩︎"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html#ml-uses-human-annotations-very-often",
    "href": "index.html#ml-uses-human-annotations-very-often",
    "title": "Advancing Perspectivist Ground Truthing with Social Science",
    "section": "",
    "text": "Geiger et al. (2021)\n\n200 randomly sampled ML papers from 3 domains:\n\nSocial Sciences & Humanities\nLife & Biomedical Sciences\nPhysical & Environmental Sciences\n\nOut of 141 classification tasks, 103 (73.05%) used human labels\nOut of 103 human labels, 58 (56.31%) used only external labels\n\ni.e. ML re-uses external labels"
  },
  {
    "objectID": "index.html#issues-using-human-annotations-in-ml",
    "href": "index.html#issues-using-human-annotations-in-ml",
    "title": "Advancing Perspectivist Ground Truthing with Social Science",
    "section": "",
    "text": "A number of works have shown issues with annotations in ML\nHullman et al. (2022) compare claims that ML is facing a reproducibility crisis to the crisis in psychology. Among the issues they note relate to benchmark datasets, which researchers often re-use as they publish on standardized benchmarks, and because they are cost prohibitive to collect.\nOne reason may be: Sambasivan et al. (2021) ML researchers broadly prefer to work on building and evaluating performance, rather than executing ground-truthing projects.\nAnother may be that the skills are not being taught: [geiger, first paper, show lack of reporting in ML textbooks on ground truthing]\nA number of papers have drawn from the social sciences to synthesize knowledge on how best to gather annotations.\n\n\ndisagreement is common\nreviewed in Cabitza et al. (2023): -social media content: Chandrasekharan 2017 -medical cases: Cabitza 2019 -various NLP tasks Aroyo & Welty (2015)\ndisagreement is often removed\n\nadjusting annotator training and instruction\nadjusting annotations via discussion post-collection\nmajority voting, post-hoc without annotators\n\nBeyond errors in judgment are questions about the target for the annotations. For at least some phenomena, the assumption that there is a single ground-truth to approximate with annotations doesn’t hold.\nAroyo & Welty (2015)\n7 ‘myths’ of human annotation:\n\nthere is one truth\ndisagreement is bad\ndetailed guidelines help\nexperts are better\none annotator is enough\nall items are created equal\nonce done, forever valid\n\nFor myths 1 and 2:\n\nlist examples from NLP where the disagreement from annotators is sensible\nthey argue that the assumptions of a single ground truth, and that disagreement is indicative of poor annotations are both false.\n\nfor myth 6: disagreement indicates that the media being rated is ambiguous.\n\n\nJacobs & Wallach (2021) there is a ‘measurement error model’ (taken from econ) that links the unobservable latent variable, and observable properties. in annotations this is via individual observations\nalthough paper focuses on attempts at measuring constructs (risk of recidivism, teacher effectiveness, patient benefit) they also show that even ‘representational measurements’ like height, are essentially a latent variable\nAroyo & Welty (2015) operationalize ‘crowd truth’ with an illustration where the ‘gold standard’ is the probability that a sentence contains an element, based on the probability that an annotator annotated that sentence with that element.\n\ni.e. the label isn’t represented as ‘present’ or ‘not present’, but as a probablility\nthus the ‘crowd truth’ attempts to capture the ‘range of reasonable interpretations’\n\nBeck et al. (2022): we should expect more variance to the degree that tasks measure opinion show work on an intuitively perspective-based use-case: hate speech\n\n\n\n\nGriffin & Brenner (2004) review errors and biases in human judgements1\n\nover/under prediction: confidence score is higher/lower than accuracy\nover/under extremity: confidence is more extreme at ends\n\nalso reviews possible reasons:\n\noptimistic overconfidence\nconfirmation bias\ncase-based judgment\necological probability\nerror model (psychometric model)\n\n\n\n\nGeiger et al. (2021) ML science studies inadequately report ‘ground truth’\nHullman et al. (2022) thus we cannot know what data generating process the resulting model represents\n[perhaps cat image parable here?]\n\n\n\nHullman et al. (2022)\nWith regards to reference data:\n\nrepresentation bias / non-representative samples\nmeasurement bias / unvalidated measurement instruments\nunderspecification of portions of input space in training data\ntransformation of data to optimize for ‘accuracy’\nlack of or poor dataset documentation\n\nIn other words, optimizing for predictive accuracy using very large datasets does not ‘absolve’ researchers from having to consider the data generating process. They note benefits that both machine learning and psychology could gain by borrowing methods from each other, but note the danger if these are misused. For the benefit of machine learning, there are lessons to be learned from social science, and the replication crisis. Among them are 1) collecting samples whose test/evaluation set distributions are drawn from the same deployment distribution, and 2) using valid measurement instruments.\n\n\n\nBeck et al. (2022)\n\nannotation collection requires design thinking\n\nTask Structure: specific wording and response options, including debates over the inclusion of “I don’t Know” option\nOrder Effects: specific judgements are affected by previous perceptions\nAnnotator Effects: backgrounds, opinions, experiences of respondents affect responses\n\n\nJacobs & Wallach (2021)\n\nreliability: do similar inputs to a measurement model present similar outputs?\n\ntest-retest: are measurements of an unobservable latent construct taken at different times via a measurement model similar, assuming the construct hasn’t changed?\n\nvalidity: is it ‘right’?\n\nno single test for validity on purpose, because it requires thinking. do our measurements:\n\nface validity: look plausible/ sensible?\ncontent validity: capture the construct?\n\nstructural validity: show the inter-correlations we expect?\nsubstantive validity: capture only observable properties thought to be related to the construct?\n\nconvergent validity: show correlations with other validated methods?\ndiscriminant validity: show correlations with other construct/properties thought not to be related to the construct?\npredictive validity: show correlations with constructs/properties thought to be related, but not in the operationalization?\nhypothesis validity: shed light on relevant hypotheses about the construct being measured?\nconsequential validity: allow for the consequences obtained from the measurement model to be assessed?\n\n\n\n\nCabitza et al. (2023): whether the target of the annotation is a subjective phenomenon or not, disagreement is always irreducible. Yet ML typically assumes there is a single ‘ground truth’, and its best indicator is inter-annotator agreement. But taking the perspectives of the annotators into account, both in the data annotation but also the modelling phase of ML projects has recently been shown to benefit ML modelling in a number of contexts.\nweak perspectivist approach: taking perspectives into account while designing and collecting annotations, but ultimately reducing annotations to a single label or rating.\nstrong perspectivist approach: taking perspectives into account for ground truthing and modelling phases.\nbenefits of this approach:\n\nis congruent with the reality of collecting annotations\nincludes the signal in the variance of labels or ratings\navoids majority group perspective appearing to be ‘objective’\nallows for the modelling of human errors and variances\nallows for uncertain, fuzzy, or soft model development\nmore complete report of the data generating process, as it also reports uncertainty\n\ndownsides:\n\nmultiple raters, and therefore costs/time/rater availability are issues\nneed for perspectivist ML approaches\nvalidation becomes more challenging\n\nrecommendations:\n\ncomplete labeling schemes, including ‘i don’t know’, ‘none of these’ etc. categories, and the ability to express issues with label set\nenough raters\nheterogenous raters\nadequate reporting:\n\nnumber of raters,\nrater expertise\nincentive\ninstructions\nlength of time for labelling\ninter rater agreement\naggregation method\nconfidence"
  }
]