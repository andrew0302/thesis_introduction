[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Advancing Perspectivist Ground Truthing with Social Science",
    "section": "",
    "text": "AI systems run on data. Data are used to ‘train’ systems, whereby algorithms - step by step instructions, executed in order - are used to estimate parameters from the data that form the models that lie at the center of these automated systems. Because these systems are too complex to directly interpret their model parameters, the systems are evaluated empirically by comparing their outputs to a reference. This reference is often a second form of data, referred to as the ‘ground truth’, ‘gold standard’, or simply ‘annotations’, which represents the ideal output of the system.\nWhether used as part of the development of the system, or to evaluate it, the quality of these datasets determines the quality of the system. As parameters are estimated from the training data, imperfections, inaccuracies, biases etc. are reflected in the parameters of the resulting model. As models are evaluated by comparing their outputs to reference data, such that the ‘best’ models are those whose outputs most closely resemble the reference data, imperfections in the reference are reflected in the models as well. Thus, the quality of the data used in training and reference represent an upper boundary of real-world ‘accuracy’ of automated systems, where the best possible performance directly corresponds to the degree to which the reference resembles the real-world.\nTraining data takes various forms but is often a form of media - text, audio, images, or video - whereas reference data very often contains aggregated input from humans. Common scenarios include human judges annotating, labeling, or rating a) individual pieces of content of the same form as the training data, or b) generated system outputs. Multiple ratings per piece of content are collected and aggregated, forming a singular ‘ground truth’ for the aspect of the content being labelled or rated. Thus, it is assumed that there is a singular canonical truth for each aspect / content pair, comprised of aggregated human responses, which forms a target to which we align our automated systems.\nAlthough far more emphasis is placed on whether models achieve state of the art ‘performance’ or efficiency (Birhane et al., 2022; Sambasivan et al., 2021), scholars over the past decade have attempted to draw attention to a lack of sophistication in how training and reference data are collected and evaluated. It has further been argued that a focus on improving the data, will result in bigger gains than a focus on improving the model1, and that dataset size can not compensate for the shortcomings in dataset quality (Hullman et al., 2022). Issues include 1) representational biases in the content sampled for inclusion in training/evaluation datasets (Hullman et al., 2022), 2) measurement biases in the annotations collected (Beck et al., 2022; Hullman et al., 2022; Jacobs & Wallach, 2021), 3) a fallacious assumption of a single canonical ‘ground-truth’ (Aroyo & Welty, 2015; Cabitza et al., 2023), and 4) poor reporting of necessary information regarding the annotation-collection process (Geiger et al., 2021; Hullman et al., 2022).\nAn additional consideration that receives little attention is the number of annotations to gather, where fields that focus on gathering data from humans also have a strong emphasis on a-priori calculations of target sample sizes. These considerations are absent in computational fields which appear to favor differing rules of thumb: e.g. in a well-cited textbook, Pustejovsky & Stubbs (2013) suggest to “have your corpus annotated by at least two people (more is preferable, but not always practical)”, whereas Artstein & Poesio (2008) suggest that “measuring reliability with only two coders is seldom enough, except for small-scale studies”. On the one hand, corpora tend to be very large, and resources are finite. However, rules of thumb lack clear substantiation, in light of the both the phenomenon being grounded and the nature of the media in which it is grounded. In other words, more variance is expected in annotation targets to the degree they are subjective e.g. whether or not a piece of text represents hate speech (Beck et al., 2022), and more variance is expected in content to the degree to which it is ambiguous (Sandri et al., 2023).\nScholars further note a gap between practices in Machine Learning fields, and knowledge from the social sciences which could address these issues, but has yet to be broadly applied. One reason for this gap may be ML researchers prefer to work on building systems and evaluating their performance rather than executing ground-truthing projects (Sambasivan et al., 2021). Another may be a lack of focus on these topics in textbooks, and thus in education more broadly (first geiger paper I think). A third may be that the social and computational sciences have conceptually different focci: the computational sciences focus on the statistical model the system with substantially less emphasis on the content, whereas the social sciences treat the statistical model as a means to better understanding the relationships in the content (Liem et al., 2018). A further more practical complication is that work on these topics and potential solutions lacks a central academic ‘home’: where psychology and economics have psychometrics and econometrics respectively, and where software engineering has software testing, the study of ground-truthing lacks a central banner under which academic work can accumulate.\n\n\nThis thesis incorporates the aforementioned shortcomings into a singular framework, guided by best practices in the social sciences. Thus our primary contribution is a worked example of a ground-truthing project which involves the collection of human annotations, and that attempts the application of practices from the social science for each of the issues above.\nSpecifically:\n\nWe attempt to mitigate representation biases in the content we select for annotation by using a stratified sampling strategy.\nWe attempt to mitigate measurement biases by treating the target measurement as a latent variable, and the survey we used to gather annotations as an instrument. We build on work that validated a questionnaire for measuring constructs, and estimating its reliability and structural validity when used for annotations.\nWe account for the potential of multiple perspectives in our dataset by recruiting participants from relevant subgroups in a single target population.\nWe report the details of the annotation collection process, and share the disaggregated dataset of the annotations\nWe further show how to estimate the number of annotators\n\n\nWe demonstrate the potential of this framework by grounding a complex phenomenon (a 10-dimensional construct, Personal Values) in ambiguous text (song lyrics). We further show an\nWhen sampling content to include in training/test datasets, samples for the training/test sets will ideally be drawn from the same distribution as the content in which they will eventually be deployed. Representation bias in content selected for training and/or evaluation datasets refers to the degree to which relevant distributions in data used to train and/or evaluate systems resembles the distribution in the environment to which it will be deployed (Hullman et al., 2022).\nMeasurement bias in the annotations collected from humans may also bias\nPerspectivism\nReporting\nWe add: a priori rating number estimation\nAnd although imperfect as leaderboard scores can be gamed, and do not perfectly represent the deployment environment, the typical leaderboard approach has shown evidence that progress can be made towards a target. This thesis thus represents an attempt to define the target better."
  },
  {
    "objectID": "index.html#issues-using-human-annotations-in-ml",
    "href": "index.html#issues-using-human-annotations-in-ml",
    "title": "Advancing Perspectivist Ground Truthing with Social Science",
    "section": "Issues using human annotations in ML",
    "text": "Issues using human annotations in ML\nA number of works have shown issues with annotations in ML\nHullman et al. (2022) compare claims that ML is facing a reproducibility crisis to the crisis in psychology. Among the issues they note relate to benchmark datasets, which researchers often re-use as they publish on standardized benchmarks, and because they are cost prohibitive to collect.\nAnother may be that the skills are not being taught: [geiger, first paper, show lack of reporting in ML textbooks on ground truthing]\nA number of papers have drawn from the social sciences to synthesize knowledge on how best to gather annotations.\n\nML treats all annotation variance as noise rather than signal\ndisagreement is common\nreviewed in Cabitza et al. (2023): -social media content: Chandrasekharan 2017 -medical cases: Cabitza 2019 -various NLP tasks Aroyo & Welty (2015)\ndisagreement is often removed\n\nadjusting annotator training and instruction\nadjusting annotations via discussion post-collection\nmajority voting, post-hoc without annotators\n\nBeyond errors in judgment are questions about the target for the annotations. For at least some phenomena, the assumption that there is a single ground-truth to approximate with annotations doesn’t hold.\nAroyo & Welty (2015)\n7 ‘myths’ of human annotation:\n\nthere is one truth\ndisagreement is bad\ndetailed guidelines help\nexperts are better\none annotator is enough\nall items are created equal\nonce done, forever valid\n\nFor myths 1 and 2:\n\nlist examples from NLP where the disagreement from annotators is sensible\nthey argue that the assumptions of a single ground truth, and that disagreement is indicative of poor annotations are both false.\n\nfor myth 6: disagreement indicates that the media being rated is ambiguous.\n\nannotations aim to measure a latent variable\nJacobs & Wallach (2021) there is a ‘measurement error model’ (taken from econ) that links the unobservable latent variable, and observable properties. in annotations this is via individual observations\nalthough paper focuses on attempts at measuring constructs (risk of recidivism, teacher effectiveness, patient benefit) they also show that even ‘representational measurements’ like height, are essentially a latent variable\nAroyo & Welty (2015) operationalize ‘crowd truth’ with an illustration where the ‘gold standard’ is the probability that a sentence contains an element, based on the probability that an annotator annotated that sentence with that element.\n\ni.e. the label isn’t represented as ‘present’ or ‘not present’, but as a probablility\nthus the ‘crowd truth’ attempts to capture the ‘range of reasonable interpretations’\n\nBeck et al. (2022): we should expect more variance to the degree that tasks measure opinion show work on an intuitively perspective-based use-case: hate speech\n\n\n\nHuman annotations aren’t always accurate\nGriffin & Brenner (2004) review errors and biases in human judgements2\n\nover/under prediction: confidence score is higher/lower than accuracy\nover/under extremity: confidence is more extreme at ends\n\nalso reviews possible reasons:\n\noptimistic overconfidence\nconfirmation bias\ncase-based judgment\necological probability\nerror model (psychometric model)\n\n\n\nInadequate reporting\nGeiger et al. (2021) ML science studies inadequately report ‘ground truth’\nHullman et al. (2022) thus we cannot know what data generating process the resulting model represents\n[perhaps cat image parable here?]\n\n\nsampling and measurement biases\nHullman et al. (2022)\nWith regards to reference data:\n\nrepresentation bias / non-representative samples\nmeasurement bias / unvalidated measurement instruments\nunderspecification of portions of input space in training data\ntransformation of data to optimize for ‘accuracy’\nlack of or poor dataset documentation\n\nIn other words, optimizing for predictive accuracy using very large datasets does not ‘absolve’ researchers from having to consider the data generating process. They note benefits that both machine learning and psychology could gain by borrowing methods from each other, but note the danger if these are misused. For the benefit of machine learning, there are lessons to be learned from social science, and the replication crisis. Among them are 1) collecting samples whose test/evaluation set distributions are drawn from the same deployment distribution, and 2) using valid measurement instruments.\n\n\nML doesn’t treat annotation generating process as an instrument\nBeck et al. (2022)\n\nannotation collection requires design thinking\n\nTask Structure: specific wording and response options, including debates over the inclusion of “I don’t Know” option\nOrder Effects: specific judgements are affected by previous perceptions\nAnnotator Effects: backgrounds, opinions, experiences of respondents affect responses\n\n\nJacobs & Wallach (2021)\n\nreliability: do similar inputs to a measurement model present similar outputs?\n\ntest-retest: are measurements of an unobservable latent construct taken at different times via a measurement model similar, assuming the construct hasn’t changed?\n\nvalidity: is it ‘right’?\n\nno single test for validity on purpose, because it requires thinking. do our measurements:\n\nface validity: look plausible/ sensible?\ncontent validity: capture the construct?\n\nstructural validity: show the inter-correlations we expect?\nsubstantive validity: capture only observable properties thought to be related to the construct?\n\nconvergent validity: show correlations with other validated methods?\ndiscriminant validity: show correlations with other construct/properties thought not to be related to the construct?\npredictive validity: show correlations with constructs/properties thought to be related, but not in the operationalization?\nhypothesis validity: shed light on relevant hypotheses about the construct being measured?\nconsequential validity: allow for the consequences obtained from the measurement model to be assessed?\n\n\n\nML ignores perspectives of annotators\nCabitza et al. (2023): whether the target of the annotation is a subjective phenomenon or not, disagreement is always irreducible. Yet ML typically assumes there is a single ‘ground truth’, and its best indicator is inter-annotator agreement. But taking the perspectives of the annotators into account, both in the data annotation but also the modelling phase of ML projects has recently been shown to benefit ML modelling in a number of contexts.\nweak perspectivist approach: taking perspectives into account while designing and collecting annotations, but ultimately reducing annotations to a single label or rating.\nstrong perspectivist approach: taking perspectives into account for ground truthing and modelling phases.\nbenefits of this approach:\n\nis congruent with the reality of collecting annotations\nincludes the signal in the variance of labels or ratings\navoids majority group perspective appearing to be ‘objective’\nallows for the modelling of human errors and variances\nallows for uncertain, fuzzy, or soft model development\nmore complete report of the data generating process, as it also reports uncertainty\n\ndownsides:\n\nmultiple raters, and therefore costs/time/rater availability are issues\nneed for perspectivist ML approaches\nvalidation becomes more challenging\n\nrecommendations:\n\ncomplete labeling schemes, including ‘i don’t know’, ‘none of these’ etc. categories, and the ability to express issues with label set\nenough raters\nheterogenous raters\nadequate reporting:\n\nnumber of raters,\nrater expertise\nincentive\ninstructions\nlength of time for labelling\ninter rater agreement\naggregation method\nconfidence"
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Advancing Perspectivist Ground Truthing with Social Science",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nGriffin & Brenner (2004) note that much of this work was about people guessing knowledge from an almanac, and then guessing how accurate they were↩︎\nGriffin & Brenner (2004) note that much of this work was about people guessing knowledge from an almanac, and then guessing how accurate they were↩︎"
  },
  {
    "objectID": "index.html#present-work",
    "href": "index.html#present-work",
    "title": "Advancing Perspectivist Ground Truthing with Social Science",
    "section": "",
    "text": "This thesis incorporates the aforementioned shortcomings into a singular framework, guided by best practices in the social sciences. Thus our primary contribution is a worked example of a ground-truthing project which involves the collection of human annotations, and that attempts the application of practices from the social science for each of the issues above.\nSpecifically:\n\nWe attempt to mitigate representation biases in the content we select for annotation by using a stratified sampling strategy.\nWe attempt to mitigate measurement biases by treating the target measurement as a latent variable, and the survey we used to gather annotations as an instrument. We build on work that validated a questionnaire for measuring constructs, and estimating its reliability and structural validity when used for annotations.\nWe account for the potential of multiple perspectives in our dataset by recruiting participants from relevant subgroups in a single target population.\nWe report the details of the annotation collection process, and share the disaggregated dataset of the annotations\nWe further show how to estimate the number of annotators\n\n\nWe demonstrate the potential of this framework by grounding a complex phenomenon (a 10-dimensional construct, Personal Values) in ambiguous text (song lyrics). We further show an\nWhen sampling content to include in training/test datasets, samples for the training/test sets will ideally be drawn from the same distribution as the content in which they will eventually be deployed. Representation bias in content selected for training and/or evaluation datasets refers to the degree to which relevant distributions in data used to train and/or evaluate systems resembles the distribution in the environment to which it will be deployed (Hullman et al., 2022).\nMeasurement bias in the annotations collected from humans may also bias\nPerspectivism\nReporting\nWe add: a priori rating number estimation\nAnd although imperfect as leaderboard scores can be gamed, and do not perfectly represent the deployment environment, the typical leaderboard approach has shown evidence that progress can be made towards a target. This thesis thus represents an attempt to define the target better."
  }
]