[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Advancing Perspectivist Ground Truthing with Social Science",
    "section": "",
    "text": "Hullman et al. (2022) compare claims that ML is facing a reproducibility crisis to the crisis in psychology. Among the issues they note relate to benchmark datasets, which researchers often re-use as they publish on standardized benchmarks, and because they are cost prohibitive to collect. It may also be the case that ML researchers broadly prefer to work on building and evaluating performance, rather than executing ground-truthing projects Sambasivan et al. (2021).\nWith regards to reference data:\n\nrepresentation bias / non-representative samples\nmeasurement bias / unvalidated measurement instruments\nunderspecification of portions of input space in training data\ntransformation of data to optimize for ‘accuracy’\nlack of or poor dataset documentation\n\nIn other words, optimizing for predictive accuracy using very large datasets does not ‘absolve’ researchers from having to consider the data generating process. They note benefits that both machine learning and psychology could gain by borrowing methods from each other, but note the danger if these are misused. For the benefit of machine learning, there are lessons to be learned from social science, and the replication crisis. Among them are 1) collecting samples whose test/evaluation set distributions are drawn from the same deployment distribution, and 2) using valid measurement instruments.\n\n\nsocial sciences:\n\ninterpretable meaning of x and y\ndesign is informed by theory\n\ncomputational sciences:\n\nlearning procedure f(x)\n\n Fig. 1: Liem et al. (2018)\nRecent trends—especially in deep learning—prioritize empirical performance over theoretical assumptions about the data generating process. A systematic analysis of highly cited ML works shows that Performance, Generalization, Quantitative evidence, Efficiency, Building on past work, and Novelty Birhane et al. (2022). Unlike the social sciences (e.g. psychology), ML work ignores attempts to model the process that gives rise to the data, assuming it cannot be learned, and aims instead at predictors that will within estimable error bounds Hullman et al. (2022).\n\n\nPonce-López et al. (2016)\n\nefficient way to gather media and annotation data\nBUT no validation of instrument, or ecologically valid media data\ndistribution of training /eval data don’t come from the target distribution\n\n Figure 2: Ponce-López et al. (2016)\nCompared to Koutsoumpis et al. (2024):\n\necological validity: media data was mock asynchronous video interviews\necological validity: interview questions designed to activate personality facets\npersonality instruments: validated HEXACO scale\nperspectives: self & observer ratings\n\n\n\n\n\nGeiger et al. (2021)\n\n200 randomly sampled ML papers from 3 domains:\n\nSocial Sciences & Humanities\nLife & Biomedical Sciences\nPhysical & Environmental Sciences\n\nOut of 141 classification tasks, 103 (73.05%) used human labels\nOut of 103 human labels, 58 (56.31%) used only external labels\n\ni.e. ML re-uses external labels, and inadequately reports ‘ground truth’\nwithout details of ground truth, we cannot know what data generating process the resulting model represents Hullman et al. (2022). [perhaps cat image parable here?]\n\n\n\nGriffin & Brenner (2004) review errors and biases in human judgements1\n\nover/under prediction: confidence score is higher/lower than accuracy\nover/under extremity: confidence is more extreme at ends\n\nalso reviews possible reasons:\n\noptimistic overconfidence\nconfirmation bias\ncase-based judgment\necological probability\nerror model (psychometric model)\n\n\n\n\n[crowd truth; perspectivist approach here]\n\n\nSolutions to sampling problems can come from sampling theory: Groves et al. (2009)\nconsiderations:\n\nsampling frame: the elements in from populations that you have access to\nineligible units: elements in the sampling frame that are not your target\nundercoverage: elements from target population that are not in the frame\n\nsolutions:\n\nstratified sampling\n\n\n\n\nBeck et al. (2022)\n\nannotation collection requires design thinking\n\nTask Structure: specific wording and response options, including debates over the inclusion of “I don’t Know” option\nOrder Effects: specific judgements are affected by previous perceptions\nAnnotator Effects: backgrounds, opinions, experiences of respondents affect responses\n\n\n[another ref that describes the target as a latent variable]"
  },
  {
    "objectID": "index.html#poor-reproducibilityreplicability-in-ml-research",
    "href": "index.html#poor-reproducibilityreplicability-in-ml-research",
    "title": "Advancing Perspectivist Ground Truthing with Social Science",
    "section": "",
    "text": "Hullman et al. (2022) compare claims that ML is facing a reproducibility crisis to the crisis in psychology. Among the issues they note relate to benchmark datasets, which researchers often re-use as they publish on standardized benchmarks, and because they are cost prohibitive to collect. It may also be the case that ML researchers broadly prefer to work on building and evaluating performance, rather than executing ground-truthing projects Sambasivan et al. (2021).\nWith regards to reference data:\n\nrepresentation bias / non-representative samples\nmeasurement bias / unvalidated measurement instruments\nunderspecification of portions of input space in training data\ntransformation of data to optimize for ‘accuracy’\nlack of or poor dataset documentation\n\nIn other words, optimizing for predictive accuracy using very large datasets does not ‘absolve’ researchers from having to consider the data generating process. They note benefits that both machine learning and psychology could gain by borrowing methods from each other, but note the danger if these are misused. For the benefit of machine learning, there are lessons to be learned from social science, and the replication crisis. Among them are 1) collecting samples whose test/evaluation set distributions are drawn from the same deployment distribution, and 2) using valid measurement instruments.\n\n\nsocial sciences:\n\ninterpretable meaning of x and y\ndesign is informed by theory\n\ncomputational sciences:\n\nlearning procedure f(x)\n\n Fig. 1: Liem et al. (2018)\nRecent trends—especially in deep learning—prioritize empirical performance over theoretical assumptions about the data generating process. A systematic analysis of highly cited ML works shows that Performance, Generalization, Quantitative evidence, Efficiency, Building on past work, and Novelty Birhane et al. (2022). Unlike the social sciences (e.g. psychology), ML work ignores attempts to model the process that gives rise to the data, assuming it cannot be learned, and aims instead at predictors that will within estimable error bounds Hullman et al. (2022).\n\n\nPonce-López et al. (2016)\n\nefficient way to gather media and annotation data\nBUT no validation of instrument, or ecologically valid media data\ndistribution of training /eval data don’t come from the target distribution\n\n Figure 2: Ponce-López et al. (2016)\nCompared to Koutsoumpis et al. (2024):\n\necological validity: media data was mock asynchronous video interviews\necological validity: interview questions designed to activate personality facets\npersonality instruments: validated HEXACO scale\nperspectives: self & observer ratings\n\n\n\n\n\nGeiger et al. (2021)\n\n200 randomly sampled ML papers from 3 domains:\n\nSocial Sciences & Humanities\nLife & Biomedical Sciences\nPhysical & Environmental Sciences\n\nOut of 141 classification tasks, 103 (73.05%) used human labels\nOut of 103 human labels, 58 (56.31%) used only external labels\n\ni.e. ML re-uses external labels, and inadequately reports ‘ground truth’\nwithout details of ground truth, we cannot know what data generating process the resulting model represents Hullman et al. (2022). [perhaps cat image parable here?]\n\n\n\nGriffin & Brenner (2004) review errors and biases in human judgements1\n\nover/under prediction: confidence score is higher/lower than accuracy\nover/under extremity: confidence is more extreme at ends\n\nalso reviews possible reasons:\n\noptimistic overconfidence\nconfirmation bias\ncase-based judgment\necological probability\nerror model (psychometric model)\n\n\n\n\n[crowd truth; perspectivist approach here]\n\n\nSolutions to sampling problems can come from sampling theory: Groves et al. (2009)\nconsiderations:\n\nsampling frame: the elements in from populations that you have access to\nineligible units: elements in the sampling frame that are not your target\nundercoverage: elements from target population that are not in the frame\n\nsolutions:\n\nstratified sampling\n\n\n\n\nBeck et al. (2022)\n\nannotation collection requires design thinking\n\nTask Structure: specific wording and response options, including debates over the inclusion of “I don’t Know” option\nOrder Effects: specific judgements are affected by previous perceptions\nAnnotator Effects: backgrounds, opinions, experiences of respondents affect responses\n\n\n[another ref that describes the target as a latent variable]"
  },
  {
    "objectID": "index.html#present-work",
    "href": "index.html#present-work",
    "title": "Advancing Perspectivist Ground Truthing with Social Science",
    "section": "Present Work",
    "text": "Present Work\nWe incorporate these considerations in the design of our study, and attempt to further the field in the following ways:\n\nwe attempt representative sampling of both media and respondents\nwe aim to estimate 10-dimensional psychological construct\nwe select media that is ambiguous (i.e. that will result in subjectivity in the ratings) as well as media that we expect not to be ambiguous for comparison\nwe estimate a-priori the number of ratings necessary rather than assuming\nwe take into account perspectives\n\ncase study of this thesis works towards path (b) in Liem et al. (2018) shown in:"
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Advancing Perspectivist Ground Truthing with Social Science",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nGriffin & Brenner (2004) note that much of this work was about people guessing knowledge from an almanac, and then guessing how accurate they were↩︎"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  }
]