---
title: "Advancing Perspectivist Ground Truthing with Social Science "
bibliography: references.bib
---

## Poor reproducibility/replicability in ML research

@hullman2022worst compare claims that ML is facing a reproducibility crisis to the crisis in psychology. Among the issues they note relate to benchmark datasets, which researchers often re-use as they publish on standardized benchmarks, and because they are cost prohibitive to collect. It may also be the case that ML researchers broadly prefer to work on building and evaluating performance, rather than executing ground-truthing projects @sambasivan2021everyone.

With regards to reference data:

* representation bias / non-representative samples
* measurement bias / unvalidated measurement instruments
* underspecification of portions of input space in training data
* transformation of data to optimize for 'accuracy'
* lack of or poor dataset documentation

In other words, optimizing for predictive accuracy using very large datasets does not 'absolve' researchers from having to consider the data generating process. They note benefits that both machine learning and psychology could gain by borrowing methods from each other, but note the danger if these are misused. For the benefit of machine learning, there are lessons to be learned from social science, and the replication crisis. Among them are 1) collecting samples whose test/evaluation set distributions are drawn from the same deployment distribution, and 2) using valid measurement instruments. 

### Social and computational sciences have different focci

social sciences:

* interpretable meaning of x and y
* design is informed by theory

computational sciences:

* learning procedure f(x)

![](images/Liem_2018_figure_1.png)
Fig. 1: @liem2018psychology

Recent trends—especially in deep learning—prioritize empirical performance over theoretical assumptions about the data generating process. A systematic analysis of highly cited ML works shows that Performance, Generalization, Quantitative evidence, Efficiency, Building on past work, and Novelty @birhane2022values. Unlike the social sciences (e.g. psychology), ML work ignores attempts to model the process that gives rise to the data, assuming it cannot be learned, and aims instead at predictors that will within estimable error bounds @hullman2022worst. 

#### A tale of two studies: the case of personnel selection

@ponce2016chalearn 

* efficient way to gather media and annotation data
* BUT no validation of instrument, or ecologically valid media data
* distribution of training /eval data don't come from the target distribution

![](images/Ponce_Lopez_2016_figure_2.png)
Figure 2: @ponce2016chalearn 

Compared to @koutsoumpis2024beyond:

* ecological validity: media data was mock asynchronous video interviews
* ecological validity: interview questions designed to activate personality facets
* personality instruments: validated HEXACO scale
* perspectives: self & observer ratings

### ML uses human annotations very often

@geiger2021garbage 

* 200 randomly sampled ML papers from 3 domains:
  * Social Sciences & Humanities
  * Life & Biomedical Sciences
  * Physical & Environmental Sciences

* Out of 141 classification tasks, 103 (73.05%) used human labels
* Out of 103 human labels, 58 (56.31%) used only external labels

i.e. ML re-uses external labels, and inadequately reports 'ground truth'

without details of ground truth, we cannot know what data generating process the resulting model represents @hullman2022worst. 
[perhaps cat image parable here?]


### Human annotations aren't always accurate

@griffin2004perspectives review errors and biases in human judgements[^2]

[^2]: @griffin2004perspectives note that much of this work was about people guessing knowledge from an almanac, and then guessing how accurate they were

* over/under prediction: confidence score is higher/lower than accuracy
* over/under extremity: confidence is more extreme at ends

also reviews possible reasons:

* optimistic overconfidence
* confirmation bias
* case-based judgment
* ecological probability
* error model (psychometric model)

### ML often ignores perspectives of annotators

[crowd truth; perspectivist approach here]


#### Issues with sampling 

Solutions to sampling problems can come from sampling theory: @groves2009survey

considerations:

* sampling frame: the elements in from populations that you have access to
* ineligible units: elements in the sampling frame that are not your target
* undercoverage: elements from target population that are not in the frame

solutions:

* stratified sampling

#### Issues with instruments

@beck2022improving 

* annotation collection requires design thinking
  * Task Structure: specific wording and response options, including debates over the inclusion of "I don't Know" option
  * Order Effects: specific judgements are affected by previous perceptions
  * Annotator Effects: backgrounds, opinions, experiences of respondents affect responses

[another ref that describes the target as a latent variable]

## Present Work

We incorporate these considerations in the design of our study, and attempt to further the field in the following ways:

* we attempt representative sampling of both media and respondents
* we aim to estimate 10-dimensional psychological construct
* we select media that is ambiguous (i.e. that will result in subjectivity in the ratings) as well as media that we expect not to be ambiguous for comparison
* we estimate a-priori the number of ratings necessary rather than assuming
* we take into account perspectives

case study of this thesis works towards path (b) in @liem2018psychology shown in:
![](images/Liem_2018_figure_2.png)


## References

