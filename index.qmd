---
title: "Advancing Perspectivist Ground Truthing with Social Science "
---



### ML uses human annotations very often

@geiger2021garbage 

* 200 randomly sampled ML papers from 3 domains:
  * Social Sciences & Humanities
  * Life & Biomedical Sciences
  * Physical & Environmental Sciences

* Out of 141 classification tasks, 103 (73.05%) used human labels
* Out of 103 human labels, 58 (56.31%) used only external labels

i.e. ML re-uses external labels, and inadequately reports 'ground truth'

without details of ground truth, we cannot know what the resulting model represents
[perhaps cat image parable here?]

### Human annotations aren't always accurate

@griffin2004perspectives review errors and biases in human judgements[^2]

[^2]: @griffin2004perspectives note that much of this work was about people guessing knowledge from an almanac, and then guessing how accurate they were

* over/under prediction: confidence score is higher/lower than accuracy
* over/under extremity: confidence is more extreme at ends

also reviews possible reasons:

* optimistic overconfidence
* confirmation bias
* case-based judgment
* ecological probability
* error model (psychometric model)

@beck2022improving 

* annotation collection requires design thinking
  * Task Structure: specific wording and response options, including debates over the inclusion of "I don't Know" option
  * Order Effects: specific judgements are affected by previous perceptions
  * Annotator Effects: backgrounds, opinions, experiences of respondents affect responses
  


### social and computational sciences have different focci

social sciences:

* interpretable meaning of x and y
* design is informed by theory

computational sciences:

* learning procedure f(x)

![](images/Liem_2018_figure_1.png)
Fig. 1: @liem2018psychology

thus, positive yet unrealized interdisciplinary potential[^1], but also strong negative potential when disciplines collaborate i.e. the "worst of both worlds" @hullman2022worst 

[^1]: case study of this thesis works towards path (b) in @liem2018psychology shown in:
![](images/Liem_2018_figure_2.png)



#### A tale of two studies: the case of personnel selection

@ponce2016chalearn 

* efficient way to gather media and annotation data
* BUT no validation of instrument, or ecologically valid media data

![](images/Ponce_Lopez_2016_figure_2.png)
Figure 2: @ponce2016chalearn 

Compared to @koutsoumpis2024beyond:

* ecological validity: media data was mock asynchronous video interviews
* ecological validity: interview questions designed to activate personality facets
* personality instruments: validated HEXACO scale
* perspectives: self & observer ratings
